{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imnn\n",
    "import imnn.lfi\n",
    "from imnn.utils import value_and_jacrev, value_and_jacfwd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jraph\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pylians halo read script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nroot = '/mnt/xfs1/home/fvillaescusa/data/Neutrino_simulations/Sims_Dec16_2/'\\n################################## INPUT ######################################\\nfolders = ['0.0eV/','0.06eV/','0.10eV/','0.10eV_degenerate/',\\n           '0.15eV/','0.6eV/',\\n           '0.0eV_0.798/','0.0eV_0.807/','0.0eV_0.818/','0.0eV_0.822/',\\n           '0.0eV_s8c/','0.0eV_s8m/']\\n###############################################################################\\n# do a loop over the different cosmologies\\nfor folder in folders:\\n    # do a loop over the different realizations\\n    for i in range(1,101):\\n        snapdir = root + folder + '%d/'%i\\n        \\n        # do a loop over the different redshift\\n        for snapnum in [0,1,2,3]:\\n            FoF_folder     = snapdir+'groups_%03d'%snapnum\\n            old_FoF_folder = snapdir+'original_groups_%03d'%snapnum\\n            if os.path.exists(FoF_folder):\\n                print('%s\\t%d\\t%d\\texists'%(folder,i,snapnum))\\n                if os.path.exists(old_FoF_folder):\\n                    continue\\n                # create new FoF file\\n                f_tab = '%s/group_tab_%03d.0'%(snapdir,snapnum)\\n                f_ids = '%s/group_ids_%03d.0'%(snapdir,snapnum)\\n                FoF = readfof.FoF_catalog(snapdir,snapnum,long_ids=False,\\n                                          swap=False,SFR=False)\\n                writeFoFCatalog(FoF, f_tab, idsFile=f_ids)\\n           \\n                # rename FoF folder, create new FoF folder and move files to it\\n                os.system('mv '+FoF_folder+' '+old_FoF_folder)\\n                os.system('mkdir '+FoF_folder)\\n                os.system('mv '+f_tab+' '+f_ids+' '+FoF_folder)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model read_FoF\"\"\"\n",
    "#readfof(Base,snapnum,endian=None)\n",
    "#Read FoF files from Gadget, P-FoF\n",
    "     #Parameters:\n",
    "        #basedir: where your FoF folder located\n",
    "        #snapnum: snapshot number\n",
    "        #long_ids: whether particles ids are uint32 or uint64\n",
    "        #swap: False or True\n",
    "     #return structures:\n",
    "        #TotNgroups,TotNids,GroupLen,GroupOffset,GroupMass,GroupPos,GroupIDs...\n",
    "     #Example:\n",
    "        #--------\n",
    "        #FoF_halos=readfof(\"/data1/villa/b500p512nu0.6z99tree\",17,long_ids=True,swap=False)\n",
    "        #Masses=FoF_halos.GroupMass\n",
    "        #IDs=FoF_halos.GroupIDs\n",
    "        #--------\n",
    "        #updated time 19 Oct 2012 by wgcui\n",
    "\n",
    "#For simulations with SFR, set SFR=True\n",
    "#The physical velocities of the halos are found multiplying the field \n",
    "#GroupVel by (1+z)\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from struct import unpack\n",
    "\n",
    "class FoF_catalog:\n",
    "    def __init__(self, basedir, snapnum, long_ids=False, swap=False,\n",
    "                 SFR=False, read_IDs=True, prefix='/groups_'):\n",
    "\n",
    "        if long_ids:  format = np.uint64\n",
    "        else:         format = np.uint32\n",
    "\n",
    "        exts=('000'+str(snapnum))[-3:]\n",
    "\n",
    "        #################  READ TAB FILES ################# \n",
    "        fnb, skip, Final = 0, 0, False\n",
    "        dt1 = np.dtype((np.float32,3))\n",
    "        dt2 = np.dtype((np.float32,6))\n",
    "        prefix = basedir + prefix + exts + \"/group_tab_\" + exts + \".\"\n",
    "        while not(Final):\n",
    "            f=open(prefix+str(fnb), 'rb')\n",
    "            self.Ngroups    = np.fromfile(f, dtype=np.int32,  count=1)[0]\n",
    "            self.TotNgroups = np.fromfile(f, dtype=np.int32,  count=1)[0]\n",
    "            self.Nids       = np.fromfile(f, dtype=np.int32,  count=1)[0]\n",
    "            self.TotNids    = np.fromfile(f, dtype=np.uint64, count=1)[0]\n",
    "            self.Nfiles     = np.fromfile(f, dtype=np.uint32, count=1)[0]\n",
    "\n",
    "            TNG, NG = self.TotNgroups, self.Ngroups\n",
    "            if fnb == 0:\n",
    "                self.GroupLen    = np.empty(TNG, dtype=np.int32)\n",
    "                self.GroupOffset = np.empty(TNG, dtype=np.int32)\n",
    "                self.GroupMass   = np.empty(TNG, dtype=np.float32)\n",
    "                self.GroupPos    = np.empty(TNG, dtype=dt1)\n",
    "                self.GroupVel    = np.empty(TNG, dtype=dt1)\n",
    "                self.GroupTLen   = np.empty(TNG, dtype=dt2)\n",
    "                self.GroupTMass  = np.empty(TNG, dtype=dt2)\n",
    "                if SFR:  self.GroupSFR = np.empty(TNG, dtype=np.float32)\n",
    "                    \n",
    "            if NG>0:\n",
    "                locs=slice(skip,skip+NG)\n",
    "                self.GroupLen[locs]    = np.fromfile(f,dtype=np.int32,count=NG)\n",
    "                self.GroupOffset[locs] = np.fromfile(f,dtype=np.int32,count=NG)\n",
    "                self.GroupMass[locs]   = np.fromfile(f,dtype=np.float32,count=NG)\n",
    "                self.GroupPos[locs]    = np.fromfile(f,dtype=dt1,count=NG)\n",
    "                self.GroupVel[locs]    = np.fromfile(f,dtype=dt1,count=NG)\n",
    "                self.GroupTLen[locs]   = np.fromfile(f,dtype=dt2,count=NG)\n",
    "                self.GroupTMass[locs]  = np.fromfile(f,dtype=dt2,count=NG)\n",
    "                if SFR:\n",
    "                    self.GroupSFR[locs]=np.fromfile(f,dtype=np.float32,count=NG)\n",
    "                skip+=NG\n",
    "\n",
    "                if swap:\n",
    "                    self.GroupLen.byteswap(True)\n",
    "                    self.GroupOffset.byteswap(True)\n",
    "                    self.GroupMass.byteswap(True)\n",
    "                    self.GroupPos.byteswap(True)\n",
    "                    self.GroupVel.byteswap(True)\n",
    "                    self.GroupTLen.byteswap(True)\n",
    "                    self.GroupTMass.byteswap(True)\n",
    "                    if SFR:  self.GroupSFR.byteswap(True)\n",
    "                        \n",
    "            curpos = f.tell()\n",
    "            f.seek(0,os.SEEK_END)\n",
    "            if curpos != f.tell():\n",
    "                raise Exception(\"Warning: finished reading before EOF for tab file\",fnb)\n",
    "            f.close()\n",
    "            fnb+=1\n",
    "            if fnb==self.Nfiles: Final=True\n",
    "\n",
    "\n",
    "        #################  READ IDS FILES ################# \n",
    "        if read_IDs:\n",
    "\n",
    "            fnb,skip=0,0\n",
    "            Final=False\n",
    "            while not(Final):\n",
    "                fname=basedir+\"/groups_\" + exts +\"/group_ids_\"+exts +\".\"+str(fnb)\n",
    "                f=open(fname,'rb')\n",
    "                Ngroups     = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                TotNgroups  = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                Nids        = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                TotNids     = np.fromfile(f,dtype=np.uint64,count=1)[0]\n",
    "                Nfiles      = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                Send_offset = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                if fnb==0:\n",
    "                    self.GroupIDs=np.zeros(dtype=format,shape=TotNids)\n",
    "                if Ngroups>0:\n",
    "                    if long_ids:\n",
    "                        IDs=np.fromfile(f,dtype=np.uint64,count=Nids)\n",
    "                    else:\n",
    "                        IDs=np.fromfile(f,dtype=np.uint32,count=Nids)\n",
    "                    if swap:\n",
    "                        IDs=IDs.byteswap(True)\n",
    "                    self.GroupIDs[skip:skip+Nids]=IDs[:]\n",
    "                    skip+=Nids\n",
    "                curpos = f.tell()\n",
    "                f.seek(0,os.SEEK_END)\n",
    "                if curpos != f.tell():\n",
    "                    raise Exception(\"Warning: finished reading before EOF for IDs file\",fnb)\n",
    "                f.close()\n",
    "                fnb+=1\n",
    "                if fnb==Nfiles: Final=True\n",
    "\n",
    "\n",
    "# This function is used to write one single file for the FoF instead of having\n",
    "# many files. This will make faster the reading of the FoF file\n",
    "def writeFoFCatalog(fc, tabFile, idsFile=None):\n",
    "    if fc.TotNids > (1<<32)-1: raise Exception('TotNids overflow')\n",
    "\n",
    "    f = open(tabFile, 'wb')\n",
    "    np.asarray(fc.TotNgroups).tofile(f)\n",
    "    np.asarray(fc.TotNgroups).tofile(f)\n",
    "    np.asarray(fc.TotNids, dtype=np.int32).tofile(f)\n",
    "    np.asarray(fc.TotNids).tofile(f)\n",
    "    np.asarray(1, dtype=np.uint32).tofile(f)\n",
    "    fc.GroupLen.tofile(f)\n",
    "    fc.GroupOffset.tofile(f)\n",
    "    fc.GroupMass.tofile(f)\n",
    "    fc.GroupPos.tofile(f)\n",
    "    fc.GroupVel.tofile(f)\n",
    "    fc.GroupTLen.tofile(f)\n",
    "    fc.GroupTMass.tofile(f)\n",
    "    if hasattr(fc, 'GroupSFR'):\n",
    "        fc.GroupSFR.tofile(f)\n",
    "    f.close()\n",
    "\n",
    "    if idsFile:\n",
    "        f = open(idsFile, 'wb')\n",
    "        np.asarray(fc.TotNgroups).tofile(f)\n",
    "        np.asarray(fc.TotNgroups).tofile(f)\n",
    "        np.asarray(fc.TotNids, dtype=np.uint32).tofile(f) \n",
    "        np.asarray(fc.TotNids).tofile(f)\n",
    "        np.asarray(1, dtype=np.uint32).tofile(f)\n",
    "        np.asarray(0, dtype=np.uint32).tofile(f) \n",
    "        fc.GroupIDs.tofile(f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "# This is an example on how to change files\n",
    "\"\"\"\n",
    "root = '/mnt/xfs1/home/fvillaescusa/data/Neutrino_simulations/Sims_Dec16_2/'\n",
    "################################## INPUT ######################################\n",
    "folders = ['0.0eV/','0.06eV/','0.10eV/','0.10eV_degenerate/',\n",
    "           '0.15eV/','0.6eV/',\n",
    "           '0.0eV_0.798/','0.0eV_0.807/','0.0eV_0.818/','0.0eV_0.822/',\n",
    "           '0.0eV_s8c/','0.0eV_s8m/']\n",
    "###############################################################################\n",
    "# do a loop over the different cosmologies\n",
    "for folder in folders:\n",
    "    # do a loop over the different realizations\n",
    "    for i in range(1,101):\n",
    "        snapdir = root + folder + '%d/'%i\n",
    "        \n",
    "        # do a loop over the different redshift\n",
    "        for snapnum in [0,1,2,3]:\n",
    "            FoF_folder     = snapdir+'groups_%03d'%snapnum\n",
    "            old_FoF_folder = snapdir+'original_groups_%03d'%snapnum\n",
    "            if os.path.exists(FoF_folder):\n",
    "                print('%s\\t%d\\t%d\\texists'%(folder,i,snapnum))\n",
    "                if os.path.exists(old_FoF_folder):\n",
    "                    continue\n",
    "                # create new FoF file\n",
    "                f_tab = '%s/group_tab_%03d.0'%(snapdir,snapnum)\n",
    "                f_ids = '%s/group_ids_%03d.0'%(snapdir,snapnum)\n",
    "                FoF = readfof.FoF_catalog(snapdir,snapnum,long_ids=False,\n",
    "                                          swap=False,SFR=False)\n",
    "                writeFoFCatalog(FoF, f_tab, idsFile=f_ids)\n",
    "           \n",
    "                # rename FoF folder, create new FoF folder and move files to it\n",
    "                os.system('mv '+FoF_folder+' '+old_FoF_folder)\n",
    "                os.system('mkdir '+FoF_folder)\n",
    "                os.system('mv '+f_tab+' '+f_ids+' '+FoF_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maindir = '/data80/makinen/quijote/Halos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "snapdir = maindir + 'Om_m/23/' #folder hosting the catalogue\n",
    "snapnum = 4                                            #redshift 0\n",
    "\n",
    "# determine the redshift of the catalogue\n",
    "z_dict = {4:0.0, 3:0.5, 2:1.0, 1:2.0, 0:3.0}\n",
    "redshift = z_dict[snapnum]\n",
    "\n",
    "# read the halo catalogue\n",
    "FoF = FoF_catalog(snapdir, snapnum, long_ids=False,\n",
    "                          swap=False, SFR=False, read_IDs=False)\n",
    "\n",
    "# get the properties of the halos\n",
    "pos_h = FoF.GroupPos/1e3            # Halo positions in Mpc/h\n",
    "mass  = FoF.GroupMass*1e10          # Halo masses in Msun/h\n",
    "vel_h = FoF.GroupVel*(1.0+redshift) # Halo peculiar velocities in km/s\n",
    "Npart = FoF.GroupLen                # Number of CDM particles in the halo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# helper functions\n",
    "def get_distances(X):\n",
    "    nx = X.shape[0]\n",
    "    return (X[:, None, :] - X[None, :, :])[jnp.tril_indices(nx, k=-1)]\n",
    "\n",
    "\n",
    "def get_receivers_senders(nx, dists, connect_radius=100):\n",
    "    '''connect nodes within `connect_radius` units'''\n",
    "    \n",
    "    senders,receivers = jnp.tril_indices(nx, k=-1)\n",
    "    mask = dists < connect_radius\n",
    "    return senders[mask], receivers[mask]\n",
    "\n",
    "def get_r2(X):\n",
    "    nx = X.shape[0]\n",
    "    # can also do\n",
    "    #a = X\n",
    "    #b = a.reshape(a.shape[0], 1, a.shape[1])\n",
    "    # jnp.sqrt(jnp.einsum('ijk, ijk->ij', a-b, a-b))\n",
    "    alldists = jnp.linalg.norm(X[:, None, :] - X[None, :, :], axis=-1)\n",
    "    return alldists[jnp.tril_indices(nx, k=-1)]\n",
    "\n",
    "#@partial(jax.jit, static_argnums=3)\n",
    "def numpy_to_graph(X, V, masses, \n",
    "                   Npart,\n",
    "                   connect_radius=50, \n",
    "                   return_components=True,\n",
    "                   scale_inputs=True, \n",
    "                   nx=None):\n",
    "    if nx is None:\n",
    "        nx = jnp.array([X.shape[0]])\n",
    "        \n",
    "    _nx = jnp.array([nx])\n",
    "    \n",
    "    masses = jnp.array(masses)\n",
    "    Npart = jnp.array(Npart)\n",
    "    \n",
    "    if scale_inputs:\n",
    "        X /= 1000. # in units of Gpc\n",
    "        connect_radius /= 1000. # in units of Gpc\n",
    "        V /= 1000. \n",
    "        masses = jnp.log(masses)\n",
    "        Npart = jnp.log(Npart)\n",
    "        \n",
    "    \n",
    "    # mask out halos with distances < connect_radius\n",
    "    dists = get_r2(X)\n",
    "    \n",
    "    receivers, senders = get_receivers_senders(nx, dists, \n",
    "                                               connect_radius=connect_radius)\n",
    "    \n",
    "    edges = dists[dists < connect_radius]\n",
    "\n",
    "    receivers = jnp.array(receivers)\n",
    "    senders = jnp.array(senders)\n",
    "\n",
    "    if masses is None:\n",
    "        # Default all masses to one\n",
    "        masses = jnp.ones(nx)\n",
    "    elif isinstance(masses, (int, float)):\n",
    "        masses = masses*jnp.ones(nx)\n",
    "    else:\n",
    "        assert len(masses) == nx, 'Wrong size for masses'\n",
    "\n",
    "\n",
    "    nodes = jnp.concatenate([masses.reshape([-1, 1]), Npart.reshape((-1,1)), X, V], axis=1)\n",
    "    \n",
    "    if return_components:\n",
    "        return nodes,senders,receivers,edges[:, None], nx, jnp.array(edges.shape[0])\n",
    "    \n",
    "    else:\n",
    "        graph = jraph.GraphsTuple(nodes=nodes, senders=senders, receivers=receivers,\n",
    "                                  edges=edges[:, None], n_node=_nx, n_edge=jnp.array([edges.shape[0]]), globals=None)\n",
    "        return graph\n",
    "\n",
    "\n",
    "\n",
    "def update_edge_dummy(edge, sender_node, receiver_node, globals_):\n",
    "    return edge\n",
    "\n",
    "\n",
    "def update_node_dummy(node, sender, receiver, globals_):\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 7.15 µs\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# do for small test data\n",
    "%time\n",
    "mass_cut = (mass > 1.5e15)\n",
    "print(np.sum(mass_cut))\n",
    "\n",
    "mymasses = jnp.array(mass[mass_cut])\n",
    "X = pos_h[mass_cut]\n",
    "V = vel_h[mass_cut]\n",
    "_npart = Npart[mass_cut]\n",
    "\n",
    "# mask out halos with distances < 50 Mpc\n",
    "connect_radius = 500 # Mpc\n",
    "dists = np.abs(get_distances(X))\n",
    "\n",
    "graph = numpy_to_graph(X, V, mymasses, _npart, return_components=False, connect_radius=connect_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.senders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a function to pull in a bunch of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data80/makinen/quijote/Halos/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_sim(folder_name, \n",
    "             sim_num,\n",
    "             mass_cut=2e15,\n",
    "             snapnum=4):\n",
    "    # input files\n",
    "    snapdir = folder_name + '/%d/'%(sim_num) #folder hosting the catalogue\n",
    "    snapnum = snapnum    #vredshift\n",
    "\n",
    "    # determine the redshift of the catalogue\n",
    "    z_dict = {4:0.0, 3:0.5, 2:1.0, 1:2.0, 0:3.0}\n",
    "    redshift = z_dict[snapnum]\n",
    "\n",
    "    # read the halo catalogue\n",
    "    FoF = FoF_catalog(snapdir, snapnum, long_ids=False,\n",
    "                              swap=False, SFR=False, read_IDs=False)\n",
    "\n",
    "    # get the properties of the halos\n",
    "    pos_h = FoF.GroupPos/1e3            # Halo positions in Mpc/h\n",
    "    mass  = FoF.GroupMass*1e10          # Halo masses in Msun/h\n",
    "    vel_h = FoF.GroupVel*(1.0+redshift) # Halo peculiar velocities in km/s\n",
    "    Npart = FoF.GroupLen                # Number of CDM particles in the halo\n",
    "    \n",
    "    mass_cut = (mass > mass_cut)\n",
    "    \n",
    "    return mass[mass_cut],pos_h[mass_cut],vel_h[mass_cut],Npart[mass_cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halo_sims(folder_name, \n",
    "                  n_sims=2,\n",
    "                  mass_cut=2e15, \n",
    "                  connect_radius=50):\n",
    "\n",
    "    \n",
    "    graphs = []\n",
    "    \n",
    "    for i in range(n_sims):\n",
    "        mass,X,V,Npart = load_single_sim(folder_name, i, mass_cut)\n",
    "        graph = numpy_to_graph(X, V, mass, Npart, \n",
    "                               connect_radius=connect_radius)\n",
    "        \n",
    "        graphs.append(graph)\n",
    "    \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halo_batch(folder_name, \n",
    "                  sim_index,\n",
    "                  pad_nodes_to=150,\n",
    "                  pad_edges_to=150,\n",
    "                  mass_cut=2e15, \n",
    "                  connect_radius=200,\n",
    "                  node_features=8,\n",
    "                  edge_features=1):\n",
    "\n",
    "    n_sims = len(sim_index)\n",
    "    \n",
    "    \n",
    "    # padding for vmapping\n",
    "    def get_padding(pad_nodes_to, pad_edges_to):\n",
    "        nodes =  jnp.zeros((n_sims, pad_nodes_to, node_features))\n",
    "        senders = jnp.zeros((n_sims, pad_edges_to), dtype=int) \n",
    "        receivers = jnp.zeros((n_sims, pad_edges_to), dtype=int)\n",
    "        edges = jnp.zeros((n_sims, pad_edges_to, edge_features))\n",
    "        n_node = []\n",
    "        n_edge = []\n",
    "        _globals = None\n",
    "        return nodes,senders,receivers,edges,n_node,n_edge,_globals\n",
    "    \n",
    "    nodes,senders,receivers,edges,n_node,n_edge,_globals = get_padding(pad_nodes_to,pad_edges_to)\n",
    "    l = 0\n",
    "    \n",
    "    while l < len(sim_index):\n",
    "        \n",
    "        i = sim_index[l]\n",
    "        \n",
    "        mass,X,V,Npart = load_single_sim(folder_name, i, mass_cut)\n",
    "        _nodes,_senders,_receivers,_edges, _nx, _n_edge = numpy_to_graph(X, V, mass, \n",
    "                                                       Npart,return_components=True,\n",
    "                                                       connect_radius=connect_radius)\n",
    "        \n",
    "        if _nx < pad_nodes_to:\n",
    "            if _n_edge < pad_edges_to:\n",
    "                nodes = nodes.at[l, :_nodes.shape[0], :].set(_nodes)\n",
    "                senders = senders.at[l, :_senders.shape[0]].set(_senders)\n",
    "                receivers = receivers.at[l, :_receivers.shape[0]].set(_receivers)\n",
    "\n",
    "                edges = edges.at[l, :_edges.shape[0], :].set(_edges)\n",
    "\n",
    "                n_node.append(_nx) # these control how many edges / nodes get counted\n",
    "                n_edge.append(_n_edge)\n",
    "                l += 1\n",
    "\n",
    "            else:\n",
    "                print('boosting edge padding; \\n restarting batch loop...')\n",
    "                pad_edges_to += 10\n",
    "                print('new edge padding length:', pad_edges_to)\n",
    "                nodes,senders,receivers,edges,n_node,n_edge,_globals = get_padding(pad_nodes_to,pad_edges_to)\n",
    "                l = 0 \n",
    "\n",
    "        else:\n",
    "            print('boosting node padding; \\n restarting batch loop...')\n",
    "            pad_nodes_to += 10\n",
    "            print('new node padding length:', pad_nodes_to)\n",
    "            nodes,senders,receivers,edges,n_node,n_edge,_globals = get_padding(pad_nodes_to,pad_edges_to)\n",
    "            l = 0 \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    n_node = jnp.array(n_node)\n",
    "    n_edge = jnp.array(n_edge)\n",
    "    \n",
    "    # assemble explicitly batched GraphsTuple\n",
    "    batched_graph = jraph.GraphsTuple(nodes=nodes, senders=senders, receivers=receivers,\n",
    "                                edges=edges, n_node=n_node, n_edge=n_edge, globals=None)\n",
    "        \n",
    "    \n",
    "    return batched_graph, (pad_nodes_to, pad_edges_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now set up (mini) datasets for IMNN testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data80/makinen/quijote/Halos/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 2\n",
    "n_summaries=n_params\n",
    "\n",
    "θ_fid = jnp.array([0.3175, 0.834])\n",
    "δθ = jnp.array([0.01, 0.015])\n",
    "\n",
    "n_s = 100\n",
    "n_d = n_s\n",
    "\n",
    "input_shape = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 s, sys: 18 s, total: 1min 5s\n",
      "Wall time: 1min 56s\n",
      "CPU times: user 19.6 s, sys: 9.1 s, total: 28.7 s\n",
      "Wall time: 55.8 s\n",
      "CPU times: user 16.6 s, sys: 7.73 s, total: 24.3 s\n",
      "Wall time: 51.3 s\n",
      "CPU times: user 13.6 s, sys: 6.81 s, total: 20.4 s\n",
      "Wall time: 42.7 s\n",
      "CPU times: user 17.3 s, sys: 8.04 s, total: 25.3 s\n",
      "Wall time: 53.1 s\n",
      "CPU times: user 19.6 s, sys: 8.83 s, total: 28.4 s\n",
      "Wall time: 58.2 s\n",
      "CPU times: user 10.3 s, sys: 5.65 s, total: 15.9 s\n",
      "Wall time: 34.7 s\n",
      "CPU times: user 10.1 s, sys: 5.48 s, total: 15.5 s\n",
      "Wall time: 33.1 s\n",
      "CPU times: user 11.6 s, sys: 6.07 s, total: 17.7 s\n",
      "Wall time: 37.3 s\n",
      "CPU times: user 16.8 s, sys: 8.03 s, total: 24.9 s\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sim_index = np.arange(100)\n",
    "val_index = np.arange(start=100, stop=200)\n",
    "mass_cut = 1.5e15 \n",
    "pad_nodes_to = 160 # could devise a function to pull in a dummy graph to get max nodes for padding\n",
    "pad_edges_to = 400\n",
    "\n",
    "# get fiducial \n",
    "folder_name = maindir + 'fiducial'\n",
    "%time fiducial,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "# get validation fiducial\n",
    "%time validation_fiducial,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "\n",
    "# get regular derivatives\n",
    "folder_name = maindir + 'Om_m'\n",
    "%time Om_m,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 'Om_p'\n",
    "%time Om_p,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_m'\n",
    "%time s8_m,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_p'\n",
    "%time s8_p,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "numerical_derivative = jraph.batch([Om_m, s8_m, Om_p, s8_p])\n",
    "\n",
    "# get validation derivatives\n",
    "folder_name = maindir + 'Om_m'\n",
    "%time Om_m,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 'Om_p'\n",
    "%time Om_p,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_m'\n",
    "%time s8_m,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_p'\n",
    "%time s8_p,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "validation_numerical_derivative = jraph.batch([Om_m, s8_m, Om_p, s8_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.18611607],\n",
       "              [0.18404277],\n",
       "              [0.19885303],\n",
       "              ...,\n",
       "              [0.        ],\n",
       "              [0.        ],\n",
       "              [0.        ]],\n",
       "\n",
       "             [[0.14739507],\n",
       "              [0.01311244],\n",
       "              [0.18747316],\n",
       "              ...,\n",
       "              [0.        ],\n",
       "              [0.        ],\n",
       "              [0.        ]],\n",
       "\n",
       "             [[0.14648525],\n",
       "              [0.14145936],\n",
       "              [0.18091497],\n",
       "              ...,\n",
       "              [0.        ],\n",
       "              [0.        ],\n",
       "              [0.        ]],\n",
       "\n",
       "             ...,\n",
       "\n",
       "             [[0.18964247],\n",
       "              [0.07308118],\n",
       "              [0.16779605],\n",
       "              ...,\n",
       "              [0.        ],\n",
       "              [0.        ],\n",
       "              [0.        ]],\n",
       "\n",
       "             [[0.15187661],\n",
       "              [0.19047037],\n",
       "              [0.18236078],\n",
       "              ...,\n",
       "              [0.        ],\n",
       "              [0.        ],\n",
       "              [0.        ]],\n",
       "\n",
       "             [[0.17316844],\n",
       "              [0.14827846],\n",
       "              [0.17360575],\n",
       "              ...,\n",
       "              [0.        ],\n",
       "              [0.        ],\n",
       "              [0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_numerical_derivative.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph network in Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from typing import Sequence\n",
    "\n",
    "import optax\n",
    "\n",
    "class ExplicitMLP(nn.Module):\n",
    "  \"\"\"A flax MLP.\"\"\"\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, lyr in enumerate([nn.Dense(feat) for feat in self.features]):\n",
    "      x = lyr(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Functions must be passed to jraph GNNs, but pytype does not recognise\n",
    "# linen Modules as callables to here we wrap in a function.\n",
    "def make_embed_fn(latent_size):\n",
    "  def embed(inputs):\n",
    "    x = inputs\n",
    "    return nn.Dense(latent_size)(x)\n",
    "  return embed\n",
    "\n",
    "\n",
    "def make_mlp(features):\n",
    "  @jraph.concatenated_args\n",
    "  def update_fn(inputs):\n",
    "    return ExplicitMLP(features)(inputs)\n",
    "  return update_fn\n",
    "\n",
    "\n",
    "class flaxGraphNetwork(nn.Module):\n",
    "  \"\"\"A flax GraphNetwork.\"\"\"\n",
    "  mlp_features: Sequence[int]\n",
    "  latent_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, graph):\n",
    "    # Add a global parameter for graph classification.\n",
    "    graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], n_params]))\n",
    "    #\n",
    "    #graph = graph._replace(edges=jnp.zeros(graph.edges.shape))\n",
    "\n",
    "    embedder = jraph.GraphMapFeatures(\n",
    "        embed_node_fn=make_embed_fn(self.latent_size),\n",
    "        embed_edge_fn=make_embed_fn(self.latent_size),\n",
    "        embed_global_fn=make_embed_fn(self.latent_size))\n",
    "    net = jraph.GraphNetwork(\n",
    "        update_node_fn=make_mlp(self.mlp_features),\n",
    "        update_edge_fn=make_mlp(self.mlp_features),\n",
    "        # The global update outputs size 1 for information maximisation.\n",
    "        update_global_fn=make_mlp(self.mlp_features + (n_params,)))  # pytype: disable=unsupported-operands\n",
    "    return net(embedder(graph)).globals.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mass,X,V,Npart = load_single_sim(folder_name, 1, 2e15)\n",
    "graph = numpy_to_graph(X, V, mass, Npart, connect_radius=200, return_components=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng,key = jax.random.split(rng)\n",
    "\n",
    "model = flaxGraphNetwork([50, 50], 50)\n",
    "\n",
    "initial_w = model.init(key, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_app = lambda d: model.apply(initial_w, d)\n",
    "_out = jax.vmap(_app)(validation_fiducial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-768.98517,  393.92627],\n",
       "             [-766.2354 ,  407.08026],\n",
       "             [-758.02966,  400.80264],\n",
       "             [-782.51605,  405.6474 ],\n",
       "             [-779.3203 ,  409.52927],\n",
       "             [-769.40424,  408.14783],\n",
       "             [-752.5132 ,  391.92737],\n",
       "             [-767.82764,  403.06015],\n",
       "             [-788.77545,  412.18723],\n",
       "             [-766.9621 ,  393.44073],\n",
       "             [-773.4403 ,  407.6631 ],\n",
       "             [-768.2393 ,  399.25476],\n",
       "             [-752.21796,  398.73114],\n",
       "             [-772.7105 ,  395.96143],\n",
       "             [-781.74255,  405.8448 ],\n",
       "             [-779.5877 ,  402.13553],\n",
       "             [-779.5493 ,  411.83752],\n",
       "             [-769.065  ,  397.4221 ],\n",
       "             [-773.3508 ,  404.24902],\n",
       "             [-771.37   ,  407.98367],\n",
       "             [-764.32416,  398.8476 ],\n",
       "             [-774.8362 ,  402.47906],\n",
       "             [-765.3595 ,  393.2975 ],\n",
       "             [-767.0595 ,  403.10867],\n",
       "             [-773.2092 ,  401.0192 ],\n",
       "             [-789.4975 ,  409.89847],\n",
       "             [-764.9263 ,  395.69287],\n",
       "             [-773.506  ,  404.34906],\n",
       "             [-789.67664,  405.75665],\n",
       "             [-781.6992 ,  401.11642],\n",
       "             [-778.27954,  398.6062 ],\n",
       "             [-772.63983,  400.02597],\n",
       "             [-764.50714,  399.22015],\n",
       "             [-774.0677 ,  399.0359 ],\n",
       "             [-771.70685,  404.01865],\n",
       "             [-764.55176,  388.54968],\n",
       "             [-769.62695,  402.0438 ],\n",
       "             [-783.04767,  400.52823],\n",
       "             [-785.8922 ,  406.47165],\n",
       "             [-755.1966 ,  389.80627],\n",
       "             [-776.22876,  408.12192],\n",
       "             [-765.5704 ,  401.9087 ],\n",
       "             [-783.6139 ,  400.19342],\n",
       "             [-769.781  ,  396.16757],\n",
       "             [-770.68787,  400.69556],\n",
       "             [-782.5786 ,  402.5786 ],\n",
       "             [-778.414  ,  406.6776 ],\n",
       "             [-780.49115,  402.63077],\n",
       "             [-766.27795,  397.364  ],\n",
       "             [-765.4802 ,  398.63525],\n",
       "             [-767.36926,  397.66635],\n",
       "             [-753.9314 ,  394.3586 ],\n",
       "             [-768.1134 ,  403.0739 ],\n",
       "             [-785.5214 ,  411.258  ],\n",
       "             [-761.00037,  402.27905],\n",
       "             [-756.8861 ,  399.2229 ],\n",
       "             [-778.1699 ,  402.45795],\n",
       "             [-772.7245 ,  397.96313],\n",
       "             [-770.8422 ,  397.9692 ],\n",
       "             [-780.5516 ,  398.80664],\n",
       "             [-782.99396,  409.94318],\n",
       "             [-777.72644,  403.09262],\n",
       "             [-754.2652 ,  399.4621 ],\n",
       "             [-783.4588 ,  405.02777],\n",
       "             [-775.4585 ,  409.94604],\n",
       "             [-774.1953 ,  404.17072],\n",
       "             [-770.04114,  395.77643],\n",
       "             [-778.06335,  399.79926],\n",
       "             [-761.44135,  402.15442],\n",
       "             [-795.9663 ,  403.9464 ],\n",
       "             [-792.58826,  406.41217],\n",
       "             [-771.37494,  395.21906],\n",
       "             [-761.99884,  396.36337],\n",
       "             [-748.82556,  399.05878],\n",
       "             [-773.41064,  394.3138 ],\n",
       "             [-783.4291 ,  403.57257],\n",
       "             [-776.2361 ,  403.32678],\n",
       "             [-779.0333 ,  400.9156 ],\n",
       "             [-778.26025,  395.9718 ],\n",
       "             [-769.7749 ,  405.95886],\n",
       "             [-775.3554 ,  406.6016 ],\n",
       "             [-775.5659 ,  397.8169 ],\n",
       "             [-780.21216,  399.12015],\n",
       "             [-762.74695,  398.77695],\n",
       "             [-782.7515 ,  411.4292 ],\n",
       "             [-763.2739 ,  396.9242 ],\n",
       "             [-764.0521 ,  403.3003 ],\n",
       "             [-775.4414 ,  397.2309 ],\n",
       "             [-781.74756,  402.4673 ],\n",
       "             [-763.6077 ,  393.5623 ],\n",
       "             [-765.2749 ,  398.90656],\n",
       "             [-758.73517,  398.53107],\n",
       "             [-758.8301 ,  399.31384],\n",
       "             [-771.43036,  404.73724],\n",
       "             [-765.94885,  409.6206 ],\n",
       "             [-795.75366,  413.83542],\n",
       "             [-765.36096,  401.62198],\n",
       "             [-781.47766,  409.1106 ],\n",
       "             [-784.1283 ,  409.58926],\n",
       "             [-767.4834 ,  397.88733]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMNN time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## numerical gradient IMNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title imnn module\n",
    "import math\n",
    "import jax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from imnn.utils.utils import _check_boolean, _check_type, _check_input, \\\n",
    "    _check_model, _check_model_output, _check_optimiser, _check_state, \\\n",
    "    _check_statistics_set\n",
    "from imnn.experimental import progress_bar\n",
    "\n",
    "np = jnp\n",
    "\n",
    "class _myIMNN:\n",
    "    \"\"\"Information maximising neural network parent class\n",
    "\n",
    "    This class defines the general fitting framework for information maximising\n",
    "    neural networks. It includes the generic calculations of the Fisher\n",
    "    information matrix from the outputs of a neural network as well as an XLA\n",
    "    compilable fitting routine (with and without a progress bar). This class\n",
    "    also provides a plotting routine for fitting history and a function to\n",
    "    calculate the score compression of network outputs to quasi-maximum\n",
    "    likelihood estimates of model parameter values.\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    simulations and :math:`n_d` derivatives with respect to physical model\n",
    "    parameters are used to calculate network outputs and their derivatives\n",
    "    with respect to the physical model parameters, :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha`, where\n",
    "    :math:`\\\\alpha` labels the physical parameter. The exact details of how\n",
    "    these are calculated depend on the type of available data (see list of\n",
    "    different IMNN below). With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function. Note for large input data-sizes, large :math:`n_s` or massive\n",
    "    networks the gradients may need manually accumulating via the\n",
    "    :func:`~imnn.imnn._aggregated_imnn._AggregatedIMNN`.\n",
    "\n",
    "    ``_IMNN`` is designed as the parent class for a range of specific case\n",
    "    IMNNs. There is a helper function (IMNN) which should return the correct\n",
    "    case when provided with the correct data. These different subclasses are:\n",
    "\n",
    "    :func:`~imnn.SimulatorIMNN`:\n",
    "\n",
    "        Fit an IMNN using simulations generated on-the-fly from a jax (XLA\n",
    "        compilable) simulator\n",
    "\n",
    "    :func:`~imnn.GradientIMNN`:\n",
    "\n",
    "        Fit an IMNN using a precalculated set of fiducial simulations and their\n",
    "        derivatives with respect to model parameters\n",
    "\n",
    "    :func:`~imnn.NumericalGradientIMNN`:\n",
    "\n",
    "        Fit an IMNN using a precalculated set of fiducial simulations and\n",
    "        simulations generated using parameter values just above and below the\n",
    "        fiducial parameter values to make a numerical estimate of the\n",
    "        derivatives of the network outputs. Best stability is achieved when\n",
    "        seeds of the simulations are matched between all parameter directions\n",
    "        for the numerical derivative\n",
    "\n",
    "    :func:`~imnn.AggregatedSimulatorIMNN`:\n",
    "\n",
    "        ``SimulatorIMNN`` distributed over multiple jax devices and gradients\n",
    "        aggregated manually. This might be necessary for very large input sizes\n",
    "        as batching cannot be done when calculating the Fisher information\n",
    "        matrix\n",
    "\n",
    "    :func:`~imnn.AggregatedGradientIMNN`:\n",
    "\n",
    "        ``GradientIMNN`` distributed over multiple jax devices and gradients\n",
    "        aggregated manually. This might be necessary for very large input sizes\n",
    "        as batching cannot be done when calculating the Fisher information\n",
    "        matrix\n",
    "\n",
    "    :func:`~imnn.AggregatedNumericalGradientIMNN`:\n",
    "\n",
    "        ``NumericalGradientIMNN`` distributed over multiple jax devices and\n",
    "        gradients aggregated manually. This might be necessary for very large\n",
    "        input sizes as batching cannot be done when calculating the Fisher\n",
    "        information matrix\n",
    "\n",
    "    :func:`~imnn.DatasetGradientIMNN`:\n",
    "\n",
    "        ``AggregatedGradientIMNN`` with prebuilt TensorFlow datasets\n",
    "\n",
    "    :func:`~imnn.DatasetNumericalGradientIMNN`:\n",
    "\n",
    "        ``AggregatedNumericalGradientIMNN`` with prebuilt TensorFlow datasets\n",
    "\n",
    "    There are currently two other parent classes\n",
    "\n",
    "    :func:`~imnn.imnn._aggregated_imnn.AggregatedIMNN`:\n",
    "\n",
    "        This is the parent class which provides the fitting routine when the\n",
    "        gradients of the network parameters are aggregated manually rather than\n",
    "        automatically by jax. This is necessary if the size of an entire batch\n",
    "        of simulations (and their derivatives with respect to model parameters)\n",
    "        and the network parameters and their calculated gradients is too large\n",
    "        to fit into memory. Note there is a significant performance loss from\n",
    "        using the aggregation so it should only be used for these large data\n",
    "        cases\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_s : int\n",
    "        Number of simulations used to calculate network output covariance\n",
    "    n_d : int\n",
    "        Number of simulations used to calculate mean of network output\n",
    "        derivative with respect to the model parameters\n",
    "    n_params : int\n",
    "        Number of model parameters\n",
    "    n_summaries : int\n",
    "        Number of summaries, i.e. outputs of the network\n",
    "    input_shape : tuple\n",
    "        The shape of a single input to the network\n",
    "    θ_fid : float(n_params,)\n",
    "        The value of the fiducial parameter values used to generate inputs\n",
    "    validate : bool\n",
    "        Whether a validation set is being used\n",
    "    simulate : bool\n",
    "        Whether input simulations are generated on the fly\n",
    "    _run_with_pbar : bool\n",
    "        Book keeping parameter noting that a progress bar is used when\n",
    "        fitting (induces a performance hit). If ``run_with_pbar = True``\n",
    "        and ``run_without_pbar = True`` then a jit compilation error will\n",
    "        occur and so it is prevented\n",
    "    _run_without_pbar : bool\n",
    "        Book keeping parameter noting that a progress bar is not used when\n",
    "        fitting. If ``run_with_pbar = True`` and ``run_without_pbar = True``\n",
    "        then a jit compilation error will occur and so it is prevented\n",
    "    F : float(n_params, n_params)\n",
    "        Fisher information matrix calculated from the network outputs\n",
    "    invF : float(n_params, n_params)\n",
    "        Inverse Fisher information matrix calculated from the network outputs\n",
    "    C : float(n_summaries, n_summaries)\n",
    "        Covariance of the network outputs\n",
    "    invC : float(n_summaries, n_summaries)\n",
    "        Inverse covariance of the network outputs\n",
    "    μ : float(n_summaries,)\n",
    "        Mean of the network outputs\n",
    "    dμ_dθ : float(n_summaries, n_params)\n",
    "        Derivative of the mean of the network outputs with respect to model\n",
    "        parameters\n",
    "    state : :obj:state\n",
    "        The optimiser state used for updating the network parameters and\n",
    "        optimisation algorithm\n",
    "    initial_w : list\n",
    "        List of the network parameters values at initialisation (to restart)\n",
    "    final_w : list\n",
    "        List of the network parameters values at the end of fitting\n",
    "    best_w : list\n",
    "        List of the network parameters values which provide the maxmimum value\n",
    "        of the determinant of the Fisher matrix\n",
    "    w : list\n",
    "        List of the network parameters values (either final or best depending\n",
    "        on setting when calling fit(...))\n",
    "    history : dict\n",
    "        A dictionary containing the fitting history. Keys are\n",
    "            - **detF** -- determinant of the Fisher information at the end of\n",
    "              each iteration\n",
    "            - **detC** -- determinant of the covariance of network outputs at\n",
    "              the end of each iteration\n",
    "            - **detinvC** -- determinant of the inverse covariance of network\n",
    "              outputs at the end of each iteration\n",
    "            - **Λ2** -- value of the covariance regularisation at the end of\n",
    "              each iteration\n",
    "            - **r** -- value of the regularisation coupling at the end of each\n",
    "              iteration\n",
    "            - **val_detF** -- determinant of the Fisher information of the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_detC** -- determinant of the covariance of network outputs\n",
    "              given the validation data at the end of each iteration\n",
    "            - **val_detinvC** -- determinant of the inverse covariance of\n",
    "              network outputs given the validation data at the end of each\n",
    "              iteration\n",
    "            - **val_Λ2** -- value of the covariance regularisation given the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_r** -- value of the regularisation coupling given the\n",
    "              validation data at the end of each iteration\n",
    "            - **max_detF** -- maximum value of the determinant of the Fisher\n",
    "              information on the validation data (if available)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    model:\n",
    "        Neural network as a function of network parameters and inputs\n",
    "    _get_parameters:\n",
    "        Function which extracts the network parameters from the state\n",
    "    _model_initialiser:\n",
    "        Function to initialise neural network weights from RNG and shape tuple\n",
    "    _opt_initialiser:\n",
    "        Function which generates the optimiser state from network parameters\n",
    "    _update:\n",
    "        Function which updates the state from a gradient\n",
    "\n",
    "    Todo\n",
    "    ----\n",
    "    - Finish all docstrings and documentation\n",
    "    - Update `NoiseNumericalGradientIMNN` to inherit from `_AggregatedIMNN`\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, θ_fid,\n",
    "                 model, optimiser, key_or_state, dummy_input=None, no_invC=False, do_reg=True,\n",
    "                 evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all _IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        θ_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float([None], input_shape)) -> float([None],\n",
    "            n_summaries)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        dummy_input : jraph.GraphsTuple or 'jax.numpy.DeviceArray'\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        self.dummy_input=dummy_input\n",
    "        self._initialise_parameters(\n",
    "            n_s, n_d, n_params, n_summaries, input_shape, θ_fid)\n",
    "        self._initialise_model(model, optimiser, key_or_state)\n",
    "        self._initialise_history()\n",
    "        self.no_invC=no_invC\n",
    "        self.do_reg=do_reg\n",
    "        self.evidence=evidence\n",
    "\n",
    "\n",
    "    def _initialise_parameters(self, n_s, n_d, n_params, n_summaries,\n",
    "                               input_shape, θ_fid):\n",
    "        \"\"\"Performs type checking and initialisation of class attributes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        θ_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            Any of the parameters are not correct type\n",
    "        ValueError\n",
    "            Any of the parameters are ``None``\n",
    "            ``Θ_fid`` has the wrong shape\n",
    "        \"\"\"\n",
    "        self.n_s = _check_type(n_s, int, \"n_s\")\n",
    "        self.n_d = _check_type(n_d, int, \"n_d\")\n",
    "        self.n_params = _check_type(n_params, int, \"n_params\")\n",
    "        self.n_summaries = _check_type(n_summaries, int, \"n_summaries\")\n",
    "        self.input_shape = _check_type(input_shape, tuple, \"input_shape\")\n",
    "        self.θ_fid = _check_input(θ_fid, (self.n_params,), \"θ_fid\")\n",
    "\n",
    "        self.validate = False\n",
    "        self.simulate = False\n",
    "        self._run_with_pbar = False\n",
    "        self._run_without_pbar = False\n",
    "\n",
    "        self.F = None\n",
    "        self.invF = None\n",
    "        self.C = None\n",
    "        self.invC = None\n",
    "        self.μ = None\n",
    "        self.dμ_dθ = None\n",
    "\n",
    "        self._model_initialiser = None\n",
    "        self.model = None\n",
    "        self._opt_initialiser = None\n",
    "        self._update = None\n",
    "        self._get_parameters = None\n",
    "        self.state = None\n",
    "        self.initial_w = None\n",
    "        self.final_w = None\n",
    "        self.best_w = None\n",
    "        self.w = None\n",
    "\n",
    "        self.history = None\n",
    "\n",
    "    def _initialise_model(self, model, optimiser, key_or_state):\n",
    "        \"\"\"Initialises neural network parameters or loads optimiser state\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and\n",
    "            the neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float([None], input_shape)) -> float([None],\n",
    "            n_summaries)``. (Essentibly stax-like, see `jax.experimental.stax\n",
    "            <https://jax.readthedocs.io/en/stable/jax.experimental.stax.html>`_\n",
    "            ))\n",
    "        optimiser : tuple or obj, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The design of the model follows `jax's stax module <https://jax.readth\n",
    "        edocs.io/en/latest/jax.experimental.stax.html>`_ in that the model is\n",
    "        encapsulated by two functions, one to initialise the network and one to\n",
    "        call the model, i.e.::\n",
    "\n",
    "            import jax\n",
    "            from jax.experimental import stax\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "\n",
    "            data_key, model_key = jax.random.split(rng)\n",
    "\n",
    "            input_shape = (10,)\n",
    "            inputs = jax.random.normal(data_key, shape=input_shape)\n",
    "\n",
    "            model = stax.serial(\n",
    "                stax.Dense(10),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(10),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(2))\n",
    "\n",
    "            output_shape, initial_params = model[0](model_key, input_shape)\n",
    "\n",
    "            outputs = model[1](initial_params, inputs)\n",
    "\n",
    "        Note that the model used in the IMNN is assumed to be totally\n",
    "        broadcastable, i.e. any batch shape can be used for inputs. This might\n",
    "        require having a layer which reshapes all batch dimensions into a\n",
    "        single dimension and then unwraps it at the last layer. A model such as\n",
    "        that above is already fully broadcastable.\n",
    "\n",
    "        The optimiser should follow `jax's experimental optimiser module <http\n",
    "        s://jax.readthedocs.io/en/stable/jax.experimental.optimizers.html>`_ in\n",
    "        that the optimiser is encapsulated by three functions, one to\n",
    "        initialise the state, one to update the state from a list of gradients\n",
    "        and one to extract the network parameters from the state, .i.e\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from jax.experimental import optimizers\n",
    "            import jax.numpy as np\n",
    "\n",
    "            optimiser = optimizers.adam(step_size=1e-3)\n",
    "\n",
    "            initial_state = optimiser[0](initial_params)\n",
    "            params = optimiser[2](initial_state)\n",
    "\n",
    "            def scalar_output(params, inputs):\n",
    "                return np.sum(model[1](params, inputs))\n",
    "\n",
    "            counter = 0\n",
    "            grad = jax.grad(scalar_output, argnums=0)(params, inputs)\n",
    "            state = optimiser[1](counter, grad, state)\n",
    "\n",
    "        This function either initialises the neural network or the state if\n",
    "        passed a stateless random number generator in ``key_or_state`` or loads\n",
    "        a predefined state if the state is passed to ``key_or_state``. The\n",
    "        functions get mapped to the class functions\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            self.model = model[1]\n",
    "            self._model_initialiser = model[0]\n",
    "\n",
    "            self._opt_initialiser = optimiser[0]\n",
    "            self._update = optimiser[1]\n",
    "            self._get_parameters = optimiser[2]\n",
    "\n",
    "        The state is made into the ``state`` class attribute and the parameters\n",
    "        are assigned to ``initial_w``, ``final_w``, ``best_w`` and ``w`` class\n",
    "        attributes (where ``w`` stands for weights).\n",
    "\n",
    "        There is some type checking done, but for freedom of choice of model\n",
    "        there will be very few raised warnings.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If the random number generator is not correct, or if there is no\n",
    "            possible way to construct a model or an optimiser from the passed\n",
    "            parameters\n",
    "        ValueError\n",
    "            If any input is ``None`` or if the functions for the model or\n",
    "            optimiser do not conform to the necessary specifications\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize FLAX model here\n",
    "        self._model_initialiser = model.init\n",
    "        self.model = model.apply\n",
    "\n",
    "        # unpack optimiser\n",
    "        self._opt_initialiser, self._update = optimiser\n",
    "\n",
    "        #state, key = _check_state(key_or_state)\n",
    "        key = key_or_state\n",
    "\n",
    "        if key is not None:\n",
    "            key = _check_input(key, (2,), \"key_or_state\")\n",
    "            if self.dummy_input is None:\n",
    "                dummy_x = jax.random.uniform(key, self.input_shape)\n",
    "            else:\n",
    "                dummy_x = self.dummy_input\n",
    "\n",
    "            # INITIAL PARAMS\n",
    "            self.initial_w = self._model_initialiser(key, dummy_x)\n",
    "            \n",
    "            # DUMMY OUTPUT\n",
    "            output = self.model(self.initial_w, dummy_x)\n",
    "            # check to see if right shape\n",
    "            _check_model_output(output.shape, (self.n_summaries,))\n",
    "            # INITIAL STATE\n",
    "            self.state = self._opt_initialiser(self.initial_w)\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.state = state\n",
    "            try:\n",
    "                self._get_parameters(self.state)\n",
    "            except Exception:\n",
    "                raise TypeError(\"`state` is not valid for extracting \" +\n",
    "                                \"parameters from\")\n",
    "\n",
    "        self.dummy_x = dummy_x\n",
    "        self.initial_w = self._model_initialiser(key, dummy_x)\n",
    "        self.final_w = self._model_initialiser(key, dummy_x)\n",
    "        self.best_w = self._model_initialiser(key, dummy_x)\n",
    "        self.w = self._model_initialiser(key, dummy_x)\n",
    "\n",
    "\n",
    "    def _initialise_history(self):\n",
    "        \"\"\"Initialises history dictionary attribute\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The contents of the history dictionary are\n",
    "            - **detF** -- determinant of the Fisher information at the end of\n",
    "              each iteration\n",
    "            - **detC** -- determinant of the covariance of network outputs at\n",
    "              the end of each iteration\n",
    "            - **detinvC** -- determinant of the inverse covariance of network\n",
    "              outputs at the end of each iteration\n",
    "            - **Λ2** -- value of the covariance regularisation at the end of\n",
    "              each iteration\n",
    "            - **r** -- value of the regularisation coupling at the end of each\n",
    "              iteration\n",
    "            - **val_detF** -- determinant of the Fisher information of the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_detC** -- determinant of the covariance of network outputs\n",
    "              given the validation data at the end of each iteration\n",
    "            - **val_detinvC** -- determinant of the inverse covariance of\n",
    "              network outputs given the validation data at the end of each\n",
    "              iteration\n",
    "            - **val_Λ2** -- value of the covariance regularisation given the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_r** -- value of the regularisation coupling given the\n",
    "              validation data at the end of each iteration\n",
    "            - **max_detF** -- maximum value of the determinant of the Fisher\n",
    "              information on the validation data (if available)\n",
    "\n",
    "        \"\"\"\n",
    "        self.history = {\n",
    "            \"detF\": np.zeros((0,)),\n",
    "            \"detC\": np.zeros((0,)),\n",
    "            \"detinvC\": np.zeros((0,)),\n",
    "            \"Λ2\": np.zeros((0,)),\n",
    "            \"r\": np.zeros((0,)),\n",
    "            \"val_detF\": np.zeros((0,)),\n",
    "            \"val_detC\": np.zeros((0,)),\n",
    "            \"val_detinvC\": np.zeros((0,)),\n",
    "            \"val_Λ2\": np.zeros((0,)),\n",
    "            \"val_r\": np.zeros((0,)),\n",
    "            \"max_detF\": np.float32(0.)\n",
    "        }\n",
    "\n",
    "    def _set_history(self, results):\n",
    "        \"\"\"Places results from fitting into the history dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results : list\n",
    "            List of results from fitting procedure. These are:\n",
    "                - **detF** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  Fisher information, ``detF[:, 0]`` for training and\n",
    "                  ``detF[:, 1]`` for validation\n",
    "                - **detC** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  covariance of network outputs, ``detC[:, 0]`` for training\n",
    "                  and ``detC[:, 1]`` for validation\n",
    "                - **detinvC** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  inverse covariance of network outputs, ``detinvC[:, 0]`` for\n",
    "                  training and ``detinvC[:, 1]`` for validation\n",
    "                - **Λ2** *(float(n_iterations, 2))* -- value of the covariance\n",
    "                  regularisation, ``Λ2[:, 0]`` for training and ``Λ2[:, 1]``\n",
    "                  for validation\n",
    "                - **r** *(float(n_iterations, 2))* -- value of the\n",
    "                  regularisation coupling, ``r[:, 0]`` for training and\n",
    "                  ``r[:, 1]`` for validation\n",
    "\n",
    "        \"\"\"\n",
    "        keys = [\"detF\", \"detC\", \"detinvC\", \"Λ2\", \"r\"]\n",
    "        for result, key in zip(results, keys):\n",
    "            self.history[key] = np.hstack([self.history[key], result[:, 0]])\n",
    "            if self.validate:\n",
    "                self.history[f\"val_{key}\"] = np.hstack(\n",
    "                    [self.history[f\"val_{key}\"], result[:, 1]])\n",
    "\n",
    "    def _set_inputs(self, rng, max_iterations):\n",
    "        \"\"\"Builds list of inputs for the XLA compilable fitting routine\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,) or None\n",
    "            A stateless random number generator\n",
    "        max_iterations : int\n",
    "            Maximum number of iterations to run the fitting procedure for\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The list of inputs to the routine are\n",
    "            - **max_detF** *(float)* -- The maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far. This is zero if\n",
    "              not run before or the value from previous calls to ``fit``\n",
    "            - **best_w** *(list)* -- The value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix. This is the initial network parameter values if not run\n",
    "              before\n",
    "            - **detF** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the Fisher information matrix during each\n",
    "              iteration of fitting. If there is no validation (for simulation\n",
    "              on-the-fly for example) then this container has a shape of\n",
    "              ``(max_iterations, 1)``, otherwise validation values are stored\n",
    "              in ``detF[:, 1]``.\n",
    "            - **detC** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the covariance of network outputs during\n",
    "              each iteration of fitting. If there is no validation (for\n",
    "              simulation on-the-fly for example) then this container has a\n",
    "              shape of ``(max_iterations, 1)``, otherwise validation values are\n",
    "              stored in ``detC[:, 1]``.\n",
    "            - **detF** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the inverse covariance of network outputs\n",
    "              during each iteration of fitting. If there is no validation (for\n",
    "              simulation on-the-fly for example) then this container has a\n",
    "              shape of ``(max_iterations, 1)``, otherwise validation values are\n",
    "              stored in ``detinvC[:, 1]``.\n",
    "            - **Λ2** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the covariance regularisation during each iteration of\n",
    "              fitting. If there is no validation (for simulation on-the-fly for\n",
    "              example) then this container has a shape of\n",
    "              ``(max_iterations, 1)``, otherwise validation values are stored\n",
    "              in ``Λ2[:, 1]``.\n",
    "            - **r** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the regularisation coupling strength during each iteration of\n",
    "              fitting. If there is no validation (for simulation on-the-fly for\n",
    "              example) then this container has a shape of\n",
    "              ``(max_iterations, 1),`` otherwise validation values are stored\n",
    "              in ``r[:, 1]``.\n",
    "            - **counter** *(int)* -- Iteration counter used to note whether the\n",
    "              while loop reaches ``max_iterations``. If not, the history\n",
    "              objects (above) get truncated to length ``counter``. This starts\n",
    "              with value zero\n",
    "            - **patience_counter** *(int)* -- Counts the number of iterations\n",
    "              where there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix, used for early stopping. This starts\n",
    "              with value zero\n",
    "            - **state** *(:obj:state)* -- The current optimiser state used for\n",
    "              updating the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- A stateless random number generator which\n",
    "              gets updated on each iteration\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        ``rng`` is currently only used for on-the-fly simulation but could\n",
    "        easily be updated to allow for stochastic models\n",
    "        \"\"\"\n",
    "        if self.validate:\n",
    "            shape = (max_iterations, 2)\n",
    "        else:\n",
    "            shape = (max_iterations, 1)\n",
    "\n",
    "        return (\n",
    "            self.history[\"max_detF\"], self.best_w, np.zeros(shape),\n",
    "            np.zeros(shape), np.zeros(shape), np.zeros(shape), np.zeros(shape),\n",
    "            np.int32(0), np.int32(0), self.state, self.w, rng)\n",
    "\n",
    "    def fit(self, λ, ϵ, γ=1000., rng=None, patience=100, min_iterations=100,\n",
    "            max_iterations=int(1e5), print_rate=None, best=True):\n",
    "        \"\"\"Fitting routine for the IMNN\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        ϵ : float\n",
    "            Closeness criterion describing how close to the 1 the determinant\n",
    "            of the covariance (and inverse covariance) of the network outputs\n",
    "            is desired to be\n",
    "        rng : int(2,) or None, default=None\n",
    "            Stateless random number generator\n",
    "        patience : int, default=10\n",
    "            Number of iterations where there is no increase in the value of the\n",
    "            determinant of the Fisher information matrix, used for early\n",
    "            stopping\n",
    "        min_iterations : int, default=100\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "        max_iterations : int, default=int(1e5)\n",
    "            Maximum number of iterations to run the fitting procedure for\n",
    "        print_rate : int or None, default=None,\n",
    "            Number of iterations before updating the progress bar whilst\n",
    "            fitting. There is a performance hit from updating the progress bar\n",
    "            more often and there is a large performance hit from using the\n",
    "            progress bar at all. (Possible ``RET_CHECK`` failure if\n",
    "            ``print_rate`` is not ``None`` when using GPUs).\n",
    "            For this reason it is set to None as default\n",
    "        best : bool, default=True\n",
    "            Whether to set the network parameter attribute ``self.w`` to the\n",
    "            parameter values that obtained the maximum determinant of\n",
    "            the Fisher information matrix or the parameter values at the final\n",
    "            iteration of fitting\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        We are going to summarise the mean and variance of some random Gaussian\n",
    "        noise with 10 data points per example using a SimulatorIMNN. In this\n",
    "        case we are going to generate the simulations on-the-fly with a\n",
    "        simulator written in jax (from the examples directory). We will use\n",
    "        1000 simulations to estimate the covariance of the network outputs and\n",
    "        the derivative of the mean of the network outputs with respect to the\n",
    "        model parameters (Gaussian mean and variance) and generate the\n",
    "        simulations at a fiducial μ=0 and Σ=1. The network will be a stax model\n",
    "        with hidden layers of ``[128, 128, 128]`` activated with leaky relu and\n",
    "        outputting 2 summaries. Optimisation will be via Adam with a step size\n",
    "        of ``1e-3``. Rather arbitrarily we'll set the regularisation strength\n",
    "        and covariance identity constraint to λ=10 and ϵ=0.1 (these are\n",
    "        relatively unimportant for such an easy model).\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            import jax\n",
    "            import jax.numpy as np\n",
    "            from jax.experimental import stax, optimizers\n",
    "            from imnn import SimulatorIMNN\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "\n",
    "            n_s = 1000\n",
    "            n_d = 1000\n",
    "            n_params = 2\n",
    "            n_summaries = 2\n",
    "            input_shape = (10,)\n",
    "            simulator_args = {\"input_shape\": input_shape}\n",
    "            θ_fid = np.array([0., 1.])\n",
    "\n",
    "            def simulator(rng, θ):\n",
    "                return θ[0] + jax.random.normal(\n",
    "                    rng, shape=input_shape) * np.sqrt(θ[1])\n",
    "\n",
    "            model = stax.serial(\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(n_summaries))\n",
    "            optimiser = optimizers.adam(step_size=1e-3)\n",
    "\n",
    "            λ = 10.\n",
    "            ϵ = 0.1\n",
    "\n",
    "            model_key, fit_key = jax.random.split(rng)\n",
    "\n",
    "            imnn = SimulatorIMNN(\n",
    "                n_s=n_s, n_d=n_d, n_params=n_params, n_summaries=n_summaries,\n",
    "                input_shape=input_shape, θ_fid=θ_fid, model=model,\n",
    "                optimiser=optimiser, key_or_state=model_key,\n",
    "                simulator=simulator)\n",
    "\n",
    "            imnn.fit(λ, ϵ, rng=fit_key, min_iterations=1000, patience=250,\n",
    "                     print_rate=None)\n",
    "\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        A minimum number of interations should be be run before stopping based\n",
    "        on a maximum determinant of the Fisher information achieved since the\n",
    "        loss function has dual objectives. Since the determinant of the\n",
    "        covariance of the network outputs is forced to 1 quickly, this can be\n",
    "        at the detriment to the value of the determinant of the Fisher\n",
    "        information matrix early in the fitting procedure. For this reason\n",
    "        starting early stopping after the covariance has converged is advised.\n",
    "        This is not currently implemented but could be considered in the\n",
    "        future.\n",
    "\n",
    "        The best fit network parameter values are probably not the most\n",
    "        representative set of parameters when simulating on-the-fly since there\n",
    "        is a high chance of a statistically overly-informative set of data\n",
    "        being generated. Instead, if using :func:`~imnn.SimulatorIMNN.fit()`\n",
    "        consider using ``best=False`` which sets ``self.w=self.final_w`` which\n",
    "        are the network parameter values obtained in the last iteration. Also\n",
    "        consider using a larger ``patience`` value if using\n",
    "        :func:`~imnn.SimulatorIMNN.fit()` to overcome the fact that a flukish\n",
    "        high value for the determinant might have been obtained due to the\n",
    "        realisation of the dataset.\n",
    "\n",
    "        Due to some unusual thing, that I can't work out, there is a massive\n",
    "        performance hit when calling ``jax.jit(self._fit)`` compared with\n",
    "        directly decorating ``_fit`` with\n",
    "        ``@partial(jax.jit(static_argnums=0))``. Unfortunately this means\n",
    "        having to duplicate ``_fit`` to include a version where the loop\n",
    "        condition is decorated with a progress bar because the ``tqdm``\n",
    "        module cannot use a jitted tracer. If the progress bar is not used then\n",
    "        the fully decorated jitted ``_fit`` function is used and it is super\n",
    "        quick. Otherwise, just the body of the loop is jitted so that the\n",
    "        condition function can be decorated by the progress bar (at the\n",
    "        expense of a performance hit). I imagine that something can be improved\n",
    "        here.\n",
    "\n",
    "        There is a chance of a ``RET_CHECK`` failure when using the progress\n",
    "        bar on GPUs (this doesn't seem to be a problem on CPUs). If this is the\n",
    "        case then `print_rate=None` should be used\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        _fit:\n",
    "            Main fitting function implemented as a ``jax.lax.while_loop``\n",
    "        _fit_pbar:\n",
    "            Main fitting function as a ``jax.lax.while_loop`` with progress bar\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If any input has the wrong type\n",
    "        ValueError\n",
    "            If any input (except ``rng`` and ``print_rate``) are ``None``\n",
    "        ValueError\n",
    "            If ``rng`` has the wrong shape\n",
    "        ValueError\n",
    "            If ``rng`` is ``None`` but simulating on-the-fly\n",
    "        ValueError\n",
    "            If calling fit with ``print_rate=None`` after previous call with\n",
    "            ``print_rate`` as an integer value\n",
    "        ValueError\n",
    "            If calling fit with ``print_rate`` as an integer after previous\n",
    "            call with ``print_rate=None``\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        - ``rng`` is currently only used for on-the-fly simulation but could\n",
    "          easily be updated to allow for stochastic models\n",
    "        - Automatic detection of convergence based on value ``r`` when early\n",
    "          stopping can be started\n",
    "        \"\"\"\n",
    "\n",
    "        @jax.jit\n",
    "        def _fit(inputs):\n",
    "\n",
    "            return jax.lax.while_loop(\n",
    "                partial(self._fit_cond, patience=patience,\n",
    "                        max_iterations=max_iterations),\n",
    "                partial(self._fit, λ=λ, α=α, γ=γ, min_iterations=min_iterations),\n",
    "                inputs)\n",
    "\n",
    "        def _fit_pbar(inputs):\n",
    "\n",
    "            return jax.lax.while_loop(\n",
    "                progress_bar(max_iterations, patience, print_rate)(\n",
    "                    partial(self._fit_cond, patience=patience,\n",
    "                            max_iterations=max_iterations)),\n",
    "                jax.jit(\n",
    "                    partial(self._fit, λ=λ, α=α,\n",
    "                            min_iterations=min_iterations)),\n",
    "                inputs)\n",
    "\n",
    "        λ = _check_type(λ, float, \"λ\")\n",
    "        ϵ = _check_type(ϵ, float, \"ϵ\")\n",
    "        γ = _check_type(γ, float, \"γ\")\n",
    "        α = self.get_α(λ, ϵ)\n",
    "        patience = _check_type(patience, int, \"patience\")\n",
    "        min_iterations = _check_type(min_iterations, int, \"min_iterations\")\n",
    "        max_iterations = _check_type(max_iterations, int, \"max_iterations\")\n",
    "        best = _check_boolean(best, \"best\")\n",
    "        if self.simulate and (rng is None):\n",
    "            raise ValueError(\"`rng` is necessary when simulating.\")\n",
    "        rng = _check_input(rng, (2,), \"rng\", allow_None=True)\n",
    "        inputs = self._set_inputs(rng, max_iterations)\n",
    "        if print_rate is None:\n",
    "            if self._run_with_pbar:\n",
    "                raise ValueError(\n",
    "                    \"Cannot run IMNN without progress bar after running \" +\n",
    "                    \"with progress bar. Either set `print_rate` to an int \" +\n",
    "                    \"or reinitialise the IMNN.\")\n",
    "            else:\n",
    "                self._run_without_pbar = True\n",
    "                results = _fit(inputs)\n",
    "        else:\n",
    "            if self._run_without_pbar:\n",
    "                raise ValueError(\n",
    "                    \"Cannot run IMNN with progress bar after running \" +\n",
    "                    \"without progress bar. Either set `print_rate` to None \" +\n",
    "                    \"or reinitialise the IMNN.\")\n",
    "            else:\n",
    "                print_rate = _check_type(print_rate, int, \"print_rate\")\n",
    "                self._run_with_pbar = True\n",
    "                results = _fit_pbar(inputs)\n",
    "        self.history[\"max_detF\"] = results[0]\n",
    "        self.best_w = results[1]\n",
    "        self._set_history(\n",
    "            (results[2][:results[7]],\n",
    "             results[3][:results[7]],\n",
    "             results[4][:results[7]],\n",
    "             results[5][:results[7]],\n",
    "             results[6][:results[7]]))\n",
    "        if len(results) == 12:\n",
    "            self.state = results[-3]\n",
    "        self.final_w = results[-2] #self._get_parameters(self.state)\n",
    "        if best:\n",
    "            w = self.best_w\n",
    "        else:\n",
    "            w = self.final_w\n",
    "        self.set_F_statistics(w, key=rng)\n",
    "\n",
    "    def _get_fitting_keys(self, rng):\n",
    "        \"\"\"Generates random numbers for simulation generation if needed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,) or None\n",
    "            A random number generator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int(2,), int(2,), int(2,) or None, None, None:\n",
    "            A new random number generator and random number generators for\n",
    "            training and validation, or empty values\n",
    "        \"\"\"\n",
    "        if rng is not None:\n",
    "            return jax.random.split(rng, num=3)\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    def get_α(self, λ, ϵ):\n",
    "        \"\"\"Calculate rate parameter for regularisation from closeness criterion\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        λ : float\n",
    "            coupling strength of the regularisation\n",
    "        ϵ : float\n",
    "            closeness criterion describing how close to the 1 the determinant\n",
    "            of the covariance (and inverse covariance) of the network outputs\n",
    "            is desired to be\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            The steepness of the tanh-like function (or rate) which determines\n",
    "            how fast the determinant of the covariance of the network outputs\n",
    "            should be pushed to 1\n",
    "        \"\"\"\n",
    "        return - math.log(ϵ * (λ - 1.) + ϵ ** 2. / (1 + ϵ)) / ϵ\n",
    "\n",
    "    def _fit(self, inputs, λ=None, α=None, γ=None,  min_iterations=None):\n",
    "        \"\"\"Single iteration fitting algorithm\n",
    "\n",
    "        This function performs the network parameter updates first getting\n",
    "        any necessary random number generators for simulators and then\n",
    "        extracting the network parameters from the state. These parameters\n",
    "        are used to calculate the gradient with respect to the network\n",
    "        parameters of the loss function (see _IMNN class docstrings).\n",
    "        Once the loss function is calculated the gradient is then used to\n",
    "        update the network parameters via the optimiser function and the\n",
    "        current iterations statistics are saved to the history arrays. If\n",
    "        validation is used (recommended for ``GradientIMNN`` and\n",
    "        ``NumericalGradientIMNN``) then all necessary statistics to\n",
    "        calculate the loss function are calculated and pushed to the\n",
    "        history arrays.\n",
    "\n",
    "        The ``patience_counter`` is increased if the value of determinant\n",
    "        of the Fisher information matrix does not increase over the\n",
    "        previous iterations upto ``patience`` number of iterations at which\n",
    "        point early stopping occurs, but only if the number of iterations\n",
    "        so far performed is greater than a specified ``min_iterations``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of the\n",
    "              Fisher information matrix calculated so far\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the Fisher information matrix\n",
    "            - **detC** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the determinant of the covariance of network\n",
    "              outputs\n",
    "            - **detinvC** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the inverse covariance of network outputs\n",
    "            - **Λ2** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the covariance regularisation\n",
    "            - **r** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the regularisation coupling strength\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **patience_counter** *(int)* -- Number of iterations where there\n",
    "              is no increase in the value of the determinant of the Fisher\n",
    "              information matrix\n",
    "            - **state** *(:obj: state)* -- Optimiser state used for updating\n",
    "              the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- Stateless random number generator\n",
    "\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Rate parameter for regularisation coupling\n",
    "        min_iterations : int\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            loop variables (described in Parameters)\n",
    "        \"\"\"\n",
    "\n",
    "        max_detF, best_w, detF, detC, detinvC, Λ2, r, \\\n",
    "            counter, patience_counter, state, w, rng = inputs\n",
    "        rng, training_key, validation_key = self._get_fitting_keys(rng)\n",
    "\n",
    "\n",
    "        grad, results = jax.grad(\n",
    "            self._get_loss, argnums=0, has_aux=True)(w, λ, α, γ, training_key)\n",
    "\n",
    "\n",
    "        updates, state = self._update(grad, state)\n",
    "\n",
    "        w = optax.apply_updates(w, updates) # UPDATE PARAMS\n",
    "\n",
    "        detF, detC, detinvC, Λ2, r = self._update_history(\n",
    "            results, (detF, detC, detinvC, Λ2, r), counter, 0)\n",
    "        if self.validate:\n",
    "            F, C, invC, *_ = self._get_F_statistics(\n",
    "                w, key=validation_key, validate=True)\n",
    "            _Λ2 = self._get_regularisation(C, invC)\n",
    "            _r = self._get_regularisation_strength(_Λ2, λ, α)\n",
    "            results = (F, C, invC, _Λ2, _r)\n",
    "            detF, detC, detinvC, Λ2, r = self._update_history(\n",
    "                results, (detF, detC, detinvC, Λ2, r), counter, 1)\n",
    "        _detF = np.linalg.det(results[0])\n",
    "        patience_counter, counter, _, max_detF, __, best_w = \\\n",
    "            jax.lax.cond(\n",
    "                np.greater(_detF, max_detF),\n",
    "                self._update_loop_vars,\n",
    "                lambda inputs: self._check_loop_vars(inputs, min_iterations),\n",
    "                (patience_counter, counter, _detF, max_detF, w, best_w))\n",
    "        return (max_detF, best_w, detF, detC, detinvC, Λ2, r,\n",
    "                counter + np.int32(1), patience_counter, state, w, rng)\n",
    "\n",
    "    def _fit_cond(self, inputs, patience, max_iterations):\n",
    "        \"\"\"Stopping condition for the fitting loop\n",
    "\n",
    "        The stopping conditions due to reaching ``max_iterations`` or the\n",
    "        patience counter reaching ``patience`` due to ``patience_counter``\n",
    "        number of iterations without increasing the determinant of the\n",
    "        Fisher information matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of the\n",
    "              Fisher information matrix calculated so far\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the Fisher information matrix\n",
    "            - **detC** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the determinant of the covariance of network\n",
    "              outputs\n",
    "            - **detinvC** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the inverse covariance of network outputs\n",
    "            - **Λ2** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the covariance regularisation\n",
    "            - **r** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the regularisation coupling strength\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **patience_counter** *(int)* -- Number of iterations where there\n",
    "              is no increase in the value of the determinant of the Fisher\n",
    "              information matrix\n",
    "            - **state** *(:obj: state)* -- Optimiser state used for updating\n",
    "              the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- Stateless random number generator\n",
    "\n",
    "        patience : int\n",
    "            Number of iterations to stop the fitting when there is no increase\n",
    "            in the value of the determinant of the Fisher information matrix\n",
    "        max_iterations : int\n",
    "        Maximum number of iterations to run the fitting procedure for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool:\n",
    "            True if either the ``patience_counter`` has not reached the\n",
    "            ``patience`` criterion or if the ``counter`` has not reached\n",
    "            ``max_iterations``\n",
    "        \"\"\"\n",
    "        return np.logical_and(\n",
    "            np.less(inputs[-4], patience),\n",
    "            np.less(inputs[-5], max_iterations))\n",
    "\n",
    "    def _update_loop_vars(self, inputs):\n",
    "        \"\"\"Updates input parameters if ``max_detF`` is increased\n",
    "\n",
    "        If the determinant of the Fisher information matrix calculated\n",
    "        in a given iteration is larger than the ``max_detF`` calculated\n",
    "        so far then the ``patience_counter`` is reset to zero and the\n",
    "        ``max_detF`` is replaced with the current value of ``detF`` and\n",
    "        the network parameters in this iteration replace the previous\n",
    "        parameters which obtained the highest determinant of the Fisher\n",
    "        information, ``best_w``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **patience_counter** *(int)* -- Number of iterations where\n",
    "              there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant\n",
    "              of the Fisher information matrix\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far\n",
    "            - **w** *(list)* -- Value of the network parameters which in\n",
    "              current iteration\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            (described in Parameters)\n",
    "        \"\"\"\n",
    "        patience_counter, counter, detF, max_detF, w, best_w = inputs\n",
    "        return (np.int32(0), counter, detF, detF, w, w)\n",
    "\n",
    "    def _check_loop_vars(self, inputs, min_iterations):\n",
    "        \"\"\"Updates ``patience_counter`` if ``max_detF`` not increased\n",
    "\n",
    "        If the determinant of the Fisher information matrix calculated\n",
    "        in a given iteration is not larger than the ``max_detF``\n",
    "        calculated so far then the ``patience_counter`` is increased by\n",
    "        one as long as the number of iterations is greater than the\n",
    "        minimum number of iterations that should be run.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **patience_counter** *(int)* -- Number of iterations where\n",
    "              there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant\n",
    "              of the Fisher information matrix\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far\n",
    "            - **w** *(list)* -- Value of the network parameters which in\n",
    "              current iteration\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "        min_iterations : int\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            (described in Parameters)\n",
    "        \"\"\"\n",
    "        patience_counter, counter, detF, max_detF, w, best_w = inputs\n",
    "        patience_counter = jax.lax.cond(\n",
    "            np.greater(counter, min_iterations),\n",
    "            lambda patience_counter: patience_counter + np.int32(1),\n",
    "            lambda patience_counter: patience_counter,\n",
    "            patience_counter)\n",
    "        return (patience_counter, counter, detF, max_detF, w, best_w)\n",
    "\n",
    "    def _update_history(self, inputs, history, counter, ind):\n",
    "        \"\"\"Puts current fitting statistics into history arrays\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **_Λ2** *(float)* -- Covariance regularisation\n",
    "                - **_r** *(float)* -- Regularisation coupling strength\n",
    "        history : tuple\n",
    "            History arrays containing fitting statistics for each iteration\n",
    "                - **detF** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Determinant of the Fisher\n",
    "                  information matrix\n",
    "                - **detC** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Determinant of the covariance\n",
    "                  of network outputs\n",
    "                - **detinvC** *(float(max_iterations, 1)\n",
    "                  or float(max_iterations, 2))* -- Determinant of the inverse\n",
    "                  covariance of network outputs\n",
    "                - **Λ2** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Covariance regularisation\n",
    "                - **r** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Regularisation coupling\n",
    "                  strength\n",
    "        counter : int\n",
    "            Current iteration to insert a single iteration statistics into the\n",
    "            history\n",
    "        ind : int\n",
    "            Values of either 0 (fitting) or 1 (validation) to separate the\n",
    "            fitting and validation historys\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the Fisher information matrix\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the covariance of network outputs\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the inverse covariance of network\n",
    "            outputs\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the covariance regularisation\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the regularisation coupling strength\n",
    "        \"\"\"\n",
    "        F, C, invC, _Λ2, _r = inputs\n",
    "        detF, detC, detinvC, Λ2, r = history\n",
    "        detF = jax.ops.index_update(\n",
    "            detF,\n",
    "            jax.ops.index[counter, ind],\n",
    "            np.linalg.det(F))\n",
    "        detC = jax.ops.index_update(\n",
    "            detC,\n",
    "            jax.ops.index[counter, ind],\n",
    "            np.linalg.det(C))\n",
    "        detinvC = jax.ops.index_update(\n",
    "            detinvC,\n",
    "            jax.ops.index[counter, ind],\n",
    "            np.linalg.det(invC))\n",
    "        Λ2 = jax.ops.index_update(\n",
    "            Λ2,\n",
    "            jax.ops.index[counter, ind],\n",
    "            _Λ2)\n",
    "        r = jax.ops.index_update(\n",
    "            r,\n",
    "            jax.ops.index[counter, ind],\n",
    "            _r)\n",
    "        return detF, detC, detinvC, Λ2, r\n",
    "\n",
    "    def _slogdet(self, matrix):\n",
    "        \"\"\"Combined summed logarithmic determinant\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix : float(n, n)\n",
    "            An n x n matrix to calculate the summed logarithmic determinant of\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            The summed logarithmic determinant multiplied by its sign\n",
    "        \"\"\"\n",
    "        lndet = np.linalg.slogdet(matrix)\n",
    "        return lndet[0] * lndet[1]\n",
    "\n",
    "    def _construct_derivatives(self, derivatives):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        An empty directive in ``_IMNN``, ``SimulatorIMNN`` and ``GradientIMNN``\n",
    "        but necessary to construct correct shaped derivatives when using\n",
    "        ``NumericalGradientIMNN``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivatives of the network ouputs with respect to the model\n",
    "            parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The derivatives of the network ouputs with respect to the model\n",
    "            parameters\n",
    "        \"\"\"\n",
    "        return derivatives\n",
    "\n",
    "    def set_F_statistics(self, w=None, key=None, validate=True):\n",
    "        \"\"\"Set necessary attributes for calculating score compressed summaries\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=True\n",
    "            Whether to calculate Fisher information using the validation set\n",
    "        \"\"\"\n",
    "        if validate and ((not self.validate) and (not self.simulate)):\n",
    "            validate = False\n",
    "        if w is not None:\n",
    "            self.w = w\n",
    "        self.F, self.C, self.invC, self.dμ_dθ, self.μ, self.F_loss = \\\n",
    "            self._get_F_statistics(key=key, validate=validate)\n",
    "        self.invF = np.linalg.inv(self.F)\n",
    "\n",
    "    def _get_F_statistics(self, w=None, key=None, validate=False):\n",
    "        \"\"\"Calculates the Fisher information and returns all statistics used\n",
    "\n",
    "        First gets the summaries and derivatives and then uses them to\n",
    "        calculate the Fisher information matrix from the outputs and return all\n",
    "        the necessary constituents to calculate the Fisher information (which)\n",
    "        are needed for the score compression or the regularisation of the loss\n",
    "        function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=True\n",
    "            Whether to calculate Fisher information using the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            - **F** *(float(n_params, n_params))* -- Fisher information matrix\n",
    "            - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "              network outputs\n",
    "            - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "              covariance of network outputs\n",
    "            - **dμ_dθ** *(float(n_summaries, n_params))* -- The derivative of\n",
    "              the mean of network outputs with respect to model parameters\n",
    "            - **μ** *(float(n_summaries,))* -- The mean of the network outputs\n",
    "\n",
    "        \"\"\"\n",
    "        if w is None:\n",
    "            w = self.w\n",
    "        summaries, derivatives = self.get_summaries(\n",
    "            w=w, key=key, validate=validate)\n",
    "        return self._calculate_F_statistics(summaries, derivatives)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _calculate_F_statistics(self, summaries, derivatives):\n",
    "        \"\"\"Calculates the Fisher information matrix from network outputs\n",
    "\n",
    "        If the numerical derivative is being calculated then the derivatives\n",
    "        are first constructed. If the mean is to be returned (for use in score\n",
    "        compression), this is calculated and pushed to the results tuple.\n",
    "        Then the covariance of the summaries is taken and inverted and the mean\n",
    "        of the derivative of network summaries with respect to the model\n",
    "        parameters is found and these are used to calculate the Gaussian form\n",
    "        of the Fisher information matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summaries : float(n_s, n_summaries)\n",
    "            The network outputs\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivative of the network outputs wrt the model parameters.\n",
    "            Note that when ``NumericalGradientIMNN`` is being used the shape is\n",
    "            ``float(n_d, 2, n_params, n_summaries)`` which is then constructed\n",
    "            into the the numerical derivative in ``_construct_derivatives``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            - **F** *(float(n_params, n_params))* -- Fisher information matrix\n",
    "            - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "              network outputs\n",
    "            - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "              covariance of network outputs\n",
    "            - **dμ_dθ** *(float(n_summaries, n_params))* -- The derivative of\n",
    "              the mean of network outputs with respect to model parameters\n",
    "            - **μ** *(float(n_summaries))* -- The mean of the\n",
    "              network outputs\n",
    "        \"\"\"\n",
    "        derivatives = self._construct_derivatives(derivatives)\n",
    "        μ = np.mean(summaries, axis=0)\n",
    "        C = np.cov(summaries, rowvar=False)\n",
    "        if self.n_summaries == 1:\n",
    "            C = np.array([[C]])\n",
    "\n",
    "        invC = np.linalg.inv(C)\n",
    "\n",
    "        if self.no_invC:\n",
    "            invC_loss = np.eye(self.n_summaries)\n",
    "        else:\n",
    "            invC_loss = invC\n",
    "        dμ_dθ = np.mean(derivatives, axis=0)\n",
    "        F = np.einsum(\"ij,ik,kl->jl\", dμ_dθ, invC, dμ_dθ)\n",
    "\n",
    "        F_loss = np.einsum(\"ij,ik,kl->jl\", dμ_dθ, invC_loss, dμ_dθ)\n",
    "        return (F, C, invC, dμ_dθ, μ, F_loss)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _get_regularisation_strength(self, Λ2, λ, α):\n",
    "        \"\"\"Coupling strength of the regularisation (amplified sigmoid)\n",
    "\n",
    "        To dynamically turn off the regularisation when the scale of the\n",
    "        covariance is set to approximately the identity matrix, a smooth\n",
    "        sigmoid conditional on the value of the regularisation is used. The\n",
    "        rate, α, is calculated from a closeness condition of the covariance\n",
    "        (and the inverse covariance) to the identity matrix using ``get_α``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Λ2 : float\n",
    "            Covariance regularisation\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Calculate rate parameter for regularisation from ϵ criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Smooth, dynamic regularisation strength\n",
    "        \"\"\"\n",
    "        return λ * Λ2 / (Λ2 + np.exp(-α * Λ2))\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _get_regularisation(self, C, invC):\n",
    "        \"\"\"Difference of the covariance (and its inverse) from identity\n",
    "\n",
    "        The negative logarithm of the determinant of the Fisher information\n",
    "        matrix needs to be regularised to fix the scale of the network outputs\n",
    "        since any linear rescaling of a sufficient statistic is also a\n",
    "        sufficient statistic. We choose to fix this scale by constraining the\n",
    "        covariance of network outputs as\n",
    "\n",
    "        .. math::\n",
    "            \\\\Lambda_2 = ||\\\\bf{C}-\\\\bf{I}|| + ||\\\\bf{C}^{-1}-\\\\bf{I}||\n",
    "\n",
    "        One benefit of choosing this constraint is that it forces the\n",
    "        covariance to be approximately parameter independent which justifies\n",
    "        choosing the covariance independent Gaussian Fisher information.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        C : float(n_summaries, n_summaries)\n",
    "            Covariance of the network ouputs\n",
    "        invC : float(n_summaries, n_summaries)\n",
    "            Inverse covariance of the network ouputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Regularisation loss terms for the distance of the covariance and\n",
    "            its determinant from the identity matrix\n",
    "        \"\"\"\n",
    "        if self.no_invC:\n",
    "            if self.evidence:\n",
    "                #reg = -(np.log(np.linalg.det(C)) - np.trace(C) + self.n_params)\n",
    "                reg = np.trace(C)\n",
    "            else:\n",
    "                reg = np.linalg.norm(C - np.eye(self.n_summaries))\n",
    "\n",
    "        else:\n",
    "            reg = np.linalg.norm(C - np.eye(self.n_summaries)) + \\\n",
    "                np.linalg.norm(invC - np.eye(self.n_summaries))\n",
    "        return reg\n",
    "\n",
    "    def _get_loss(self, w, λ, α, γ, key=None):\n",
    "        \"\"\"Calculates the loss function and returns auxillary variables\n",
    "\n",
    "        First gets the summaries and derivatives and then uses them to\n",
    "        calculate the loss function. This function is separated to be able to\n",
    "        use ``jax.grad`` directly rather than calculating the derivative of the\n",
    "        summaries as is done with ``_AggregatedIMNN``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Calculate rate parameter for regularisation from ϵ criterion\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Value of the regularised loss function\n",
    "        tuple:\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **Λ2** *(float)* -- Covariance regularisation\n",
    "                - **r** *(float)* -- Regularisation coupling strength\n",
    "\n",
    "        \"\"\"\n",
    "        summaries, derivatives = self.get_summaries(w=w, key=key)\n",
    "        return self._calculate_loss(summaries, derivatives, λ, α, γ)\n",
    "\n",
    "    def _calculate_loss(self, summaries, derivatives, λ, α, γ):\n",
    "        \"\"\"Calculates the loss function from network summaries and derivatives\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summaries : float(n_s, n_summaries)\n",
    "            The network outputs\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivative of the network outputs wrt the model parameters.\n",
    "            Note that when ``NumericalGradientIMNN`` is being used the shape is\n",
    "            ``float(n_d, 2, n_params, n_summaries)`` which is then constructed\n",
    "            into the the numerical derivative in ``_construct_derivatives``.\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Calculate rate parameter for regularisation from ϵ criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Value of the regularised loss function\n",
    "        tuple:\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **Λ2** *(float)* -- Covariance regularisation\n",
    "                - **r** *(float)* -- Regularisation coupling strength\n",
    "\n",
    "        \"\"\"\n",
    "        F, C, invC, dμ_dθ, _, F_loss = self._calculate_F_statistics(\n",
    "            summaries, derivatives)\n",
    "        lndetF = self._slogdet(F_loss)\n",
    "        Λ2 = self._get_regularisation(C, invC)\n",
    "        if self.do_reg:\n",
    "            r = self._get_regularisation_strength(Λ2, λ, α)\n",
    "        else:\n",
    "            r = γ*0.5\n",
    "        return - lndetF + r * Λ2, (F, C, invC, Λ2, r)\n",
    "\n",
    "    def get_summaries(self, w=None, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "        \"\"\"\n",
    "        raise ValueError(\"`get_summaries` not implemented\")\n",
    "\n",
    "    def get_estimate(self, d):\n",
    "        \"\"\"Calculate score compressed parameter estimates from network outputs\n",
    "\n",
    "        Using score compression we can get parameter estimates under the\n",
    "        transformation\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{\\\\boldsymbol{\\\\theta}}_\\\\alpha=\\\\theta^{\\\\rm{fid}}_\\\\alpha+\n",
    "            \\\\bf{F}^{-1}_{\\\\alpha\\\\beta}\\\\frac{\\\\partial\\\\mu_i}{\\\\partial\n",
    "            \\\\theta_\\\\beta}\\\\bf{C}^{-1}_{ij}(x(\\\\bf{w}, \\\\bf{d})-\\\\mu)_j\n",
    "\n",
    "        where :math:`x_j` is the :math:`j` output of the network with network\n",
    "        parameters :math:`\\\\bf{w}` and input data :math:`\\\\bf{d}`.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        Assuming that an IMNN has been fit (as in the example in\n",
    "        :py:meth:`imnn.imnn._imnn.IMNN.fit`) then we can obtain a\n",
    "        pseudo-maximum likelihood estimate of some target data (which is\n",
    "        generated with parameter values μ=1, Σ=2) using\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            rng, target_key = jax.random.split(rng)\n",
    "            target_data = model_simulator(target_key, np.array([1., 2.]))\n",
    "\n",
    "            imnn.get_estimate(target_data)\n",
    "            >>> DeviceArray([0.1108716, 1.7881424], dtype=float32)\n",
    "\n",
    "        The one standard deviation uncertainty on these parameter estimates\n",
    "        (assuming the fiducial is at the maximum-likelihood estimate - which we\n",
    "        know it isn't here) estimated by the square root of the inverse Fisher\n",
    "        information matrix is\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            np.sqrt(np.diag(imnn.invF))\n",
    "            >>> DeviceArray([0.31980422, 0.47132865], dtype=float32)\n",
    "\n",
    "        Note that we can compare the values estimated by the IMNN to the value\n",
    "        of the mean and the variance of the target data itself, which is what\n",
    "        the IMNN should be summarising\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            np.mean(target_data)\n",
    "            >>> DeviceArray(0.10693721, dtype=float32)\n",
    "\n",
    "            np.var(target_data)\n",
    "            >>> DeviceArray(1.70872, dtype=float32)\n",
    "\n",
    "        Note that batches of data can be summarised at once using\n",
    "        ``get_estimate``. In this example we will draw 10 different values of μ\n",
    "        from between :math:`-10 < \\\\mu < 10` and 10 different values of Σ from\n",
    "        between :math:`0 < \\\\Sigma < 10` and generate a batch of 10 different\n",
    "        input data which we can summarise using the IMNN.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            rng, mean_keys, var_keys = jax.random.split(rng, num=3)\n",
    "\n",
    "            mean_vals = jax.random.uniform(\n",
    "                mean_keys, minval=-10, maxval=10, shape=(10,))\n",
    "            var_vals = jax.random.uniform(\n",
    "                var_keys, minval=0, maxval=10, shape=(10,))\n",
    "\n",
    "            np.stack([mean_vals, var_vals], -1)\n",
    "            >>> DeviceArray([[ 3.8727236,  1.6727388],\n",
    "                             [-3.1113386,  8.14554  ],\n",
    "                             [ 9.87299  ,  1.4134324],\n",
    "                             [ 4.4837523,  1.5812075],\n",
    "                             [-9.398947 ,  3.5737753],\n",
    "                             [-2.0789695,  9.978279 ],\n",
    "                             [-6.2622285,  6.828809 ],\n",
    "                             [ 4.6470118,  6.0823894],\n",
    "                             [ 5.7369494,  8.856505 ],\n",
    "                             [ 4.248898 ,  5.114669 ]], dtype=float32)\n",
    "\n",
    "            batch_target_keys = np.array(jax.random.split(rng, num=10))\n",
    "\n",
    "            batch_target_data = jax.vmap(model_simulator)(\n",
    "                batch_target_keys, (mean_vals, var_vals))\n",
    "\n",
    "            imnn.get_estimate(batch_target_data)\n",
    "            >>> DeviceArray([[ 4.6041985,  8.344688 ],\n",
    "                             [-3.5172062,  7.7219954],\n",
    "                             [13.229679 , 23.668312 ],\n",
    "                             [ 5.745726 , 10.020965 ],\n",
    "                             [-9.734651 , 21.076218 ],\n",
    "                             [-1.8083427,  6.1901293],\n",
    "                             [-8.626409 , 18.894459 ],\n",
    "                             [ 5.7684307,  9.482665 ],\n",
    "                             [ 6.7861238, 14.128591 ],\n",
    "                             [ 4.900367 ,  9.472563 ]], dtype=float32)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : float(None, input_shape)\n",
    "            Input data to be compressed to score compressed parameter estimates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(None, n_params):\n",
    "            Score compressed parameter estimates\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        single_element:\n",
    "            Returns a single score compressed summary\n",
    "        multiple_elements:\n",
    "            Returns a batch of score compressed summaries\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the Fisher statistics are not set after running ``fit`` or\n",
    "            ``set_F_statistics``.\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        - Do proper checking on input shape (should just be a call to\n",
    "          ``_check_input``)\n",
    "        \"\"\"\n",
    "        @jax.jit\n",
    "        def single_element(d):\n",
    "            \"\"\"Returns a single score compressed summary\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            d : float(input_shape)\n",
    "                Input data to be compressed to score compressed parameter\n",
    "                estimate\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float(n_params,):\n",
    "                Score compressed parameter estimate\n",
    "            \"\"\"\n",
    "            return self.θ_fid + np.einsum(\n",
    "                \"ij,kj,kl,l->i\",\n",
    "                self.invF,\n",
    "                self.dμ_dθ,\n",
    "                self.invC,\n",
    "                self.model(self.w, d) - self.μ)\n",
    "\n",
    "        @jax.jit\n",
    "        def multiple_elements(d):\n",
    "            \"\"\"Returns a batch of score compressed summaries\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            d : float(None, input_shape)\n",
    "                Input data to be compressed to score compressed parameter\n",
    "                estimates\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float(None, n_params):\n",
    "                Score compressed parameter estimates\n",
    "\n",
    "            Methods\n",
    "            -------\n",
    "            fn:\n",
    "                Returns the output of the evaluated model\n",
    "            \"\"\"\n",
    "            def fn(d):\n",
    "                \"\"\"Returns the output of the evaluated model\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                d : float(input_shape)\n",
    "                    Input data to the neural network\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                float(None, n_summaries):\n",
    "                    Neural network output\n",
    "                \"\"\"\n",
    "                return self.model(self.w, d)\n",
    "            return self.θ_fid + np.einsum(\n",
    "                \"ij,kj,kl,ml->mi\",\n",
    "                self.invF,\n",
    "                self.dμ_dθ,\n",
    "                self.invC,\n",
    "                jax.vmap(fn)(d) - self.μ)\n",
    "\n",
    "        _check_statistics_set(self.invF, self.dμ_dθ, self.invC, self.μ)\n",
    "        # check shape: array or graph ?\n",
    "        if self.dummy_input is None:\n",
    "          if len(d.shape) == 1:\n",
    "              return single_element(d)\n",
    "          else:\n",
    "              return multiple_elements(d)\n",
    "        else:\n",
    "            return single_element(d)\n",
    "\n",
    "    def _setup_plot(self, ax=None, expected_detF=None, figsize=(5, 15)):\n",
    "        \"\"\"Builds axes for history plot\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax : mpl.axes or None, default=None\n",
    "            An axes object of predefined axes to be labelled\n",
    "        expected_detF : float or None, default=None\n",
    "            Value of the expected determinant of the Fisher information to plot\n",
    "            a horizontal line at to check fitting progress\n",
    "        figsize : tuple, default=(5, 15)\n",
    "            The size of the figure to be produced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mpl.axes:\n",
    "            An axes object of labelled axes\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(3, 1, sharex=True, figsize=figsize)\n",
    "            plt.subplots_adjust(hspace=0.05)\n",
    "        ax = [x for x in ax] + [ax[2].twinx()]\n",
    "        if expected_detF is not None:\n",
    "            ax[0].axhline(expected_detF, linestyle=\"dashed\", color=\"black\")\n",
    "        ax[0].set_ylabel(r\"$|{\\bf F}|$\")\n",
    "        ax[1].axhline(1, linestyle=\"dashed\", color=\"black\")\n",
    "        ax[1].set_ylabel(r\"$|{\\bf C}|$ and $|{\\bf C}^{-1}|$\")\n",
    "        ax[1].set_yscale(\"log\")\n",
    "        ax[2].set_xlabel(\"Number of iterations\")\n",
    "        ax[2].set_ylabel(r\"$\\Lambda_2$\")\n",
    "        ax[3].set_ylabel(r\"$r$\")\n",
    "        return ax\n",
    "\n",
    "    def plot(self, ax=None, expected_detF=None, colour=\"C0\", figsize=(5, 15),\n",
    "             label=\"\", filename=None, ncol=1):\n",
    "        \"\"\"Plot fitting history\n",
    "\n",
    "        Plots a three panel vertical plot with the determinant of the Fisher\n",
    "        information matrix in the first sublot, the covariance and the inverse\n",
    "        covariance in the second and the regularisation term and the\n",
    "        regularisation coupling strength in the final subplot.\n",
    "\n",
    "        A predefined axes can be passed to fill, and these axes can be\n",
    "        decorated via a call to ``_setup_plot`` (for horizonal plots for\n",
    "        example).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        Assuming that an IMNN has been fit (as in the example in\n",
    "        :py:meth:`imnn.imnn._imnn.IMNN.fit`) then we can make a training plot\n",
    "        of the history by simply running\n",
    "\n",
    "        .. code-block::\n",
    "\n",
    "            imnn.fit(expected_detF=50, filename=\"history_plot.png\")\n",
    "\n",
    "        .. image:: /_images/history_plot.png\n",
    "\n",
    "        Note we know the analytic value of the determinant of the Fisher\n",
    "        information for this problem (:math:`|\\\\bf{F}|=50`) so we can add this\n",
    "        line to the plot too, and save the output as a png named\n",
    "        ``history_plot``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax : mpl.axes or None, default=None\n",
    "            An axes object of predefined axes to be labelled\n",
    "        expected_detF : float or None, default=None\n",
    "            Value of the expected determinant of the Fisher information to plot\n",
    "            a horizontal line at to check fitting progress\n",
    "        colour : str or rgb/a value or list, default=\"C0\"\n",
    "            Colour to plot the lines\n",
    "        figsize : tuple, default=(5, 15)\n",
    "            The size of the figure to be produced\n",
    "        label : str, default=\"\"\n",
    "            Name to add to description in legend\n",
    "        filename : str or None, default=None\n",
    "            Filename to save plot to\n",
    "        ncol : int, default=1\n",
    "            Number of columns to have in the legend\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mpl.axes:\n",
    "            An axes object of the filled plot\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = self._setup_plot(expected_detF=expected_detF, figsize=figsize)\n",
    "        ax[0].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[0].plot(self.history[\"detF\"], color=colour,\n",
    "                   label=r\"{} $|F|$ (training)\".format(label))\n",
    "        ax[1].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[1].plot(self.history[\"detC\"], color=colour,\n",
    "                   label=r\"{} $|C|$ (training)\".format(label))\n",
    "        ax[1].plot(self.history[\"detinvC\"], linestyle=\"dotted\", color=colour,\n",
    "                   label=label + r\" $|C^{-1}|$ (training)\")\n",
    "        ax[3].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[2].plot(self.history[\"Λ2\"], color=colour,\n",
    "                   label=r\"{} $\\Lambda_2$ (training)\".format(label))\n",
    "        ax[3].plot(self.history[\"r\"], color=colour, linestyle=\"dashed\",\n",
    "                   label=r\"{} $r$ (training)\".format(label))\n",
    "        if self.validate:\n",
    "            ax[0].plot(self.history[\"val_detF\"], color=colour,\n",
    "                       label=r\"{} $|F|$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[1].plot(self.history[\"val_detC\"], color=colour,\n",
    "                       label=r\"{} $|C|$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[1].plot(self.history[\"val_detinvC\"],\n",
    "                       color=colour,\n",
    "                       label=label + r\" $|C^{-1}|$ (validation)\",\n",
    "                       linestyle=\"dashdot\")\n",
    "            ax[2].plot(self.history[\"val_Λ2\"], color=colour,\n",
    "                       label=r\"{} $\\Lambda_2$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[3].plot(self.history[\"val_r\"], color=colour,\n",
    "                       label=r\"{} $r$ (validation)\".format(label),\n",
    "                       linestyle=\"dashdot\")\n",
    "        h1, l1 = ax[2].get_legend_handles_labels()\n",
    "        h2, l2 = ax[3].get_legend_handles_labels()\n",
    "        ax[0].legend(bbox_to_anchor=(1.0, 1.0), frameon=False, ncol=ncol)\n",
    "        ax[1].legend(frameon=False, bbox_to_anchor=(1.0, 1.0), ncol=ncol * 2)\n",
    "        ax[3].legend(h1 + h2, l1 + l2, bbox_to_anchor=(1.05, 1.0),\n",
    "                     frameon=False, ncol=ncol * 2)\n",
    "\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches=\"tight\", transparent=True)\n",
    "        return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title numerical gradient imnn module\n",
    "\n",
    "import optax\n",
    "import jax.numpy as np\n",
    "# from imnn.imnn._imnn import _IMNN\n",
    "from imnn.utils.utils import _check_input\n",
    "\n",
    "class myNumericalGradientIMNN(_myIMNN):\n",
    "    \"\"\"Information maximising neural network fit using numerical derivatives\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    simulations :math:`{\\\\bf d}^i` originally generated at fiducial model\n",
    "    parameter :math:`{\\\\bf\\\\theta}^\\\\rm{fid}`, and a set of\n",
    "    :math:`i\\\\in[1, n_d]` simulations,\n",
    "    :math:`\\\\{{\\\\bf d}_{\\\\alpha^-}^i, {\\\\bf d}_{\\\\alpha^+}^i\\\\}`, generated\n",
    "    with the same seed at each :math:`i` generated at\n",
    "    :math:`{\\\\bf\\\\theta}^\\\\rm{fid}` apart from at parameter label\n",
    "    :math:`\\\\alpha` with values\n",
    "\n",
    "    .. math::\n",
    "        \\\\theta_{\\\\alpha^-} = \\\\theta_\\\\alpha^\\\\rm{fid}-\\\\delta\\\\theta_\\\\alpha\n",
    "\n",
    "    and\n",
    "\n",
    "    .. math::\n",
    "        \\\\theta_{\\\\alpha^+} = \\\\theta_\\\\alpha^\\\\rm{fid}+\\\\delta\\\\theta_\\\\alpha\n",
    "\n",
    "    where :math:`\\\\delta\\\\theta_\\\\alpha` is a :math:`n_{params}` length vector\n",
    "    with the :math:`\\\\alpha` element having a value which perturbs the\n",
    "    parameter :math:`\\\\theta^{\\\\rm fid}_\\\\alpha`. This means there are\n",
    "    :math:`2\\\\times n_{params}\\\\times n_d` simulations used to calculate the\n",
    "    numerical derivatives (this is extremely cheap compared to other machine\n",
    "    learning methods). All these simulations are passed through a network\n",
    "    :math:`f_{{\\\\bf w}}({\\\\bf d})` with network parameters :math:`{\\\\bf w}` to\n",
    "    obtain network outputs :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\{{\\\\bf x}_{\\\\alpha^-}^i,{\\\\bf x}_{\\\\alpha^+}^i\\\\}`. These\n",
    "    perturbed values are combined to obtain\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial{{\\\\bf x}^i}}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "        \\\\frac{{\\\\bf x}_{\\\\alpha^+}^i - {\\\\bf x}_{\\\\alpha^-}^i}\n",
    "        {\\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "    With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    δθ : float(n_params,)\n",
    "        Size of perturbation to model parameters for the numerical derivative\n",
    "    fiducial : float(n_s, input_shape)\n",
    "        The simulations generated at the fiducial model parameter values used\n",
    "        for calculating the covariance of network outputs (for fitting)\n",
    "    derivative : float(n_d, 2, n_params, input_shape)\n",
    "        The simulations generated at parameter values perturbed from the\n",
    "        fiducial used to calculate the numerical derivative of network outputs\n",
    "        with respect to model parameters (for fitting)\n",
    "    validation_fiducial : float(n_s, input_shape) or None\n",
    "        The simulations generated at the fiducial model parameter values used\n",
    "        for calculating the covariance of network outputs (for validation)\n",
    "    validation_derivative : float(n_d, 2, n_params, input_shape) or None\n",
    "        The simulations generated at parameter values perturbed from the\n",
    "        fiducial used to calculate the numerical derivative of network outputs\n",
    "        with respect to model parameters (for validation)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, θ_fid,\n",
    "                 model, optimiser, key_or_state, fiducial, derivative, δθ,\n",
    "                 validation_fiducial=None, validation_derivative=None, \n",
    "                 dummy_input=None,\n",
    "                 no_invC=False, do_reg=True, evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary. Also fills the\n",
    "        simulation attributes (and validation if available).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        θ_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float(None, input_shape)) -> float(None, n_summari\n",
    "            es)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, 2, n_params, input_shape)\n",
    "            The simulations generated at parameter values perturbed from the\n",
    "            fiducial used to calculate the numerical derivative of network\n",
    "            outputs with respect to model parameters (for fitting)\n",
    "        δθ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation)\n",
    "        validation_derivative : float(n_d, 2, n_params, input_shape) or None\n",
    "            The simulations generated at parameter values perturbed from the\n",
    "            fiducial used to calculate the numerical derivative of network\n",
    "            outputs with respect to model parameters (for validation)\n",
    "        dummy_input : jraph.GraphsTuple or jax.numpy.DeviceArray\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            n_s=n_s,\n",
    "            n_d=n_d,\n",
    "            n_params=n_params,\n",
    "            n_summaries=n_summaries,\n",
    "            input_shape=input_shape,\n",
    "            θ_fid=θ_fid,\n",
    "            model=model,\n",
    "            key_or_state=key_or_state,\n",
    "            optimiser=optimiser,\n",
    "            dummy_input=dummy_input,\n",
    "            no_invC=no_invC,\n",
    "            do_reg=do_reg,\n",
    "            evidence=evidence)\n",
    "        self._set_data(δθ, fiducial, derivative, validation_fiducial,\n",
    "                       validation_derivative)\n",
    "        self.dummy_input = dummy_input\n",
    "\n",
    "    def _set_data(self, δθ, fiducial, derivative, validation_fiducial,\n",
    "                  validation_derivative):\n",
    "        \"\"\"Checks and sets data attributes with the correct shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        δθ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, input_shape, n_params)\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for fitting)\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation). Sets ``validate = True`` attribute if provided\n",
    "        validation_derivative : float(n_d, input_shape, n_params) or None\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for validation). Sets ``validate = True`` attribute if\n",
    "            provided\n",
    "        \"\"\"\n",
    "        self.δθ = np.expand_dims(\n",
    "            _check_input(δθ, (self.n_params,), \"δθ\"), (0, 1))\n",
    "        if self.dummy_input is None:\n",
    "          self.fiducial = _check_input(\n",
    "              fiducial, (self.n_s,) + self.input_shape, \"fiducial\")\n",
    "          self.derivative = _check_input(\n",
    "              derivative, (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "              \"derivative\")\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = _check_input(\n",
    "                  validation_fiducial, (self.n_s,) + self.input_shape,\n",
    "                  \"validation_fiducial\")\n",
    "              self.validation_derivative = _check_input(\n",
    "                  validation_derivative,\n",
    "                  (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "                  \"validation_derivative\")\n",
    "              self.validate = True\n",
    "        else:\n",
    "          self.fiducial = fiducial\n",
    "          self.derivative = derivative\n",
    "\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = validation_fiducial\n",
    "              self.validation_derivative =  validation_derivative\n",
    "              self.validate = True\n",
    "\n",
    "\n",
    "    def _collect_input(self, key, validate=False):\n",
    "        \"\"\" Returns validation or fitting sets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : None or int(2,)\n",
    "            Random number generators not used in this case\n",
    "        validate : bool\n",
    "            Whether to return the set for validation or for fitting\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, input_shape):\n",
    "            The fiducial simulations for fitting or validation\n",
    "        float(n_d, 2, n_params, input_shape):\n",
    "            The derivative simulations for fitting or validation\n",
    "        \"\"\"\n",
    "        if validate:\n",
    "            fiducial = self.validation_fiducial\n",
    "            derivative = self.validation_derivative\n",
    "        else:\n",
    "            fiducial = self.fiducial\n",
    "            derivative = self.derivative\n",
    "        return fiducial, derivative\n",
    "\n",
    "    def get_summaries(self, w, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Selects either the fitting or validation sets and passes them through\n",
    "        the network to get the network outputs. For the numerical derivatives,\n",
    "        the array is first flattened along the batch axis before being passed\n",
    "        through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, n_summaries):\n",
    "            The set of all network outputs used to calculate the covariance\n",
    "        float(n_d, 2, n_params, n_summaries):\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "        \"\"\"\n",
    "        d, d_mp = self._collect_input(key, validate=validate)\n",
    "        \n",
    "        \n",
    "        if self.dummy_input is None:\n",
    "          x = self.model(w, d)\n",
    "          x_mp = np.reshape(\n",
    "              self.model(\n",
    "                  w, d_mp.reshape(\n",
    "                      (self.n_d * 2 * self.n_params,) + self.input_shape)),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "        else:\n",
    "          # if operating on graph data, we need to vmap the implicit\n",
    "          # batch dimension\n",
    "          _model = lambda d: self.model(w, d)\n",
    "          x = jax.vmap(_model)(d)\n",
    "          x_mp = np.reshape(\n",
    "              jax.vmap(_model)(d_mp),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "\n",
    "        return x, x_mp\n",
    "\n",
    "    def _construct_derivatives(self, x_mp):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        The network outputs from the simulations generated with model parameter\n",
    "        values above and below the fiducial are subtracted from each other and\n",
    "        divided by the perturbation size in each model parameter value. The\n",
    "        axes are swapped such that the derivatives with respect to parameters\n",
    "        are in the last axis.\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial{\\\\bf x}^i}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "            \\\\frac{{\\\\bf x}^i_{\\\\alpha^+}-{\\\\bf x}^i_{\\\\alpha^+}}{\n",
    "            \\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, 2, n_params, n_summaries)\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The numerical derivatives of the network ouputs with respect to the\n",
    "            model parameters\n",
    "        \"\"\"\n",
    "        return np.swapaxes(x_mp[:, 1] - x_mp[:, 0], 1, 2) / self.δθ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "IMNN = myNumericalGradientIMNN(\n",
    "    n_s=n_s, n_d=n_d, n_params=n_params, n_summaries=n_summaries,\n",
    "    input_shape=input_shape, θ_fid=θ_fid, model=model,\n",
    "    optimiser=optimiser, key_or_state=jnp.array(key), δθ=δθ,\n",
    "    fiducial=fiducial, derivative=numerical_derivative,\n",
    "    validation_fiducial=validation_fiducial,\n",
    "    validation_derivative=validation_numerical_derivative, \n",
    "    dummy_input=graph,\n",
    "    no_invC=True,\n",
    "    do_reg=False,\n",
    "    evidence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMNN.set_F_statistics(w=initial_w, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2143200., dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(IMNN.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.4 s, sys: 73.6 ms, total: 36.5 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "key,rng = jax.random.split(rng)\n",
    "IMNN.fit(1.0, 0.1, γ=1e10, rng=np.array(key), patience=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = IMNN.plot(expected_detF=None);\n",
    "ax[0].set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(4.443286e+28, dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(IMNN.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fe164c7e650>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(IMNN.F)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(IMNN.history['detF'][:1750], label='training')\n",
    "plt.plot(IMNN.history['val_detF'][:1750], color='k', linestyle=':', label='validation')\n",
    "#plt.plot(jnp.ones(len(IMNN.history['detF'][:]))*det_theoryF, c='k', linestyle='--', label='lognormal information')\n",
    "#plt.ylim(1e-2, 1e8)\n",
    "plt.legend()\n",
    "plt.ylabel(r'$\\det F$')\n",
    "plt.xlabel('number of epochs')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
