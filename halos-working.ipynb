{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imnn\n",
    "import imnn.lfi\n",
    "from imnn.utils import value_and_jacrev, value_and_jacfwd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import jraph\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# pylians halo read script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nroot = '/mnt/xfs1/home/fvillaescusa/data/Neutrino_simulations/Sims_Dec16_2/'\\n################################## INPUT ######################################\\nfolders = ['0.0eV/','0.06eV/','0.10eV/','0.10eV_degenerate/',\\n           '0.15eV/','0.6eV/',\\n           '0.0eV_0.798/','0.0eV_0.807/','0.0eV_0.818/','0.0eV_0.822/',\\n           '0.0eV_s8c/','0.0eV_s8m/']\\n###############################################################################\\n# do a loop over the different cosmologies\\nfor folder in folders:\\n    # do a loop over the different realizations\\n    for i in range(1,101):\\n        snapdir = root + folder + '%d/'%i\\n        \\n        # do a loop over the different redshift\\n        for snapnum in [0,1,2,3]:\\n            FoF_folder     = snapdir+'groups_%03d'%snapnum\\n            old_FoF_folder = snapdir+'original_groups_%03d'%snapnum\\n            if os.path.exists(FoF_folder):\\n                print('%s\\t%d\\t%d\\texists'%(folder,i,snapnum))\\n                if os.path.exists(old_FoF_folder):\\n                    continue\\n                # create new FoF file\\n                f_tab = '%s/group_tab_%03d.0'%(snapdir,snapnum)\\n                f_ids = '%s/group_ids_%03d.0'%(snapdir,snapnum)\\n                FoF = readfof.FoF_catalog(snapdir,snapnum,long_ids=False,\\n                                          swap=False,SFR=False)\\n                writeFoFCatalog(FoF, f_tab, idsFile=f_ids)\\n           \\n                # rename FoF folder, create new FoF folder and move files to it\\n                os.system('mv '+FoF_folder+' '+old_FoF_folder)\\n                os.system('mkdir '+FoF_folder)\\n                os.system('mv '+f_tab+' '+f_ids+' '+FoF_folder)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model read_FoF\"\"\"\n",
    "#readfof(Base,snapnum,endian=None)\n",
    "#Read FoF files from Gadget, P-FoF\n",
    "     #Parameters:\n",
    "        #basedir: where your FoF folder located\n",
    "        #snapnum: snapshot number\n",
    "        #long_ids: whether particles ids are uint32 or uint64\n",
    "        #swap: False or True\n",
    "     #return structures:\n",
    "        #TotNgroups,TotNids,GroupLen,GroupOffset,GroupMass,GroupPos,GroupIDs...\n",
    "     #Example:\n",
    "        #--------\n",
    "        #FoF_halos=readfof(\"/data1/villa/b500p512nu0.6z99tree\",17,long_ids=True,swap=False)\n",
    "        #Masses=FoF_halos.GroupMass\n",
    "        #IDs=FoF_halos.GroupIDs\n",
    "        #--------\n",
    "        #updated time 19 Oct 2012 by wgcui\n",
    "\n",
    "#For simulations with SFR, set SFR=True\n",
    "#The physical velocities of the halos are found multiplying the field \n",
    "#GroupVel by (1+z)\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from struct import unpack\n",
    "\n",
    "class FoF_catalog:\n",
    "    def __init__(self, basedir, snapnum, long_ids=False, swap=False,\n",
    "                 SFR=False, read_IDs=True, prefix='/groups_'):\n",
    "\n",
    "        if long_ids:  format = np.uint64\n",
    "        else:         format = np.uint32\n",
    "\n",
    "        exts=('000'+str(snapnum))[-3:]\n",
    "\n",
    "        #################  READ TAB FILES ################# \n",
    "        fnb, skip, Final = 0, 0, False\n",
    "        dt1 = np.dtype((np.float32,3))\n",
    "        dt2 = np.dtype((np.float32,6))\n",
    "        prefix = basedir + prefix + exts + \"/group_tab_\" + exts + \".\"\n",
    "        while not(Final):\n",
    "            f=open(prefix+str(fnb), 'rb')\n",
    "            self.Ngroups    = np.fromfile(f, dtype=np.int32,  count=1)[0]\n",
    "            self.TotNgroups = np.fromfile(f, dtype=np.int32,  count=1)[0]\n",
    "            self.Nids       = np.fromfile(f, dtype=np.int32,  count=1)[0]\n",
    "            self.TotNids    = np.fromfile(f, dtype=np.uint64, count=1)[0]\n",
    "            self.Nfiles     = np.fromfile(f, dtype=np.uint32, count=1)[0]\n",
    "\n",
    "            TNG, NG = self.TotNgroups, self.Ngroups\n",
    "            if fnb == 0:\n",
    "                self.GroupLen    = np.empty(TNG, dtype=np.int32)\n",
    "                self.GroupOffset = np.empty(TNG, dtype=np.int32)\n",
    "                self.GroupMass   = np.empty(TNG, dtype=np.float32)\n",
    "                self.GroupPos    = np.empty(TNG, dtype=dt1)\n",
    "                self.GroupVel    = np.empty(TNG, dtype=dt1)\n",
    "                self.GroupTLen   = np.empty(TNG, dtype=dt2)\n",
    "                self.GroupTMass  = np.empty(TNG, dtype=dt2)\n",
    "                if SFR:  self.GroupSFR = np.empty(TNG, dtype=np.float32)\n",
    "                    \n",
    "            if NG>0:\n",
    "                locs=slice(skip,skip+NG)\n",
    "                self.GroupLen[locs]    = np.fromfile(f,dtype=np.int32,count=NG)\n",
    "                self.GroupOffset[locs] = np.fromfile(f,dtype=np.int32,count=NG)\n",
    "                self.GroupMass[locs]   = np.fromfile(f,dtype=np.float32,count=NG)\n",
    "                self.GroupPos[locs]    = np.fromfile(f,dtype=dt1,count=NG)\n",
    "                self.GroupVel[locs]    = np.fromfile(f,dtype=dt1,count=NG)\n",
    "                self.GroupTLen[locs]   = np.fromfile(f,dtype=dt2,count=NG)\n",
    "                self.GroupTMass[locs]  = np.fromfile(f,dtype=dt2,count=NG)\n",
    "                if SFR:\n",
    "                    self.GroupSFR[locs]=np.fromfile(f,dtype=np.float32,count=NG)\n",
    "                skip+=NG\n",
    "\n",
    "                if swap:\n",
    "                    self.GroupLen.byteswap(True)\n",
    "                    self.GroupOffset.byteswap(True)\n",
    "                    self.GroupMass.byteswap(True)\n",
    "                    self.GroupPos.byteswap(True)\n",
    "                    self.GroupVel.byteswap(True)\n",
    "                    self.GroupTLen.byteswap(True)\n",
    "                    self.GroupTMass.byteswap(True)\n",
    "                    if SFR:  self.GroupSFR.byteswap(True)\n",
    "                        \n",
    "            curpos = f.tell()\n",
    "            f.seek(0,os.SEEK_END)\n",
    "            if curpos != f.tell():\n",
    "                raise Exception(\"Warning: finished reading before EOF for tab file\",fnb)\n",
    "            f.close()\n",
    "            fnb+=1\n",
    "            if fnb==self.Nfiles: Final=True\n",
    "\n",
    "\n",
    "        #################  READ IDS FILES ################# \n",
    "        if read_IDs:\n",
    "\n",
    "            fnb,skip=0,0\n",
    "            Final=False\n",
    "            while not(Final):\n",
    "                fname=basedir+\"/groups_\" + exts +\"/group_ids_\"+exts +\".\"+str(fnb)\n",
    "                f=open(fname,'rb')\n",
    "                Ngroups     = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                TotNgroups  = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                Nids        = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                TotNids     = np.fromfile(f,dtype=np.uint64,count=1)[0]\n",
    "                Nfiles      = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                Send_offset = np.fromfile(f,dtype=np.uint32,count=1)[0]\n",
    "                if fnb==0:\n",
    "                    self.GroupIDs=np.zeros(dtype=format,shape=TotNids)\n",
    "                if Ngroups>0:\n",
    "                    if long_ids:\n",
    "                        IDs=np.fromfile(f,dtype=np.uint64,count=Nids)\n",
    "                    else:\n",
    "                        IDs=np.fromfile(f,dtype=np.uint32,count=Nids)\n",
    "                    if swap:\n",
    "                        IDs=IDs.byteswap(True)\n",
    "                    self.GroupIDs[skip:skip+Nids]=IDs[:]\n",
    "                    skip+=Nids\n",
    "                curpos = f.tell()\n",
    "                f.seek(0,os.SEEK_END)\n",
    "                if curpos != f.tell():\n",
    "                    raise Exception(\"Warning: finished reading before EOF for IDs file\",fnb)\n",
    "                f.close()\n",
    "                fnb+=1\n",
    "                if fnb==Nfiles: Final=True\n",
    "\n",
    "\n",
    "# This function is used to write one single file for the FoF instead of having\n",
    "# many files. This will make faster the reading of the FoF file\n",
    "def writeFoFCatalog(fc, tabFile, idsFile=None):\n",
    "    if fc.TotNids > (1<<32)-1: raise Exception('TotNids overflow')\n",
    "\n",
    "    f = open(tabFile, 'wb')\n",
    "    np.asarray(fc.TotNgroups).tofile(f)\n",
    "    np.asarray(fc.TotNgroups).tofile(f)\n",
    "    np.asarray(fc.TotNids, dtype=np.int32).tofile(f)\n",
    "    np.asarray(fc.TotNids).tofile(f)\n",
    "    np.asarray(1, dtype=np.uint32).tofile(f)\n",
    "    fc.GroupLen.tofile(f)\n",
    "    fc.GroupOffset.tofile(f)\n",
    "    fc.GroupMass.tofile(f)\n",
    "    fc.GroupPos.tofile(f)\n",
    "    fc.GroupVel.tofile(f)\n",
    "    fc.GroupTLen.tofile(f)\n",
    "    fc.GroupTMass.tofile(f)\n",
    "    if hasattr(fc, 'GroupSFR'):\n",
    "        fc.GroupSFR.tofile(f)\n",
    "    f.close()\n",
    "\n",
    "    if idsFile:\n",
    "        f = open(idsFile, 'wb')\n",
    "        np.asarray(fc.TotNgroups).tofile(f)\n",
    "        np.asarray(fc.TotNgroups).tofile(f)\n",
    "        np.asarray(fc.TotNids, dtype=np.uint32).tofile(f) \n",
    "        np.asarray(fc.TotNids).tofile(f)\n",
    "        np.asarray(1, dtype=np.uint32).tofile(f)\n",
    "        np.asarray(0, dtype=np.uint32).tofile(f) \n",
    "        fc.GroupIDs.tofile(f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "# This is an example on how to change files\n",
    "\"\"\"\n",
    "root = '/mnt/xfs1/home/fvillaescusa/data/Neutrino_simulations/Sims_Dec16_2/'\n",
    "################################## INPUT ######################################\n",
    "folders = ['0.0eV/','0.06eV/','0.10eV/','0.10eV_degenerate/',\n",
    "           '0.15eV/','0.6eV/',\n",
    "           '0.0eV_0.798/','0.0eV_0.807/','0.0eV_0.818/','0.0eV_0.822/',\n",
    "           '0.0eV_s8c/','0.0eV_s8m/']\n",
    "###############################################################################\n",
    "# do a loop over the different cosmologies\n",
    "for folder in folders:\n",
    "    # do a loop over the different realizations\n",
    "    for i in range(1,101):\n",
    "        snapdir = root + folder + '%d/'%i\n",
    "        \n",
    "        # do a loop over the different redshift\n",
    "        for snapnum in [0,1,2,3]:\n",
    "            FoF_folder     = snapdir+'groups_%03d'%snapnum\n",
    "            old_FoF_folder = snapdir+'original_groups_%03d'%snapnum\n",
    "            if os.path.exists(FoF_folder):\n",
    "                print('%s\\t%d\\t%d\\texists'%(folder,i,snapnum))\n",
    "                if os.path.exists(old_FoF_folder):\n",
    "                    continue\n",
    "                # create new FoF file\n",
    "                f_tab = '%s/group_tab_%03d.0'%(snapdir,snapnum)\n",
    "                f_ids = '%s/group_ids_%03d.0'%(snapdir,snapnum)\n",
    "                FoF = readfof.FoF_catalog(snapdir,snapnum,long_ids=False,\n",
    "                                          swap=False,SFR=False)\n",
    "                writeFoFCatalog(FoF, f_tab, idsFile=f_ids)\n",
    "           \n",
    "                # rename FoF folder, create new FoF folder and move files to it\n",
    "                os.system('mv '+FoF_folder+' '+old_FoF_folder)\n",
    "                os.system('mkdir '+FoF_folder)\n",
    "                os.system('mv '+f_tab+' '+f_ids+' '+FoF_folder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "maindir = '/data80/makinen/quijote/Halos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = onp\n",
    "# input files\n",
    "snapdir = maindir + 'Om_m/23/' #folder hosting the catalogue\n",
    "snapnum = 4                                            #redshift 0\n",
    "\n",
    "# determine the redshift of the catalogue\n",
    "z_dict = {4:0.0, 3:0.5, 2:1.0, 1:2.0, 0:3.0}\n",
    "redshift = z_dict[snapnum]\n",
    "\n",
    "# read the halo catalogue\n",
    "FoF = FoF_catalog(snapdir, snapnum, long_ids=False,\n",
    "                          swap=False, SFR=False, read_IDs=False)\n",
    "\n",
    "# get the properties of the halos\n",
    "pos_h = FoF.GroupPos/1e3            # Halo positions in Mpc/h\n",
    "mass  = FoF.GroupMass*1e10          # Halo masses in Msun/h\n",
    "vel_h = FoF.GroupVel*(1.0+redshift) # Halo peculiar velocities in km/s\n",
    "Npart = FoF.GroupLen                # Number of CDM particles in the halo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# helper functions\n",
    "def get_distances(X):\n",
    "    nx = X.shape[0]\n",
    "    return (X[:, None, :] - X[None, :, :])[jnp.tril_indices(nx, k=-1)]\n",
    "\n",
    "\n",
    "def get_receivers_senders(nx, dists, connect_radius=100):\n",
    "    '''connect nodes within `connect_radius` units'''\n",
    "    \n",
    "    senders,receivers = jnp.tril_indices(nx, k=-1)\n",
    "    mask = dists < connect_radius\n",
    "    return senders[mask], receivers[mask]\n",
    "\n",
    "def get_r2(X):\n",
    "    nx = X.shape[0]\n",
    "    # can also do\n",
    "    #a = X\n",
    "    #b = a.reshape(a.shape[0], 1, a.shape[1])\n",
    "    # jnp.sqrt(jnp.einsum('ijk, ijk->ij', a-b, a-b))\n",
    "    alldists = jnp.linalg.norm(X[:, None, :] - X[None, :, :], axis=-1)\n",
    "    return alldists[jnp.tril_indices(nx, k=-1)]\n",
    "\n",
    "#@partial(jax.jit, static_argnums=3)\n",
    "def numpy_to_graph(X, V, masses, \n",
    "                   Npart,\n",
    "                   connect_radius=50, \n",
    "                   return_components=True,\n",
    "                   scale_inputs=True, \n",
    "                   nx=None):\n",
    "    if nx is None:\n",
    "        nx = jnp.array([X.shape[0]])\n",
    "        \n",
    "    _nx = jnp.array([nx])\n",
    "    \n",
    "    masses = jnp.array(masses)\n",
    "    Npart = jnp.array(Npart)\n",
    "    \n",
    "    if scale_inputs:\n",
    "        X /= 1000. # in units of Gpc\n",
    "        connect_radius /= 1000. # in units of Gpc\n",
    "        V /= 1000. \n",
    "        masses = jnp.log(masses)\n",
    "        masses = (masses - jnp.mean(masses)) / jnp.std(masses)\n",
    "        Npart = jnp.log(Npart)\n",
    "        Npart = (Npart - jnp.mean(Npart)) / jnp.std(Npart)\n",
    "        \n",
    "    \n",
    "    # mask out halos with distances < connect_radius\n",
    "    dists = get_r2(X)\n",
    "    \n",
    "    receivers, senders = get_receivers_senders(nx, dists, \n",
    "                                               connect_radius=connect_radius)\n",
    "    \n",
    "    edges = dists[dists < connect_radius]\n",
    "\n",
    "    receivers = jnp.array(receivers)\n",
    "    senders = jnp.array(senders)\n",
    "\n",
    "    if masses is None:\n",
    "        # Default all masses to one\n",
    "        masses = jnp.ones(nx)\n",
    "    elif isinstance(masses, (int, float)):\n",
    "        masses = masses*jnp.ones(nx)\n",
    "    else:\n",
    "        assert len(masses) == nx, 'Wrong size for masses'\n",
    "\n",
    "\n",
    "    nodes = jnp.concatenate([masses.reshape([-1, 1]), Npart.reshape((-1,1)), X, V], axis=1)\n",
    "    \n",
    "    if return_components:\n",
    "        return nodes,senders,receivers,edges[:, None], nx, jnp.array(edges.shape[0])\n",
    "    \n",
    "    else:\n",
    "        graph = jraph.GraphsTuple(nodes=nodes, senders=senders, receivers=receivers,\n",
    "                                  edges=edges[:, None], n_node=_nx, n_edge=jnp.array([edges.shape[0]]), globals=None)\n",
    "        return graph\n",
    "\n",
    "\n",
    "\n",
    "def update_edge_dummy(edge, sender_node, receiver_node, globals_):\n",
    "    return edge\n",
    "\n",
    "\n",
    "def update_node_dummy(node, sender, receiver, globals_):\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 µs, sys: 5 µs, total: 35 µs\n",
      "Wall time: 71.5 µs\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# do for small test data\n",
    "%time\n",
    "mass_cut = (mass > 1.5e15)\n",
    "print(np.sum(mass_cut))\n",
    "\n",
    "mymasses = jnp.array(mass[mass_cut])\n",
    "X = pos_h[mass_cut]\n",
    "V = vel_h[mass_cut]\n",
    "_npart = Npart[mass_cut]\n",
    "\n",
    "# mask out halos with distances < 50 Mpc\n",
    "connect_radius = 500 # Mpc\n",
    "dists = np.abs(get_distances(X))\n",
    "\n",
    "graph = numpy_to_graph(X, V, mymasses, _npart, return_components=False, connect_radius=connect_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([32., 27., 13.,  9.,  3.,  3.,  1.,  0.,  0.,  1.]),\n",
       " array([-1.1333517 , -0.5242682 ,  0.08481522,  0.6938987 ,  1.3029821 ,\n",
       "         1.9120655 ,  2.521149  ,  3.1302326 ,  3.739316  ,  4.348399  ,\n",
       "         4.957483  ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMuUlEQVR4nO3db4hdBX7G8edpzOKyWnTJbRoS6Sxb2RIKjmVIXVyK665L1pSqpS0VKnkhzL5QUBBKun3RXeiLLHS1b8pCtgkGat1KVZTNsrupDYiwaCdu1MTsopVZmpB1rlhR32xJfPpizrTDeMd75/6ZM7/x+4Fh7j33zD2/k+jX48m5J04iAEA9v9b2AACA4RBwACiKgANAUQQcAIoi4ABQ1GXrubFt27ZlampqPTcJAOWdPHnyrSSdlcvXNeBTU1Oam5tbz00CQHm2f9FrOadQAKAoAg4ARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoKh1/STmKKYOHGtt2/MH97W2bQBYDUfgAFAUAQeAogg4ABRFwAGgKAIOAEURcAAoioADQFEEHACK6htw25fbfsH2S7bP2P5ms/wztp+3/brtf7H9icmPCwBYMsgR+K8k3ZzkOknTkvbavkHStyQ9lOS3Jf23pLsnNiUA4EP6BjyL3m+ebm2+IulmSf/aLD8q6fZJDAgA6G2gc+C2t9g+JWlB0nFJ/ynpnSQXm1XOSdo5kQkBAD0NFPAkl5JMS9olaY+k3xl0A7Znbc/Znut2u8NNCQD4kDVdhZLkHUknJH1e0lW2l+5muEvS+VV+5lCSmSQznU5nlFkBAMsMchVKx/ZVzeNPSrpF0lkthvxPmtX2S3pqQjMCAHoY5H7gOyQdtb1Fi8F/LMn3bb8q6Xu2/1bSTyUdnuCcAIAV+gY8ycuSru+x/A0tng8HALSAT2ICQFEEHACKIuAAUBQBB4CiCDgAFEXAAaAoAg4ARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoKhB7gf+sTd14Fgr250/uK+V7QKogSNwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoCgCDgBFEXAAKKpvwG1fY/uE7Vdtn7F9X7P8G7bP2z7VfN06+XEBAEsG+Sj9RUkPJHnR9pWSTto+3rz2UJK/m9x4AIDV9A14kguSLjSP37N9VtLOSQ8GAPhoazoHbntK0vWSnm8W3Wv7ZdtHbF+9ys/M2p6zPdftdkebFgDwfwYOuO0rJD0u6f4k70r6jqTPSprW4hH6t3v9XJJDSWaSzHQ6ndEnBgBIGjDgtrdqMd6PJHlCkpK8meRSkg8kfVfSnsmNCQBYaZCrUCzpsKSzSR5ctnzHstXukHR6/OMBAFYzyFUoN0q6S9Irtk81y74u6U7b05IiaV7S1yYwHwBgFYNchfKcJPd46QfjHwcAMCg+iQkARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoCgCDgBFEXAAKIqAA0BRBBwAiiLgAFAUAQeAogg4ABRFwAGgKAIOAEURcAAoioADQFEEHACKIuAAUBQBB4CiCDgAFNU34LavsX3C9qu2z9i+r1n+advHbb/WfL968uMCAJYMcgR+UdIDSXZLukHSPbZ3Szog6Zkk10p6pnkOAFgnfQOe5EKSF5vH70k6K2mnpNskHW1WOyrp9gnNCADoYU3nwG1PSbpe0vOStie50Lz0S0nbV/mZWdtztue63e4oswIAlhk44LavkPS4pPuTvLv8tSSRlF4/l+RQkpkkM51OZ6RhAQD/b6CA296qxXg/kuSJZvGbtnc0r++QtDCZEQEAvQxyFYolHZZ0NsmDy156WtL+5vF+SU+NfzwAwGouG2CdGyXdJekV26eaZV+XdFDSY7bvlvQLSX82kQkBAD31DXiS5yR5lZe/NN5xAACD4pOYAFAUAQeAogg4ABRFwAGgKAIOAEURcAAoioADQFEEHACKIuAAUBQBB4CiCDgAFEXAAaAoAg4ARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoCgCDgBFEXAAKIqAA0BRBBwAiuobcNtHbC/YPr1s2Tdsn7d9qvm6dbJjAgBWGuQI/GFJe3ssfyjJdPP1g/GOBQDop2/Akzwr6e11mAUAsAajnAO/1/bLzSmWq1dbyfas7Tnbc91ud4TNAQCWGzbg35H0WUnTki5I+vZqKyY5lGQmyUyn0xlycwCAlYYKeJI3k1xK8oGk70raM96xAAD9DBVw2zuWPb1D0unV1gUATMZl/Vaw/aikmyRts31O0t9Iusn2tKRImpf0tcmNCADopW/Ak9zZY/HhCcwCAFgDPokJAEURcAAoioADQFEEHACKIuAAUBQBB4CiCDgAFNX3OnC0Z+rAsVa2O39wXyvbBbA2HIEDQFEEHACKIuAAUBQBB4CiCDgAFEXAAaAoAg4ARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoCgCDgBF9Q247SO2F2yfXrbs07aP236t+X71ZMcEAKw0yBH4w5L2rlh2QNIzSa6V9EzzHACwjvoGPMmzkt5esfg2SUebx0cl3T7esQAA/Qx7Dnx7kgvN419K2r7airZnbc/Znut2u0NuDgCw0sh/iJkkkvIRrx9KMpNkptPpjLo5AEBj2IC/aXuHJDXfF8Y3EgBgEMMG/GlJ+5vH+yU9NZ5xAACDGuQywkcl/UTS52yfs323pIOSbrH9mqQvN88BAOvosn4rJLlzlZe+NOZZAABrwCcxAaCovkfg+PiZOnCstW3PH9zX2raBajgCB4CiCDgAFEXAAaAoAg4ARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoCgCDgBFEXAAKIqAA0BRBBwAiiLgAFAUAQeAogg4ABRFwAGgKAIOAEURcAAoaqS/ld72vKT3JF2SdDHJzDiGAgD0N1LAG19M8tYY3gcAsAacQgGAokYNeCT92PZJ27O9VrA9a3vO9ly32x1xcwCAJaMG/AtJfk/SVyXdY/sPVq6Q5FCSmSQznU5nxM0BAJaMFPAk55vvC5KelLRnHEMBAPobOuC2P2X7yqXHkr4i6fS4BgMAfLRRrkLZLulJ20vv889JfjiWqQAAfQ0d8CRvSLpujLMAANaAywgBoCgCDgBFEXAAKIqAA0BRBBwAiiLgAFAUAQeAosZxO1lgbKYOHGt7hHU3f3Bf2yOgKI7AAaAoAg4ARRFwACiKgANAUQQcAIoi4ABQFAEHgKIIOAAURcABoCgCDgBFEXAAKIqAA0BR3MwKaFlbN/D6ON5Eq82bpU3i15sjcAAoioADQFEEHACKGingtvfa/rnt120fGNdQAID+hg647S2S/kHSVyXtlnSn7d3jGgwA8NFGOQLfI+n1JG8k+R9J35N023jGAgD0M8plhDsl/dey5+ck/f7KlWzPSpptnr5v++cjbHM12yS9NYH3XW/sx8ayqffD32phktGU/v1Y9us9zH78Vq+FE78OPMkhSYcmuQ3bc0lmJrmN9cB+bCzsx8bCfnzYKKdQzku6ZtnzXc0yAMA6GCXg/yHpWtufsf0JSX8u6enxjAUA6GfoUyhJLtq+V9KPJG2RdCTJmbFNtjYTPUWzjtiPjYX92FjYjxWcZFzvBQBYR3wSEwCKIuAAUNSmCbjtP7V9xvYHtktdarRZbklg+4jtBdun255lWLavsX3C9qvNP0/3tT3TMGxfbvsF2y81+/HNtmcahe0ttn9q+/ttzzIs2/O2X7F9yvbcON5z0wRc0mlJfyzp2bYHWYtNdkuChyXtbXuIEV2U9ECS3ZJukHRP0d+PX0m6Ocl1kqYl7bV9Q7sjjeQ+SWfbHmIMvphkeiNcB76hJDmbZBKf8py0TXNLgiTPSnq77TlGkeRCkhebx+9pMRo7251q7bLo/ebp1uar5BULtndJ2ifpH9ueZaPZNAEvrNctCcoFYzOyPSXpeknPtzzKUJrTDqckLUg6nqTkfkj6e0l/KemDlucYVST92PbJ5hYjIyv1V6rZ/jdJv9njpb9O8tR6z4PNy/YVkh6XdH+Sd9ueZxhJLkmatn2VpCdt/26SUn8+YfsPJS0kOWn7ppbHGdUXkpy3/RuSjtv+WfN/rUMrFfAkX257hgnglgQbjO2tWoz3I0meaHueUSV5x/YJLf75RKmAS7pR0h/ZvlXS5ZJ+3fY/JfmLludasyTnm+8Ltp/U4unTkQLOKZT2cUuCDcS2JR2WdDbJg23PMyzbnebIW7Y/KekWST9rdaghJPmrJLuSTGnx341/rxhv25+yfeXSY0lf0Rj+Y7ppAm77DtvnJH1e0jHbP2p7pkEkuShp6ZYEZyU91uItCUZi+1FJP5H0OdvnbN/d9kxDuFHSXZJubi73OtUc/VWzQ9IJ2y9r8SDheJKyl+BtAtslPWf7JUkvSDqW5IejvikfpQeAojbNETgAfNwQcAAoioADQFEEHACKIuAAUBQBB4CiCDgAFPW/xdoETyu00doAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(graph.nodes[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218,)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.senders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218, 1)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a function to pull in a bunch of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data80/makinen/quijote/Halos/'"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_sim(folder_name, \n",
    "             sim_num,\n",
    "             mass_cut=2e15,\n",
    "             snapnum=4):\n",
    "    # input files\n",
    "    snapdir = folder_name + '/%d/'%(sim_num) #folder hosting the catalogue\n",
    "    snapnum = snapnum    #vredshift\n",
    "\n",
    "    # determine the redshift of the catalogue\n",
    "    z_dict = {4:0.0, 3:0.5, 2:1.0, 1:2.0, 0:3.0}\n",
    "    redshift = z_dict[snapnum]\n",
    "\n",
    "    # read the halo catalogue\n",
    "    FoF = FoF_catalog(snapdir, snapnum, long_ids=False,\n",
    "                              swap=False, SFR=False, read_IDs=False)\n",
    "\n",
    "    # get the properties of the halos\n",
    "    pos_h = FoF.GroupPos/1e3            # Halo positions in Mpc/h\n",
    "    mass  = FoF.GroupMass*1e10          # Halo masses in Msun/h\n",
    "    vel_h = FoF.GroupVel*(1.0+redshift) # Halo peculiar velocities in km/s\n",
    "    Npart = FoF.GroupLen                # Number of CDM particles in the halo\n",
    "    \n",
    "    mass_cut = (mass > mass_cut)\n",
    "    \n",
    "    return mass[mass_cut],pos_h[mass_cut],vel_h[mass_cut],Npart[mass_cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halo_sims(folder_name, \n",
    "                  n_sims=2,\n",
    "                  mass_cut=2e15, \n",
    "                  connect_radius=50):\n",
    "\n",
    "    \n",
    "    graphs = []\n",
    "    \n",
    "    for i in range(n_sims):\n",
    "        mass,X,V,Npart = load_single_sim(folder_name, i, mass_cut)\n",
    "        graph = numpy_to_graph(X, V, mass, Npart, \n",
    "                               connect_radius=connect_radius)\n",
    "        \n",
    "        graphs.append(graph)\n",
    "    \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halo_batch(folder_name, \n",
    "                  sim_index,\n",
    "                  pad_nodes_to=150,\n",
    "                  pad_edges_to=150,\n",
    "                  mass_cut=2e15, \n",
    "                  connect_radius=200,\n",
    "                  node_features=8,\n",
    "                  edge_features=1):\n",
    "\n",
    "    n_sims = len(sim_index)\n",
    "    \n",
    "    \n",
    "    # padding for vmapping\n",
    "    def get_padding(pad_nodes_to, pad_edges_to):\n",
    "        nodes =  jnp.zeros((n_sims, pad_nodes_to, node_features))\n",
    "        senders = jnp.zeros((n_sims, pad_edges_to), dtype=int) \n",
    "        receivers = jnp.zeros((n_sims, pad_edges_to), dtype=int)\n",
    "        edges = jnp.zeros((n_sims, pad_edges_to, edge_features))\n",
    "        n_node = []\n",
    "        n_edge = []\n",
    "        _globals = None\n",
    "        return nodes,senders,receivers,edges,n_node,n_edge,_globals\n",
    "    \n",
    "    nodes,senders,receivers,edges,n_node,n_edge,_globals = get_padding(pad_nodes_to,pad_edges_to)\n",
    "    l = 0\n",
    "    \n",
    "    while l < len(sim_index):\n",
    "        \n",
    "        i = sim_index[l]\n",
    "        \n",
    "        mass,X,V,Npart = load_single_sim(folder_name, i, mass_cut)\n",
    "        _nodes,_senders,_receivers,_edges, _nx, _n_edge = numpy_to_graph(X, V, mass, \n",
    "                                                       Npart,return_components=True,\n",
    "                                                       connect_radius=connect_radius)\n",
    "        \n",
    "        if _nx < pad_nodes_to:\n",
    "            if _n_edge < pad_edges_to:\n",
    "                nodes = nodes.at[l, :_nodes.shape[0], :].set(_nodes)\n",
    "                senders = senders.at[l, :_senders.shape[0]].set(_senders)\n",
    "                receivers = receivers.at[l, :_receivers.shape[0]].set(_receivers)\n",
    "\n",
    "                edges = edges.at[l, :_edges.shape[0], :].set(_edges)\n",
    "\n",
    "                n_node.append(_nx) # these control how many edges / nodes get counted\n",
    "                n_edge.append(_n_edge)\n",
    "                l += 1\n",
    "\n",
    "            else:\n",
    "                print('boosting edge padding; \\n restarting batch loop...')\n",
    "                pad_edges_to += 10\n",
    "                print('new edge padding length:', pad_edges_to)\n",
    "                nodes,senders,receivers,edges,n_node,n_edge,_globals = get_padding(pad_nodes_to,pad_edges_to)\n",
    "                l = 0 \n",
    "\n",
    "        else:\n",
    "            print('boosting node padding; \\n restarting batch loop...')\n",
    "            pad_nodes_to += 10\n",
    "            print('new node padding length:', pad_nodes_to)\n",
    "            nodes,senders,receivers,edges,n_node,n_edge,_globals = get_padding(pad_nodes_to,pad_edges_to)\n",
    "            l = 0 \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    n_node = jnp.array(n_node)\n",
    "    n_edge = jnp.array(n_edge)\n",
    "    \n",
    "    # assemble explicitly batched GraphsTuple\n",
    "    batched_graph = jraph.GraphsTuple(nodes=nodes, senders=senders, receivers=receivers,\n",
    "                                edges=edges, n_node=n_node, n_edge=n_edge, globals=None)\n",
    "        \n",
    "    \n",
    "    return batched_graph, (pad_nodes_to, pad_edges_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now set up some datasets for IMNN training, varying the connectivity radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data80/makinen/quijote/Halos/'"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 2\n",
    "n_summaries=n_params\n",
    "\n",
    "θ_fid = jnp.array([0.3175, 0.834])\n",
    "δθ = jnp.array([0.01, 0.015])\n",
    "\n",
    "n_s = 100\n",
    "n_d = n_s\n",
    "\n",
    "input_shape = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "config.update('jax_enable_x64', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(connect_radius):\n",
    "    np = onp\n",
    "    sim_index = np.arange(100)\n",
    "    val_index = np.arange(start=100, stop=200)\n",
    "    mass_cut = 1.5e15 \n",
    "    pad_nodes_to = 160 # could devise a function to pull in a dummy graph to get max nodes for padding\n",
    "    pad_edges_to = 400\n",
    "\n",
    "    # get fiducial \n",
    "    folder_name = maindir + 'fiducial'\n",
    "    fiducial,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                          pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    # get validation fiducial\n",
    "    validation_fiducial,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                                 pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "\n",
    "    # get regular derivatives\n",
    "    folder_name = maindir + 'Om_m'\n",
    "    Om_m,_ = get_halo_batch(\n",
    "        folder_name, sim_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                    pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to\n",
    "    )\n",
    "\n",
    "    folder_name = maindir + 'Om_p'\n",
    "    Om_p,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    folder_name = maindir + 's8_m'\n",
    "    s8_m,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    folder_name = maindir + 's8_p'\n",
    "    s8_p,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    numerical_derivative = jraph.batch([Om_m, s8_m, Om_p, s8_p])\n",
    "\n",
    "    # get validation derivatives\n",
    "    folder_name = maindir + 'Om_m'\n",
    "    Om_m,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    folder_name = maindir + 'Om_p'\n",
    "    Om_p,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    folder_name = maindir + 's8_m'\n",
    "    s8_m,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    folder_name = maindir + 's8_p'\n",
    "    s8_p,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, connect_radius=connect_radius,\n",
    "                                      pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "    validation_numerical_derivative = jraph.batch([Om_m, s8_m, Om_p, s8_p])\n",
    "\n",
    "    return fiducial,validation_fiducial,numerical_derivative,validation_numerical_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 s, sys: 7.39 s, total: 29.4 s\n",
      "Wall time: 43 s\n",
      "CPU times: user 12.6 s, sys: 6.53 s, total: 19.1 s\n",
      "Wall time: 29.1 s\n",
      "CPU times: user 13.5 s, sys: 6.58 s, total: 20.1 s\n",
      "Wall time: 30.2 s\n",
      "CPU times: user 12.4 s, sys: 6.78 s, total: 19.2 s\n",
      "Wall time: 28.2 s\n",
      "CPU times: user 19.4 s, sys: 8.68 s, total: 28 s\n",
      "Wall time: 36.5 s\n",
      "CPU times: user 20.2 s, sys: 9.11 s, total: 29.4 s\n",
      "Wall time: 37.4 s\n",
      "CPU times: user 10.2 s, sys: 5.96 s, total: 16.2 s\n",
      "Wall time: 25.9 s\n",
      "CPU times: user 9.82 s, sys: 5.98 s, total: 15.8 s\n",
      "Wall time: 25.7 s\n",
      "CPU times: user 12.3 s, sys: 6.44 s, total: 18.7 s\n",
      "Wall time: 28.5 s\n",
      "CPU times: user 17.5 s, sys: 8.49 s, total: 26 s\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sim_index = np.arange(100)\n",
    "val_index = np.arange(start=100, stop=200)\n",
    "mass_cut = 1.5e15 \n",
    "pad_nodes_to = 160 # could devise a function to pull in a dummy graph to get max nodes for padding\n",
    "pad_edges_to = 400\n",
    "\n",
    "# get fiducial \n",
    "folder_name = maindir + 'fiducial'\n",
    "%time fiducial,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "# get validation fiducial\n",
    "%time validation_fiducial,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "\n",
    "# get regular derivatives\n",
    "folder_name = maindir + 'Om_m'\n",
    "%time Om_m,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 'Om_p'\n",
    "%time Om_p,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_m'\n",
    "%time s8_m,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_p'\n",
    "%time s8_p,_ = get_halo_batch(folder_name, sim_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "numerical_derivative = jraph.batch([Om_m, s8_m, Om_p, s8_p])\n",
    "\n",
    "# get validation derivatives\n",
    "folder_name = maindir + 'Om_m'\n",
    "%time Om_m,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut, pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 'Om_p'\n",
    "%time Om_p,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_m'\n",
    "%time s8_m,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "folder_name = maindir + 's8_p'\n",
    "%time s8_p,_ = get_halo_batch(folder_name, val_index, mass_cut=mass_cut,pad_nodes_to=pad_nodes_to, pad_edges_to=pad_edges_to)\n",
    "\n",
    "validation_numerical_derivative = jraph.batch([Om_m, s8_m, Om_p, s8_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_numerical_derivative.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph network in Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from typing import Sequence\n",
    "\n",
    "import optax\n",
    "\n",
    "class ExplicitMLP(nn.Module):\n",
    "  \"\"\"A flax MLP.\"\"\"\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, lyr in enumerate([nn.Dense(feat) for feat in self.features]):\n",
    "      x = lyr(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Functions must be passed to jraph GNNs, but pytype does not recognise\n",
    "# linen Modules as callables to here we wrap in a function.\n",
    "def make_embed_fn(latent_size):\n",
    "  def embed(inputs):\n",
    "    x = jnp.arcsinh(inputs)\n",
    "    return nn.Dense(latent_size)(x)\n",
    "  return embed\n",
    "\n",
    "\n",
    "def make_mlp(features):\n",
    "  @jraph.concatenated_args\n",
    "  def update_fn(inputs):\n",
    "    return ExplicitMLP(features)(inputs)\n",
    "  return update_fn\n",
    "\n",
    "\n",
    "aggregate_nodes_for_globals_fn = jraph.segment_sum\n",
    "aggregate_edges_for_globals_fn = jraph.segment_sum\n",
    "\n",
    "\n",
    "class flaxGraphNetwork(nn.Module):\n",
    "  \"\"\"A flax GraphNetwork.\"\"\"\n",
    "  mlp_features: Sequence[int]\n",
    "  latent_size: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, graph):\n",
    "    # Add a global parameter for graph classification.\n",
    "    graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], n_params]))\n",
    "    #\n",
    "    #graph = graph._replace(edges=jnp.zeros(graph.edges.shape))\n",
    "\n",
    "    embedder = jraph.GraphMapFeatures(\n",
    "        embed_node_fn=make_embed_fn(self.latent_size),\n",
    "        embed_edge_fn=make_embed_fn(self.latent_size),\n",
    "        embed_global_fn=make_embed_fn(self.latent_size))\n",
    "    net = jraph.GraphNetwork(\n",
    "        update_node_fn=make_mlp(self.mlp_features),\n",
    "        update_edge_fn=make_mlp(self.mlp_features),\n",
    "        # The global update outputs size 1 for information maximisation.\n",
    "        update_global_fn=make_mlp(self.mlp_features + (n_params,)),  # pytype: disable=unsupported-operands\n",
    "        aggregate_edges_for_globals_fn=aggregate_edges_for_globals_fn,\n",
    "        aggregate_nodes_for_globals_fn=aggregate_nodes_for_globals_fn\n",
    "    )\n",
    "    return net(embedder(graph)).globals.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([23., 19., 15.,  9.,  6.,  8.,  3.,  4.,  1.,  1.]),\n",
       " array([-0.9725967 , -0.6449305 , -0.31726423,  0.01040202,  0.33806825,\n",
       "         0.6657345 ,  0.99340075,  1.321067  ,  1.6487333 ,  1.9763994 ,\n",
       "         2.3040657 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.arcsinh(graph.nodes[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mass,X,V,Npart = load_single_sim(folder_name, 1, 2e15)\n",
    "graph = numpy_to_graph(X, V, mass, Npart, connect_radius=200, return_components=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng,key = jax.random.split(rng)\n",
    "\n",
    "model = flaxGraphNetwork([50, 50], 50)\n",
    "\n",
    "initial_w = model.init(key, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.56140333, -0.37265953], dtype=float32)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initial_w, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMNN time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## numerical gradient IMNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title imnn module\n",
    "import math\n",
    "import jax\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from imnn.utils.utils import _check_boolean, _check_type, _check_input, \\\n",
    "    _check_model, _check_model_output, _check_optimiser, _check_state, \\\n",
    "    _check_statistics_set\n",
    "from imnn.experimental import progress_bar\n",
    "\n",
    "np = jnp\n",
    "\n",
    "class _myIMNN:\n",
    "    \"\"\"Information maximising neural network parent class\n",
    "\n",
    "    This class defines the general fitting framework for information maximising\n",
    "    neural networks. It includes the generic calculations of the Fisher\n",
    "    information matrix from the outputs of a neural network as well as an XLA\n",
    "    compilable fitting routine (with and without a progress bar). This class\n",
    "    also provides a plotting routine for fitting history and a function to\n",
    "    calculate the score compression of network outputs to quasi-maximum\n",
    "    likelihood estimates of model parameter values.\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    simulations and :math:`n_d` derivatives with respect to physical model\n",
    "    parameters are used to calculate network outputs and their derivatives\n",
    "    with respect to the physical model parameters, :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha`, where\n",
    "    :math:`\\\\alpha` labels the physical parameter. The exact details of how\n",
    "    these are calculated depend on the type of available data (see list of\n",
    "    different IMNN below). With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function. Note for large input data-sizes, large :math:`n_s` or massive\n",
    "    networks the gradients may need manually accumulating via the\n",
    "    :func:`~imnn.imnn._aggregated_imnn._AggregatedIMNN`.\n",
    "\n",
    "    ``_IMNN`` is designed as the parent class for a range of specific case\n",
    "    IMNNs. There is a helper function (IMNN) which should return the correct\n",
    "    case when provided with the correct data. These different subclasses are:\n",
    "\n",
    "    :func:`~imnn.SimulatorIMNN`:\n",
    "\n",
    "        Fit an IMNN using simulations generated on-the-fly from a jax (XLA\n",
    "        compilable) simulator\n",
    "\n",
    "    :func:`~imnn.GradientIMNN`:\n",
    "\n",
    "        Fit an IMNN using a precalculated set of fiducial simulations and their\n",
    "        derivatives with respect to model parameters\n",
    "\n",
    "    :func:`~imnn.NumericalGradientIMNN`:\n",
    "\n",
    "        Fit an IMNN using a precalculated set of fiducial simulations and\n",
    "        simulations generated using parameter values just above and below the\n",
    "        fiducial parameter values to make a numerical estimate of the\n",
    "        derivatives of the network outputs. Best stability is achieved when\n",
    "        seeds of the simulations are matched between all parameter directions\n",
    "        for the numerical derivative\n",
    "\n",
    "    :func:`~imnn.AggregatedSimulatorIMNN`:\n",
    "\n",
    "        ``SimulatorIMNN`` distributed over multiple jax devices and gradients\n",
    "        aggregated manually. This might be necessary for very large input sizes\n",
    "        as batching cannot be done when calculating the Fisher information\n",
    "        matrix\n",
    "\n",
    "    :func:`~imnn.AggregatedGradientIMNN`:\n",
    "\n",
    "        ``GradientIMNN`` distributed over multiple jax devices and gradients\n",
    "        aggregated manually. This might be necessary for very large input sizes\n",
    "        as batching cannot be done when calculating the Fisher information\n",
    "        matrix\n",
    "\n",
    "    :func:`~imnn.AggregatedNumericalGradientIMNN`:\n",
    "\n",
    "        ``NumericalGradientIMNN`` distributed over multiple jax devices and\n",
    "        gradients aggregated manually. This might be necessary for very large\n",
    "        input sizes as batching cannot be done when calculating the Fisher\n",
    "        information matrix\n",
    "\n",
    "    :func:`~imnn.DatasetGradientIMNN`:\n",
    "\n",
    "        ``AggregatedGradientIMNN`` with prebuilt TensorFlow datasets\n",
    "\n",
    "    :func:`~imnn.DatasetNumericalGradientIMNN`:\n",
    "\n",
    "        ``AggregatedNumericalGradientIMNN`` with prebuilt TensorFlow datasets\n",
    "\n",
    "    There are currently two other parent classes\n",
    "\n",
    "    :func:`~imnn.imnn._aggregated_imnn.AggregatedIMNN`:\n",
    "\n",
    "        This is the parent class which provides the fitting routine when the\n",
    "        gradients of the network parameters are aggregated manually rather than\n",
    "        automatically by jax. This is necessary if the size of an entire batch\n",
    "        of simulations (and their derivatives with respect to model parameters)\n",
    "        and the network parameters and their calculated gradients is too large\n",
    "        to fit into memory. Note there is a significant performance loss from\n",
    "        using the aggregation so it should only be used for these large data\n",
    "        cases\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_s : int\n",
    "        Number of simulations used to calculate network output covariance\n",
    "    n_d : int\n",
    "        Number of simulations used to calculate mean of network output\n",
    "        derivative with respect to the model parameters\n",
    "    n_params : int\n",
    "        Number of model parameters\n",
    "    n_summaries : int\n",
    "        Number of summaries, i.e. outputs of the network\n",
    "    input_shape : tuple\n",
    "        The shape of a single input to the network\n",
    "    θ_fid : float(n_params,)\n",
    "        The value of the fiducial parameter values used to generate inputs\n",
    "    validate : bool\n",
    "        Whether a validation set is being used\n",
    "    simulate : bool\n",
    "        Whether input simulations are generated on the fly\n",
    "    _run_with_pbar : bool\n",
    "        Book keeping parameter noting that a progress bar is used when\n",
    "        fitting (induces a performance hit). If ``run_with_pbar = True``\n",
    "        and ``run_without_pbar = True`` then a jit compilation error will\n",
    "        occur and so it is prevented\n",
    "    _run_without_pbar : bool\n",
    "        Book keeping parameter noting that a progress bar is not used when\n",
    "        fitting. If ``run_with_pbar = True`` and ``run_without_pbar = True``\n",
    "        then a jit compilation error will occur and so it is prevented\n",
    "    F : float(n_params, n_params)\n",
    "        Fisher information matrix calculated from the network outputs\n",
    "    invF : float(n_params, n_params)\n",
    "        Inverse Fisher information matrix calculated from the network outputs\n",
    "    C : float(n_summaries, n_summaries)\n",
    "        Covariance of the network outputs\n",
    "    invC : float(n_summaries, n_summaries)\n",
    "        Inverse covariance of the network outputs\n",
    "    μ : float(n_summaries,)\n",
    "        Mean of the network outputs\n",
    "    dμ_dθ : float(n_summaries, n_params)\n",
    "        Derivative of the mean of the network outputs with respect to model\n",
    "        parameters\n",
    "    state : :obj:state\n",
    "        The optimiser state used for updating the network parameters and\n",
    "        optimisation algorithm\n",
    "    initial_w : list\n",
    "        List of the network parameters values at initialisation (to restart)\n",
    "    final_w : list\n",
    "        List of the network parameters values at the end of fitting\n",
    "    best_w : list\n",
    "        List of the network parameters values which provide the maxmimum value\n",
    "        of the determinant of the Fisher matrix\n",
    "    w : list\n",
    "        List of the network parameters values (either final or best depending\n",
    "        on setting when calling fit(...))\n",
    "    history : dict\n",
    "        A dictionary containing the fitting history. Keys are\n",
    "            - **detF** -- determinant of the Fisher information at the end of\n",
    "              each iteration\n",
    "            - **detC** -- determinant of the covariance of network outputs at\n",
    "              the end of each iteration\n",
    "            - **detinvC** -- determinant of the inverse covariance of network\n",
    "              outputs at the end of each iteration\n",
    "            - **Λ2** -- value of the covariance regularisation at the end of\n",
    "              each iteration\n",
    "            - **r** -- value of the regularisation coupling at the end of each\n",
    "              iteration\n",
    "            - **val_detF** -- determinant of the Fisher information of the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_detC** -- determinant of the covariance of network outputs\n",
    "              given the validation data at the end of each iteration\n",
    "            - **val_detinvC** -- determinant of the inverse covariance of\n",
    "              network outputs given the validation data at the end of each\n",
    "              iteration\n",
    "            - **val_Λ2** -- value of the covariance regularisation given the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_r** -- value of the regularisation coupling given the\n",
    "              validation data at the end of each iteration\n",
    "            - **max_detF** -- maximum value of the determinant of the Fisher\n",
    "              information on the validation data (if available)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    model:\n",
    "        Neural network as a function of network parameters and inputs\n",
    "    _get_parameters:\n",
    "        Function which extracts the network parameters from the state\n",
    "    _model_initialiser:\n",
    "        Function to initialise neural network weights from RNG and shape tuple\n",
    "    _opt_initialiser:\n",
    "        Function which generates the optimiser state from network parameters\n",
    "    _update:\n",
    "        Function which updates the state from a gradient\n",
    "\n",
    "    Todo\n",
    "    ----\n",
    "    - Finish all docstrings and documentation\n",
    "    - Update `NoiseNumericalGradientIMNN` to inherit from `_AggregatedIMNN`\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, θ_fid,\n",
    "                 model, optimiser, key_or_state, dummy_input=None, no_invC=False, do_reg=True,\n",
    "                 evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all _IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        θ_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float([None], input_shape)) -> float([None],\n",
    "            n_summaries)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        dummy_input : jraph.GraphsTuple or 'jax.numpy.DeviceArray'\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        self.dummy_input=dummy_input\n",
    "        self._initialise_parameters(\n",
    "            n_s, n_d, n_params, n_summaries, input_shape, θ_fid)\n",
    "        self._initialise_model(model, optimiser, key_or_state)\n",
    "        self._initialise_history()\n",
    "        self.no_invC=no_invC\n",
    "        self.do_reg=do_reg\n",
    "        self.evidence=evidence\n",
    "\n",
    "\n",
    "    def _initialise_parameters(self, n_s, n_d, n_params, n_summaries,\n",
    "                               input_shape, θ_fid):\n",
    "        \"\"\"Performs type checking and initialisation of class attributes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        θ_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            Any of the parameters are not correct type\n",
    "        ValueError\n",
    "            Any of the parameters are ``None``\n",
    "            ``Θ_fid`` has the wrong shape\n",
    "        \"\"\"\n",
    "        self.n_s = _check_type(n_s, int, \"n_s\")\n",
    "        self.n_d = _check_type(n_d, int, \"n_d\")\n",
    "        self.n_params = _check_type(n_params, int, \"n_params\")\n",
    "        self.n_summaries = _check_type(n_summaries, int, \"n_summaries\")\n",
    "        self.input_shape = _check_type(input_shape, tuple, \"input_shape\")\n",
    "        self.θ_fid = _check_input(θ_fid, (self.n_params,), \"θ_fid\")\n",
    "\n",
    "        self.validate = False\n",
    "        self.simulate = False\n",
    "        self._run_with_pbar = False\n",
    "        self._run_without_pbar = False\n",
    "\n",
    "        self.F = None\n",
    "        self.invF = None\n",
    "        self.C = None\n",
    "        self.invC = None\n",
    "        self.μ = None\n",
    "        self.dμ_dθ = None\n",
    "\n",
    "        self._model_initialiser = None\n",
    "        self.model = None\n",
    "        self._opt_initialiser = None\n",
    "        self._update = None\n",
    "        self._get_parameters = None\n",
    "        self.state = None\n",
    "        self.initial_w = None\n",
    "        self.final_w = None\n",
    "        self.best_w = None\n",
    "        self.w = None\n",
    "\n",
    "        self.history = None\n",
    "\n",
    "    def _initialise_model(self, model, optimiser, key_or_state):\n",
    "        \"\"\"Initialises neural network parameters or loads optimiser state\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and\n",
    "            the neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float([None], input_shape)) -> float([None],\n",
    "            n_summaries)``. (Essentibly stax-like, see `jax.experimental.stax\n",
    "            <https://jax.readthedocs.io/en/stable/jax.experimental.stax.html>`_\n",
    "            ))\n",
    "        optimiser : tuple or obj, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The design of the model follows `jax's stax module <https://jax.readth\n",
    "        edocs.io/en/latest/jax.experimental.stax.html>`_ in that the model is\n",
    "        encapsulated by two functions, one to initialise the network and one to\n",
    "        call the model, i.e.::\n",
    "\n",
    "            import jax\n",
    "            from jax.experimental import stax\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "\n",
    "            data_key, model_key = jax.random.split(rng)\n",
    "\n",
    "            input_shape = (10,)\n",
    "            inputs = jax.random.normal(data_key, shape=input_shape)\n",
    "\n",
    "            model = stax.serial(\n",
    "                stax.Dense(10),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(10),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(2))\n",
    "\n",
    "            output_shape, initial_params = model[0](model_key, input_shape)\n",
    "\n",
    "            outputs = model[1](initial_params, inputs)\n",
    "\n",
    "        Note that the model used in the IMNN is assumed to be totally\n",
    "        broadcastable, i.e. any batch shape can be used for inputs. This might\n",
    "        require having a layer which reshapes all batch dimensions into a\n",
    "        single dimension and then unwraps it at the last layer. A model such as\n",
    "        that above is already fully broadcastable.\n",
    "\n",
    "        The optimiser should follow `jax's experimental optimiser module <http\n",
    "        s://jax.readthedocs.io/en/stable/jax.experimental.optimizers.html>`_ in\n",
    "        that the optimiser is encapsulated by three functions, one to\n",
    "        initialise the state, one to update the state from a list of gradients\n",
    "        and one to extract the network parameters from the state, .i.e\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from jax.experimental import optimizers\n",
    "            import jax.numpy as np\n",
    "\n",
    "            optimiser = optimizers.adam(step_size=1e-3)\n",
    "\n",
    "            initial_state = optimiser[0](initial_params)\n",
    "            params = optimiser[2](initial_state)\n",
    "\n",
    "            def scalar_output(params, inputs):\n",
    "                return np.sum(model[1](params, inputs))\n",
    "\n",
    "            counter = 0\n",
    "            grad = jax.grad(scalar_output, argnums=0)(params, inputs)\n",
    "            state = optimiser[1](counter, grad, state)\n",
    "\n",
    "        This function either initialises the neural network or the state if\n",
    "        passed a stateless random number generator in ``key_or_state`` or loads\n",
    "        a predefined state if the state is passed to ``key_or_state``. The\n",
    "        functions get mapped to the class functions\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            self.model = model[1]\n",
    "            self._model_initialiser = model[0]\n",
    "\n",
    "            self._opt_initialiser = optimiser[0]\n",
    "            self._update = optimiser[1]\n",
    "            self._get_parameters = optimiser[2]\n",
    "\n",
    "        The state is made into the ``state`` class attribute and the parameters\n",
    "        are assigned to ``initial_w``, ``final_w``, ``best_w`` and ``w`` class\n",
    "        attributes (where ``w`` stands for weights).\n",
    "\n",
    "        There is some type checking done, but for freedom of choice of model\n",
    "        there will be very few raised warnings.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If the random number generator is not correct, or if there is no\n",
    "            possible way to construct a model or an optimiser from the passed\n",
    "            parameters\n",
    "        ValueError\n",
    "            If any input is ``None`` or if the functions for the model or\n",
    "            optimiser do not conform to the necessary specifications\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize FLAX model here\n",
    "        self._model_initialiser = model.init\n",
    "        self.model = model.apply\n",
    "\n",
    "        # unpack optimiser\n",
    "        self._opt_initialiser, self._update = optimiser\n",
    "\n",
    "        #state, key = _check_state(key_or_state)\n",
    "        key = key_or_state\n",
    "\n",
    "        if key is not None:\n",
    "            key = _check_input(key, (2,), \"key_or_state\")\n",
    "            if self.dummy_input is None:\n",
    "                dummy_x = jax.random.uniform(key, self.input_shape)\n",
    "            else:\n",
    "                dummy_x = self.dummy_input\n",
    "\n",
    "            # INITIAL PARAMS\n",
    "            self.initial_w = self._model_initialiser(key, dummy_x)\n",
    "            \n",
    "            # DUMMY OUTPUT\n",
    "            output = self.model(self.initial_w, dummy_x)\n",
    "            # check to see if right shape\n",
    "            _check_model_output(output.shape, (self.n_summaries,))\n",
    "            # INITIAL STATE\n",
    "            self.state = self._opt_initialiser(self.initial_w)\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.state = state\n",
    "            try:\n",
    "                self._get_parameters(self.state)\n",
    "            except Exception:\n",
    "                raise TypeError(\"`state` is not valid for extracting \" +\n",
    "                                \"parameters from\")\n",
    "\n",
    "        self.dummy_x = dummy_x\n",
    "        self.initial_w = self._model_initialiser(key, dummy_x)\n",
    "        self.final_w = self._model_initialiser(key, dummy_x)\n",
    "        self.best_w = self._model_initialiser(key, dummy_x)\n",
    "        self.w = self._model_initialiser(key, dummy_x)\n",
    "\n",
    "\n",
    "    def _initialise_history(self):\n",
    "        \"\"\"Initialises history dictionary attribute\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The contents of the history dictionary are\n",
    "            - **detF** -- determinant of the Fisher information at the end of\n",
    "              each iteration\n",
    "            - **detC** -- determinant of the covariance of network outputs at\n",
    "              the end of each iteration\n",
    "            - **detinvC** -- determinant of the inverse covariance of network\n",
    "              outputs at the end of each iteration\n",
    "            - **Λ2** -- value of the covariance regularisation at the end of\n",
    "              each iteration\n",
    "            - **r** -- value of the regularisation coupling at the end of each\n",
    "              iteration\n",
    "            - **val_detF** -- determinant of the Fisher information of the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_detC** -- determinant of the covariance of network outputs\n",
    "              given the validation data at the end of each iteration\n",
    "            - **val_detinvC** -- determinant of the inverse covariance of\n",
    "              network outputs given the validation data at the end of each\n",
    "              iteration\n",
    "            - **val_Λ2** -- value of the covariance regularisation given the\n",
    "              validation data at the end of each iteration\n",
    "            - **val_r** -- value of the regularisation coupling given the\n",
    "              validation data at the end of each iteration\n",
    "            - **max_detF** -- maximum value of the determinant of the Fisher\n",
    "              information on the validation data (if available)\n",
    "\n",
    "        \"\"\"\n",
    "        self.history = {\n",
    "            \"detF\": np.zeros((0,)),\n",
    "            \"detC\": np.zeros((0,)),\n",
    "            \"detinvC\": np.zeros((0,)),\n",
    "            \"Λ2\": np.zeros((0,)),\n",
    "            \"r\": np.zeros((0,)),\n",
    "            \"val_detF\": np.zeros((0,)),\n",
    "            \"val_detC\": np.zeros((0,)),\n",
    "            \"val_detinvC\": np.zeros((0,)),\n",
    "            \"val_Λ2\": np.zeros((0,)),\n",
    "            \"val_r\": np.zeros((0,)),\n",
    "            \"max_detF\": np.float32(0.)\n",
    "        }\n",
    "\n",
    "    def _set_history(self, results):\n",
    "        \"\"\"Places results from fitting into the history dictionary\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results : list\n",
    "            List of results from fitting procedure. These are:\n",
    "                - **detF** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  Fisher information, ``detF[:, 0]`` for training and\n",
    "                  ``detF[:, 1]`` for validation\n",
    "                - **detC** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  covariance of network outputs, ``detC[:, 0]`` for training\n",
    "                  and ``detC[:, 1]`` for validation\n",
    "                - **detinvC** *(float(n_iterations, 2))* -- determinant of the\n",
    "                  inverse covariance of network outputs, ``detinvC[:, 0]`` for\n",
    "                  training and ``detinvC[:, 1]`` for validation\n",
    "                - **Λ2** *(float(n_iterations, 2))* -- value of the covariance\n",
    "                  regularisation, ``Λ2[:, 0]`` for training and ``Λ2[:, 1]``\n",
    "                  for validation\n",
    "                - **r** *(float(n_iterations, 2))* -- value of the\n",
    "                  regularisation coupling, ``r[:, 0]`` for training and\n",
    "                  ``r[:, 1]`` for validation\n",
    "\n",
    "        \"\"\"\n",
    "        keys = [\"detF\", \"detC\", \"detinvC\", \"Λ2\", \"r\"]\n",
    "        for result, key in zip(results, keys):\n",
    "            self.history[key] = np.hstack([self.history[key], result[:, 0]])\n",
    "            if self.validate:\n",
    "                self.history[f\"val_{key}\"] = np.hstack(\n",
    "                    [self.history[f\"val_{key}\"], result[:, 1]])\n",
    "\n",
    "    def _set_inputs(self, rng, max_iterations):\n",
    "        \"\"\"Builds list of inputs for the XLA compilable fitting routine\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,) or None\n",
    "            A stateless random number generator\n",
    "        max_iterations : int\n",
    "            Maximum number of iterations to run the fitting procedure for\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The list of inputs to the routine are\n",
    "            - **max_detF** *(float)* -- The maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far. This is zero if\n",
    "              not run before or the value from previous calls to ``fit``\n",
    "            - **best_w** *(list)* -- The value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix. This is the initial network parameter values if not run\n",
    "              before\n",
    "            - **detF** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the Fisher information matrix during each\n",
    "              iteration of fitting. If there is no validation (for simulation\n",
    "              on-the-fly for example) then this container has a shape of\n",
    "              ``(max_iterations, 1)``, otherwise validation values are stored\n",
    "              in ``detF[:, 1]``.\n",
    "            - **detC** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the covariance of network outputs during\n",
    "              each iteration of fitting. If there is no validation (for\n",
    "              simulation on-the-fly for example) then this container has a\n",
    "              shape of ``(max_iterations, 1)``, otherwise validation values are\n",
    "              stored in ``detC[:, 1]``.\n",
    "            - **detF** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the determinant of the inverse covariance of network outputs\n",
    "              during each iteration of fitting. If there is no validation (for\n",
    "              simulation on-the-fly for example) then this container has a\n",
    "              shape of ``(max_iterations, 1)``, otherwise validation values are\n",
    "              stored in ``detinvC[:, 1]``.\n",
    "            - **Λ2** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the covariance regularisation during each iteration of\n",
    "              fitting. If there is no validation (for simulation on-the-fly for\n",
    "              example) then this container has a shape of\n",
    "              ``(max_iterations, 1)``, otherwise validation values are stored\n",
    "              in ``Λ2[:, 1]``.\n",
    "            - **r** *(float(max_iterations, 1) or\n",
    "              float(max_iterations, 2))* -- A container for all possible values\n",
    "              of the regularisation coupling strength during each iteration of\n",
    "              fitting. If there is no validation (for simulation on-the-fly for\n",
    "              example) then this container has a shape of\n",
    "              ``(max_iterations, 1),`` otherwise validation values are stored\n",
    "              in ``r[:, 1]``.\n",
    "            - **counter** *(int)* -- Iteration counter used to note whether the\n",
    "              while loop reaches ``max_iterations``. If not, the history\n",
    "              objects (above) get truncated to length ``counter``. This starts\n",
    "              with value zero\n",
    "            - **patience_counter** *(int)* -- Counts the number of iterations\n",
    "              where there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix, used for early stopping. This starts\n",
    "              with value zero\n",
    "            - **state** *(:obj:state)* -- The current optimiser state used for\n",
    "              updating the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- A stateless random number generator which\n",
    "              gets updated on each iteration\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        ``rng`` is currently only used for on-the-fly simulation but could\n",
    "        easily be updated to allow for stochastic models\n",
    "        \"\"\"\n",
    "        if self.validate:\n",
    "            shape = (max_iterations, 2)\n",
    "        else:\n",
    "            shape = (max_iterations, 1)\n",
    "\n",
    "        return (\n",
    "            self.history[\"max_detF\"], self.best_w, np.zeros(shape),\n",
    "            np.zeros(shape), np.zeros(shape), np.zeros(shape), np.zeros(shape),\n",
    "            np.int64(0), np.int64(0), self.state, self.w, rng)\n",
    "\n",
    "    def fit(self, λ, ϵ, γ=1000., rng=None, patience=100, min_iterations=100,\n",
    "            max_iterations=int(1e5), print_rate=None, best=True):\n",
    "        \"\"\"Fitting routine for the IMNN\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        ϵ : float\n",
    "            Closeness criterion describing how close to the 1 the determinant\n",
    "            of the covariance (and inverse covariance) of the network outputs\n",
    "            is desired to be\n",
    "        rng : int(2,) or None, default=None\n",
    "            Stateless random number generator\n",
    "        patience : int, default=10\n",
    "            Number of iterations where there is no increase in the value of the\n",
    "            determinant of the Fisher information matrix, used for early\n",
    "            stopping\n",
    "        min_iterations : int, default=100\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "        max_iterations : int, default=int(1e5)\n",
    "            Maximum number of iterations to run the fitting procedure for\n",
    "        print_rate : int or None, default=None,\n",
    "            Number of iterations before updating the progress bar whilst\n",
    "            fitting. There is a performance hit from updating the progress bar\n",
    "            more often and there is a large performance hit from using the\n",
    "            progress bar at all. (Possible ``RET_CHECK`` failure if\n",
    "            ``print_rate`` is not ``None`` when using GPUs).\n",
    "            For this reason it is set to None as default\n",
    "        best : bool, default=True\n",
    "            Whether to set the network parameter attribute ``self.w`` to the\n",
    "            parameter values that obtained the maximum determinant of\n",
    "            the Fisher information matrix or the parameter values at the final\n",
    "            iteration of fitting\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        We are going to summarise the mean and variance of some random Gaussian\n",
    "        noise with 10 data points per example using a SimulatorIMNN. In this\n",
    "        case we are going to generate the simulations on-the-fly with a\n",
    "        simulator written in jax (from the examples directory). We will use\n",
    "        1000 simulations to estimate the covariance of the network outputs and\n",
    "        the derivative of the mean of the network outputs with respect to the\n",
    "        model parameters (Gaussian mean and variance) and generate the\n",
    "        simulations at a fiducial μ=0 and Σ=1. The network will be a stax model\n",
    "        with hidden layers of ``[128, 128, 128]`` activated with leaky relu and\n",
    "        outputting 2 summaries. Optimisation will be via Adam with a step size\n",
    "        of ``1e-3``. Rather arbitrarily we'll set the regularisation strength\n",
    "        and covariance identity constraint to λ=10 and ϵ=0.1 (these are\n",
    "        relatively unimportant for such an easy model).\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            import jax\n",
    "            import jax.numpy as np\n",
    "            from jax.experimental import stax, optimizers\n",
    "            from imnn import SimulatorIMNN\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "\n",
    "            n_s = 1000\n",
    "            n_d = 1000\n",
    "            n_params = 2\n",
    "            n_summaries = 2\n",
    "            input_shape = (10,)\n",
    "            simulator_args = {\"input_shape\": input_shape}\n",
    "            θ_fid = np.array([0., 1.])\n",
    "\n",
    "            def simulator(rng, θ):\n",
    "                return θ[0] + jax.random.normal(\n",
    "                    rng, shape=input_shape) * np.sqrt(θ[1])\n",
    "\n",
    "            model = stax.serial(\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(128),\n",
    "                stax.LeakyRelu,\n",
    "                stax.Dense(n_summaries))\n",
    "            optimiser = optimizers.adam(step_size=1e-3)\n",
    "\n",
    "            λ = 10.\n",
    "            ϵ = 0.1\n",
    "\n",
    "            model_key, fit_key = jax.random.split(rng)\n",
    "\n",
    "            imnn = SimulatorIMNN(\n",
    "                n_s=n_s, n_d=n_d, n_params=n_params, n_summaries=n_summaries,\n",
    "                input_shape=input_shape, θ_fid=θ_fid, model=model,\n",
    "                optimiser=optimiser, key_or_state=model_key,\n",
    "                simulator=simulator)\n",
    "\n",
    "            imnn.fit(λ, ϵ, rng=fit_key, min_iterations=1000, patience=250,\n",
    "                     print_rate=None)\n",
    "\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        A minimum number of interations should be be run before stopping based\n",
    "        on a maximum determinant of the Fisher information achieved since the\n",
    "        loss function has dual objectives. Since the determinant of the\n",
    "        covariance of the network outputs is forced to 1 quickly, this can be\n",
    "        at the detriment to the value of the determinant of the Fisher\n",
    "        information matrix early in the fitting procedure. For this reason\n",
    "        starting early stopping after the covariance has converged is advised.\n",
    "        This is not currently implemented but could be considered in the\n",
    "        future.\n",
    "\n",
    "        The best fit network parameter values are probably not the most\n",
    "        representative set of parameters when simulating on-the-fly since there\n",
    "        is a high chance of a statistically overly-informative set of data\n",
    "        being generated. Instead, if using :func:`~imnn.SimulatorIMNN.fit()`\n",
    "        consider using ``best=False`` which sets ``self.w=self.final_w`` which\n",
    "        are the network parameter values obtained in the last iteration. Also\n",
    "        consider using a larger ``patience`` value if using\n",
    "        :func:`~imnn.SimulatorIMNN.fit()` to overcome the fact that a flukish\n",
    "        high value for the determinant might have been obtained due to the\n",
    "        realisation of the dataset.\n",
    "\n",
    "        Due to some unusual thing, that I can't work out, there is a massive\n",
    "        performance hit when calling ``jax.jit(self._fit)`` compared with\n",
    "        directly decorating ``_fit`` with\n",
    "        ``@partial(jax.jit(static_argnums=0))``. Unfortunately this means\n",
    "        having to duplicate ``_fit`` to include a version where the loop\n",
    "        condition is decorated with a progress bar because the ``tqdm``\n",
    "        module cannot use a jitted tracer. If the progress bar is not used then\n",
    "        the fully decorated jitted ``_fit`` function is used and it is super\n",
    "        quick. Otherwise, just the body of the loop is jitted so that the\n",
    "        condition function can be decorated by the progress bar (at the\n",
    "        expense of a performance hit). I imagine that something can be improved\n",
    "        here.\n",
    "\n",
    "        There is a chance of a ``RET_CHECK`` failure when using the progress\n",
    "        bar on GPUs (this doesn't seem to be a problem on CPUs). If this is the\n",
    "        case then `print_rate=None` should be used\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        _fit:\n",
    "            Main fitting function implemented as a ``jax.lax.while_loop``\n",
    "        _fit_pbar:\n",
    "            Main fitting function as a ``jax.lax.while_loop`` with progress bar\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If any input has the wrong type\n",
    "        ValueError\n",
    "            If any input (except ``rng`` and ``print_rate``) are ``None``\n",
    "        ValueError\n",
    "            If ``rng`` has the wrong shape\n",
    "        ValueError\n",
    "            If ``rng`` is ``None`` but simulating on-the-fly\n",
    "        ValueError\n",
    "            If calling fit with ``print_rate=None`` after previous call with\n",
    "            ``print_rate`` as an integer value\n",
    "        ValueError\n",
    "            If calling fit with ``print_rate`` as an integer after previous\n",
    "            call with ``print_rate=None``\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        - ``rng`` is currently only used for on-the-fly simulation but could\n",
    "          easily be updated to allow for stochastic models\n",
    "        - Automatic detection of convergence based on value ``r`` when early\n",
    "          stopping can be started\n",
    "        \"\"\"\n",
    "\n",
    "        @jax.jit\n",
    "        def _fit(inputs):\n",
    "\n",
    "            return jax.lax.while_loop(\n",
    "                partial(self._fit_cond, patience=patience,\n",
    "                        max_iterations=max_iterations),\n",
    "                partial(self._fit, λ=λ, α=α, γ=γ, min_iterations=min_iterations),\n",
    "                inputs)\n",
    "\n",
    "        def _fit_pbar(inputs):\n",
    "\n",
    "            return jax.lax.while_loop(\n",
    "                progress_bar(max_iterations, patience, print_rate)(\n",
    "                    partial(self._fit_cond, patience=patience,\n",
    "                            max_iterations=max_iterations)),\n",
    "                jax.jit(\n",
    "                    partial(self._fit, λ=λ, α=α,\n",
    "                            min_iterations=min_iterations)),\n",
    "                inputs)\n",
    "\n",
    "        λ = _check_type(λ, float, \"λ\")\n",
    "        ϵ = _check_type(ϵ, float, \"ϵ\")\n",
    "        γ = _check_type(γ, float, \"γ\")\n",
    "        α = self.get_α(λ, ϵ)\n",
    "        patience = _check_type(patience, int, \"patience\")\n",
    "        min_iterations = _check_type(min_iterations, int, \"min_iterations\")\n",
    "        max_iterations = _check_type(max_iterations, int, \"max_iterations\")\n",
    "        best = _check_boolean(best, \"best\")\n",
    "        if self.simulate and (rng is None):\n",
    "            raise ValueError(\"`rng` is necessary when simulating.\")\n",
    "        rng = _check_input(rng, (2,), \"rng\", allow_None=True)\n",
    "        inputs = self._set_inputs(rng, max_iterations)\n",
    "        if print_rate is None:\n",
    "            if self._run_with_pbar:\n",
    "                raise ValueError(\n",
    "                    \"Cannot run IMNN without progress bar after running \" +\n",
    "                    \"with progress bar. Either set `print_rate` to an int \" +\n",
    "                    \"or reinitialise the IMNN.\")\n",
    "            else:\n",
    "                self._run_without_pbar = True\n",
    "                results = _fit(inputs)\n",
    "        else:\n",
    "            if self._run_without_pbar:\n",
    "                raise ValueError(\n",
    "                    \"Cannot run IMNN with progress bar after running \" +\n",
    "                    \"without progress bar. Either set `print_rate` to None \" +\n",
    "                    \"or reinitialise the IMNN.\")\n",
    "            else:\n",
    "                print_rate = _check_type(print_rate, int, \"print_rate\")\n",
    "                self._run_with_pbar = True\n",
    "                results = _fit_pbar(inputs)\n",
    "        self.history[\"max_detF\"] = results[0]\n",
    "        self.best_w = results[1]\n",
    "        self._set_history(\n",
    "            (results[2][:results[7]],\n",
    "             results[3][:results[7]],\n",
    "             results[4][:results[7]],\n",
    "             results[5][:results[7]],\n",
    "             results[6][:results[7]]))\n",
    "        if len(results) == 12:\n",
    "            self.state = results[-3]\n",
    "        self.final_w = results[-2] #self._get_parameters(self.state)\n",
    "        if best:\n",
    "            w = self.best_w\n",
    "        else:\n",
    "            w = self.final_w\n",
    "        self.set_F_statistics(w, key=rng)\n",
    "\n",
    "    def _get_fitting_keys(self, rng):\n",
    "        \"\"\"Generates random numbers for simulation generation if needed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rng : int(2,) or None\n",
    "            A random number generator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int(2,), int(2,), int(2,) or None, None, None:\n",
    "            A new random number generator and random number generators for\n",
    "            training and validation, or empty values\n",
    "        \"\"\"\n",
    "        if rng is not None:\n",
    "            return jax.random.split(rng, num=3)\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    def get_α(self, λ, ϵ):\n",
    "        \"\"\"Calculate rate parameter for regularisation from closeness criterion\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        λ : float\n",
    "            coupling strength of the regularisation\n",
    "        ϵ : float\n",
    "            closeness criterion describing how close to the 1 the determinant\n",
    "            of the covariance (and inverse covariance) of the network outputs\n",
    "            is desired to be\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            The steepness of the tanh-like function (or rate) which determines\n",
    "            how fast the determinant of the covariance of the network outputs\n",
    "            should be pushed to 1\n",
    "        \"\"\"\n",
    "        return - math.log(ϵ * (λ - 1.) + ϵ ** 2. / (1 + ϵ)) / ϵ\n",
    "\n",
    "    def _fit(self, inputs, λ=None, α=None, γ=None,  min_iterations=None):\n",
    "        \"\"\"Single iteration fitting algorithm\n",
    "\n",
    "        This function performs the network parameter updates first getting\n",
    "        any necessary random number generators for simulators and then\n",
    "        extracting the network parameters from the state. These parameters\n",
    "        are used to calculate the gradient with respect to the network\n",
    "        parameters of the loss function (see _IMNN class docstrings).\n",
    "        Once the loss function is calculated the gradient is then used to\n",
    "        update the network parameters via the optimiser function and the\n",
    "        current iterations statistics are saved to the history arrays. If\n",
    "        validation is used (recommended for ``GradientIMNN`` and\n",
    "        ``NumericalGradientIMNN``) then all necessary statistics to\n",
    "        calculate the loss function are calculated and pushed to the\n",
    "        history arrays.\n",
    "\n",
    "        The ``patience_counter`` is increased if the value of determinant\n",
    "        of the Fisher information matrix does not increase over the\n",
    "        previous iterations upto ``patience`` number of iterations at which\n",
    "        point early stopping occurs, but only if the number of iterations\n",
    "        so far performed is greater than a specified ``min_iterations``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of the\n",
    "              Fisher information matrix calculated so far\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the Fisher information matrix\n",
    "            - **detC** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the determinant of the covariance of network\n",
    "              outputs\n",
    "            - **detinvC** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the inverse covariance of network outputs\n",
    "            - **Λ2** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the covariance regularisation\n",
    "            - **r** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the regularisation coupling strength\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **patience_counter** *(int)* -- Number of iterations where there\n",
    "              is no increase in the value of the determinant of the Fisher\n",
    "              information matrix\n",
    "            - **state** *(:obj: state)* -- Optimiser state used for updating\n",
    "              the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- Stateless random number generator\n",
    "\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Rate parameter for regularisation coupling\n",
    "        min_iterations : int\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            loop variables (described in Parameters)\n",
    "        \"\"\"\n",
    "\n",
    "        max_detF, best_w, detF, detC, detinvC, Λ2, r, \\\n",
    "            counter, patience_counter, state, w, rng = inputs\n",
    "        rng, training_key, validation_key = self._get_fitting_keys(rng)\n",
    "\n",
    "\n",
    "        grad, results = jax.grad(\n",
    "            self._get_loss, argnums=0, has_aux=True)(w, λ, α, γ, training_key)\n",
    "\n",
    "\n",
    "        updates, state = self._update(grad, state)\n",
    "\n",
    "        w = optax.apply_updates(w, updates) # UPDATE PARAMS\n",
    "\n",
    "        detF, detC, detinvC, Λ2, r = self._update_history(\n",
    "            results, (detF, detC, detinvC, Λ2, r), counter, 0)\n",
    "        if self.validate:\n",
    "            F, C, invC, *_ = self._get_F_statistics(\n",
    "                w, key=validation_key, validate=True)\n",
    "            _Λ2 = self._get_regularisation(C, invC)\n",
    "            _r = self._get_regularisation_strength(_Λ2, λ, α)\n",
    "            results = (F, C, invC, _Λ2, _r)\n",
    "            detF, detC, detinvC, Λ2, r = self._update_history(\n",
    "                results, (detF, detC, detinvC, Λ2, r), counter, 1)\n",
    "        _detF = np.linalg.det(results[0])\n",
    "        patience_counter, counter, _, max_detF, __, best_w = \\\n",
    "            jax.lax.cond(\n",
    "                np.greater(_detF, max_detF),\n",
    "                self._update_loop_vars,\n",
    "                lambda inputs: self._check_loop_vars(inputs, min_iterations),\n",
    "                (patience_counter, counter, _detF, max_detF, w, best_w))\n",
    "        return (max_detF, best_w, detF, detC, detinvC, Λ2, r,\n",
    "                counter + np.int32(1), patience_counter, state, w, rng)\n",
    "\n",
    "    def _fit_cond(self, inputs, patience, max_iterations):\n",
    "        \"\"\"Stopping condition for the fitting loop\n",
    "\n",
    "        The stopping conditions due to reaching ``max_iterations`` or the\n",
    "        patience counter reaching ``patience`` due to ``patience_counter``\n",
    "        number of iterations without increasing the determinant of the\n",
    "        Fisher information matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of the\n",
    "              Fisher information matrix calculated so far\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the Fisher information matrix\n",
    "            - **detC** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the determinant of the covariance of network\n",
    "              outputs\n",
    "            - **detinvC** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant of\n",
    "              the inverse covariance of network outputs\n",
    "            - **Λ2** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the covariance regularisation\n",
    "            - **r** *(float(max_iterations, 1) or float(max_iterations, 2))*\n",
    "              -- History of the regularisation coupling strength\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **patience_counter** *(int)* -- Number of iterations where there\n",
    "              is no increase in the value of the determinant of the Fisher\n",
    "              information matrix\n",
    "            - **state** *(:obj: state)* -- Optimiser state used for updating\n",
    "              the network parameters and optimisation algorithm\n",
    "            - **rng** *(int(2,))* -- Stateless random number generator\n",
    "\n",
    "        patience : int\n",
    "            Number of iterations to stop the fitting when there is no increase\n",
    "            in the value of the determinant of the Fisher information matrix\n",
    "        max_iterations : int\n",
    "        Maximum number of iterations to run the fitting procedure for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool:\n",
    "            True if either the ``patience_counter`` has not reached the\n",
    "            ``patience`` criterion or if the ``counter`` has not reached\n",
    "            ``max_iterations``\n",
    "        \"\"\"\n",
    "        return np.logical_and(\n",
    "            np.less(inputs[-4], patience),\n",
    "            np.less(inputs[-5], max_iterations))\n",
    "\n",
    "    def _update_loop_vars(self, inputs):\n",
    "        \"\"\"Updates input parameters if ``max_detF`` is increased\n",
    "\n",
    "        If the determinant of the Fisher information matrix calculated\n",
    "        in a given iteration is larger than the ``max_detF`` calculated\n",
    "        so far then the ``patience_counter`` is reset to zero and the\n",
    "        ``max_detF`` is replaced with the current value of ``detF`` and\n",
    "        the network parameters in this iteration replace the previous\n",
    "        parameters which obtained the highest determinant of the Fisher\n",
    "        information, ``best_w``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **patience_counter** *(int)* -- Number of iterations where\n",
    "              there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant\n",
    "              of the Fisher information matrix\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far\n",
    "            - **w** *(list)* -- Value of the network parameters which in\n",
    "              current iteration\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            (described in Parameters)\n",
    "        \"\"\"\n",
    "        patience_counter, counter, detF, max_detF, w, best_w = inputs\n",
    "        return (np.int32(0), counter, detF, detF, w, w)\n",
    "\n",
    "    def _check_loop_vars(self, inputs, min_iterations):\n",
    "        \"\"\"Updates ``patience_counter`` if ``max_detF`` not increased\n",
    "\n",
    "        If the determinant of the Fisher information matrix calculated\n",
    "        in a given iteration is not larger than the ``max_detF``\n",
    "        calculated so far then the ``patience_counter`` is increased by\n",
    "        one as long as the number of iterations is greater than the\n",
    "        minimum number of iterations that should be run.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            - **patience_counter** *(int)* -- Number of iterations where\n",
    "              there is no increase in the value of the determinant of the\n",
    "              Fisher information matrix\n",
    "            - **counter** *(int)* -- While loop iteration counter\n",
    "            - **detF** *(float(max_iterations, 1)\n",
    "              or float(max_iterations, 2))* -- History of the determinant\n",
    "              of the Fisher information matrix\n",
    "            - **max_detF** *(float)* -- Maximum value of the determinant of\n",
    "              the Fisher information matrix calculated so far\n",
    "            - **w** *(list)* -- Value of the network parameters which in\n",
    "              current iteration\n",
    "            - **best_w** *(list)* -- Value of the network parameters which\n",
    "              obtained the maxmimum determinant of the Fisher information\n",
    "              matrix\n",
    "        min_iterations : int\n",
    "            Number of iterations that should be run before considering early\n",
    "            stopping using the patience counter\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            (described in Parameters)\n",
    "        \"\"\"\n",
    "        patience_counter, counter, detF, max_detF, w, best_w = inputs\n",
    "        patience_counter = jax.lax.cond(\n",
    "            np.greater(counter, min_iterations),\n",
    "            lambda patience_counter: patience_counter + np.int32(1),\n",
    "            lambda patience_counter: patience_counter,\n",
    "            patience_counter)\n",
    "        return (patience_counter, counter, detF, max_detF, w, best_w)\n",
    "\n",
    "    def _update_history(self, inputs, history, counter, ind):\n",
    "        \"\"\"Puts current fitting statistics into history arrays\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : tuple\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **_Λ2** *(float)* -- Covariance regularisation\n",
    "                - **_r** *(float)* -- Regularisation coupling strength\n",
    "        history : tuple\n",
    "            History arrays containing fitting statistics for each iteration\n",
    "                - **detF** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Determinant of the Fisher\n",
    "                  information matrix\n",
    "                - **detC** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Determinant of the covariance\n",
    "                  of network outputs\n",
    "                - **detinvC** *(float(max_iterations, 1)\n",
    "                  or float(max_iterations, 2))* -- Determinant of the inverse\n",
    "                  covariance of network outputs\n",
    "                - **Λ2** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Covariance regularisation\n",
    "                - **r** *(float(max_iterations, 1) or\n",
    "                  float(max_iterations, 2))* -- Regularisation coupling\n",
    "                  strength\n",
    "        counter : int\n",
    "            Current iteration to insert a single iteration statistics into the\n",
    "            history\n",
    "        ind : int\n",
    "            Values of either 0 (fitting) or 1 (validation) to separate the\n",
    "            fitting and validation historys\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the Fisher information matrix\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the covariance of network outputs\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the determinant of the inverse covariance of network\n",
    "            outputs\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the covariance regularisation\n",
    "        float(max_iterations, 1) or float(max_iterations, 2):\n",
    "            History of the regularisation coupling strength\n",
    "        \"\"\"\n",
    "        F, C, invC, _Λ2, _r = inputs\n",
    "        detF, detC, detinvC, Λ2, r = history\n",
    "        detF = jax.ops.index_update(\n",
    "            detF,\n",
    "            jax.ops.index[counter, ind],\n",
    "            np.linalg.det(F))\n",
    "        detC = jax.ops.index_update(\n",
    "            detC,\n",
    "            jax.ops.index[counter, ind],\n",
    "            np.linalg.det(C))\n",
    "        detinvC = jax.ops.index_update(\n",
    "            detinvC,\n",
    "            jax.ops.index[counter, ind],\n",
    "            np.linalg.det(invC))\n",
    "        Λ2 = jax.ops.index_update(\n",
    "            Λ2,\n",
    "            jax.ops.index[counter, ind],\n",
    "            _Λ2)\n",
    "        r = jax.ops.index_update(\n",
    "            r,\n",
    "            jax.ops.index[counter, ind],\n",
    "            _r)\n",
    "        return detF, detC, detinvC, Λ2, r\n",
    "\n",
    "    def _slogdet(self, matrix):\n",
    "        \"\"\"Combined summed logarithmic determinant\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        matrix : float(n, n)\n",
    "            An n x n matrix to calculate the summed logarithmic determinant of\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            The summed logarithmic determinant multiplied by its sign\n",
    "        \"\"\"\n",
    "        lndet = np.linalg.slogdet(matrix)\n",
    "        return lndet[0] * lndet[1]\n",
    "\n",
    "    def _construct_derivatives(self, derivatives):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        An empty directive in ``_IMNN``, ``SimulatorIMNN`` and ``GradientIMNN``\n",
    "        but necessary to construct correct shaped derivatives when using\n",
    "        ``NumericalGradientIMNN``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivatives of the network ouputs with respect to the model\n",
    "            parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The derivatives of the network ouputs with respect to the model\n",
    "            parameters\n",
    "        \"\"\"\n",
    "        return derivatives\n",
    "\n",
    "    def set_F_statistics(self, w=None, key=None, validate=True):\n",
    "        \"\"\"Set necessary attributes for calculating score compressed summaries\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=True\n",
    "            Whether to calculate Fisher information using the validation set\n",
    "        \"\"\"\n",
    "        if validate and ((not self.validate) and (not self.simulate)):\n",
    "            validate = False\n",
    "        if w is not None:\n",
    "            self.w = w\n",
    "        self.F, self.C, self.invC, self.dμ_dθ, self.μ, self.F_loss = \\\n",
    "            self._get_F_statistics(key=key, validate=validate)\n",
    "        self.invF = np.linalg.inv(self.F)\n",
    "\n",
    "    def _get_F_statistics(self, w=None, key=None, validate=False):\n",
    "        \"\"\"Calculates the Fisher information and returns all statistics used\n",
    "\n",
    "        First gets the summaries and derivatives and then uses them to\n",
    "        calculate the Fisher information matrix from the outputs and return all\n",
    "        the necessary constituents to calculate the Fisher information (which)\n",
    "        are needed for the score compression or the regularisation of the loss\n",
    "        function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=True\n",
    "            Whether to calculate Fisher information using the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            - **F** *(float(n_params, n_params))* -- Fisher information matrix\n",
    "            - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "              network outputs\n",
    "            - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "              covariance of network outputs\n",
    "            - **dμ_dθ** *(float(n_summaries, n_params))* -- The derivative of\n",
    "              the mean of network outputs with respect to model parameters\n",
    "            - **μ** *(float(n_summaries,))* -- The mean of the network outputs\n",
    "\n",
    "        \"\"\"\n",
    "        if w is None:\n",
    "            w = self.w\n",
    "        summaries, derivatives = self.get_summaries(\n",
    "            w=w, key=key, validate=validate)\n",
    "        return self._calculate_F_statistics(summaries, derivatives)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _calculate_F_statistics(self, summaries, derivatives):\n",
    "        \"\"\"Calculates the Fisher information matrix from network outputs\n",
    "\n",
    "        If the numerical derivative is being calculated then the derivatives\n",
    "        are first constructed. If the mean is to be returned (for use in score\n",
    "        compression), this is calculated and pushed to the results tuple.\n",
    "        Then the covariance of the summaries is taken and inverted and the mean\n",
    "        of the derivative of network summaries with respect to the model\n",
    "        parameters is found and these are used to calculate the Gaussian form\n",
    "        of the Fisher information matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summaries : float(n_s, n_summaries)\n",
    "            The network outputs\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivative of the network outputs wrt the model parameters.\n",
    "            Note that when ``NumericalGradientIMNN`` is being used the shape is\n",
    "            ``float(n_d, 2, n_params, n_summaries)`` which is then constructed\n",
    "            into the the numerical derivative in ``_construct_derivatives``.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple:\n",
    "            - **F** *(float(n_params, n_params))* -- Fisher information matrix\n",
    "            - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "              network outputs\n",
    "            - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "              covariance of network outputs\n",
    "            - **dμ_dθ** *(float(n_summaries, n_params))* -- The derivative of\n",
    "              the mean of network outputs with respect to model parameters\n",
    "            - **μ** *(float(n_summaries))* -- The mean of the\n",
    "              network outputs\n",
    "        \"\"\"\n",
    "        derivatives = self._construct_derivatives(derivatives)\n",
    "        μ = np.mean(summaries, axis=0)\n",
    "        C = np.cov(summaries, rowvar=False)\n",
    "        if self.n_summaries == 1:\n",
    "            C = np.array([[C]])\n",
    "\n",
    "        invC = np.linalg.inv(C)\n",
    "\n",
    "        if self.no_invC:\n",
    "            invC_loss = np.eye(self.n_summaries)\n",
    "        else:\n",
    "            invC_loss = invC\n",
    "        dμ_dθ = np.mean(derivatives, axis=0)\n",
    "        F = np.einsum(\"ij,ik,kl->jl\", dμ_dθ, invC, dμ_dθ)\n",
    "\n",
    "        F_loss = np.einsum(\"ij,ik,kl->jl\", dμ_dθ, invC_loss, dμ_dθ)\n",
    "        return (F, C, invC, dμ_dθ, μ, F_loss)\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _get_regularisation_strength(self, Λ2, λ, α):\n",
    "        \"\"\"Coupling strength of the regularisation (amplified sigmoid)\n",
    "\n",
    "        To dynamically turn off the regularisation when the scale of the\n",
    "        covariance is set to approximately the identity matrix, a smooth\n",
    "        sigmoid conditional on the value of the regularisation is used. The\n",
    "        rate, α, is calculated from a closeness condition of the covariance\n",
    "        (and the inverse covariance) to the identity matrix using ``get_α``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Λ2 : float\n",
    "            Covariance regularisation\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Calculate rate parameter for regularisation from ϵ criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Smooth, dynamic regularisation strength\n",
    "        \"\"\"\n",
    "        return λ * Λ2 / (Λ2 + np.exp(-α * Λ2))\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def _get_regularisation(self, C, invC):\n",
    "        \"\"\"Difference of the covariance (and its inverse) from identity\n",
    "\n",
    "        The negative logarithm of the determinant of the Fisher information\n",
    "        matrix needs to be regularised to fix the scale of the network outputs\n",
    "        since any linear rescaling of a sufficient statistic is also a\n",
    "        sufficient statistic. We choose to fix this scale by constraining the\n",
    "        covariance of network outputs as\n",
    "\n",
    "        .. math::\n",
    "            \\\\Lambda_2 = ||\\\\bf{C}-\\\\bf{I}|| + ||\\\\bf{C}^{-1}-\\\\bf{I}||\n",
    "\n",
    "        One benefit of choosing this constraint is that it forces the\n",
    "        covariance to be approximately parameter independent which justifies\n",
    "        choosing the covariance independent Gaussian Fisher information.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        C : float(n_summaries, n_summaries)\n",
    "            Covariance of the network ouputs\n",
    "        invC : float(n_summaries, n_summaries)\n",
    "            Inverse covariance of the network ouputs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Regularisation loss terms for the distance of the covariance and\n",
    "            its determinant from the identity matrix\n",
    "        \"\"\"\n",
    "        if self.no_invC:\n",
    "            if self.evidence:\n",
    "                #reg = -(np.log(np.linalg.det(C)) - np.trace(C) + self.n_params)\n",
    "                reg = np.trace(C)\n",
    "            else:\n",
    "                reg = np.linalg.norm(C - np.eye(self.n_summaries))\n",
    "\n",
    "        else:\n",
    "            reg = np.linalg.norm(C - np.eye(self.n_summaries)) + \\\n",
    "                np.linalg.norm(invC - np.eye(self.n_summaries))\n",
    "        return reg\n",
    "\n",
    "    def _get_loss(self, w, λ, α, γ, key=None):\n",
    "        \"\"\"Calculates the loss function and returns auxillary variables\n",
    "\n",
    "        First gets the summaries and derivatives and then uses them to\n",
    "        calculate the loss function. This function is separated to be able to\n",
    "        use ``jax.grad`` directly rather than calculating the derivative of the\n",
    "        summaries as is done with ``_AggregatedIMNN``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Calculate rate parameter for regularisation from ϵ criterion\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Value of the regularised loss function\n",
    "        tuple:\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **Λ2** *(float)* -- Covariance regularisation\n",
    "                - **r** *(float)* -- Regularisation coupling strength\n",
    "\n",
    "        \"\"\"\n",
    "        summaries, derivatives = self.get_summaries(w=w, key=key)\n",
    "        return self._calculate_loss(summaries, derivatives, λ, α, γ)\n",
    "\n",
    "    def _calculate_loss(self, summaries, derivatives, λ, α, γ):\n",
    "        \"\"\"Calculates the loss function from network summaries and derivatives\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summaries : float(n_s, n_summaries)\n",
    "            The network outputs\n",
    "        derivatives : float(n_d, n_summaries, n_params)\n",
    "            The derivative of the network outputs wrt the model parameters.\n",
    "            Note that when ``NumericalGradientIMNN`` is being used the shape is\n",
    "            ``float(n_d, 2, n_params, n_summaries)`` which is then constructed\n",
    "            into the the numerical derivative in ``_construct_derivatives``.\n",
    "        λ : float\n",
    "            Coupling strength of the regularisation\n",
    "        α : float\n",
    "            Calculate rate parameter for regularisation from ϵ criterion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float:\n",
    "            Value of the regularised loss function\n",
    "        tuple:\n",
    "            Fitting statistics calculated on a single iteration\n",
    "                - **F** *(float(n_params, n_params))* -- Fisher information\n",
    "                  matrix\n",
    "                - **C** *(float(n_summaries, n_summaries))* -- Covariance of\n",
    "                  network outputs\n",
    "                - **invC** *(float(n_summaries, n_summaries))* -- Inverse\n",
    "                  covariance of network outputs\n",
    "                - **Λ2** *(float)* -- Covariance regularisation\n",
    "                - **r** *(float)* -- Regularisation coupling strength\n",
    "\n",
    "        \"\"\"\n",
    "        F, C, invC, dμ_dθ, _, F_loss = self._calculate_F_statistics(\n",
    "            summaries, derivatives)\n",
    "        lndetF = self._slogdet(F_loss)\n",
    "        Λ2 = self._get_regularisation(C, invC)\n",
    "        if self.do_reg:\n",
    "            r = self._get_regularisation_strength(Λ2, λ, α)\n",
    "        else:\n",
    "            r = γ*0.5\n",
    "        return - lndetF + r * Λ2, (F, C, invC, Λ2, r)\n",
    "\n",
    "    def get_summaries(self, w=None, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "        \"\"\"\n",
    "        raise ValueError(\"`get_summaries` not implemented\")\n",
    "\n",
    "    def get_estimate(self, d):\n",
    "        \"\"\"Calculate score compressed parameter estimates from network outputs\n",
    "\n",
    "        Using score compression we can get parameter estimates under the\n",
    "        transformation\n",
    "\n",
    "        .. math::\n",
    "            \\\\hat{\\\\boldsymbol{\\\\theta}}_\\\\alpha=\\\\theta^{\\\\rm{fid}}_\\\\alpha+\n",
    "            \\\\bf{F}^{-1}_{\\\\alpha\\\\beta}\\\\frac{\\\\partial\\\\mu_i}{\\\\partial\n",
    "            \\\\theta_\\\\beta}\\\\bf{C}^{-1}_{ij}(x(\\\\bf{w}, \\\\bf{d})-\\\\mu)_j\n",
    "\n",
    "        where :math:`x_j` is the :math:`j` output of the network with network\n",
    "        parameters :math:`\\\\bf{w}` and input data :math:`\\\\bf{d}`.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        Assuming that an IMNN has been fit (as in the example in\n",
    "        :py:meth:`imnn.imnn._imnn.IMNN.fit`) then we can obtain a\n",
    "        pseudo-maximum likelihood estimate of some target data (which is\n",
    "        generated with parameter values μ=1, Σ=2) using\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            rng, target_key = jax.random.split(rng)\n",
    "            target_data = model_simulator(target_key, np.array([1., 2.]))\n",
    "\n",
    "            imnn.get_estimate(target_data)\n",
    "            >>> DeviceArray([0.1108716, 1.7881424], dtype=float32)\n",
    "\n",
    "        The one standard deviation uncertainty on these parameter estimates\n",
    "        (assuming the fiducial is at the maximum-likelihood estimate - which we\n",
    "        know it isn't here) estimated by the square root of the inverse Fisher\n",
    "        information matrix is\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            np.sqrt(np.diag(imnn.invF))\n",
    "            >>> DeviceArray([0.31980422, 0.47132865], dtype=float32)\n",
    "\n",
    "        Note that we can compare the values estimated by the IMNN to the value\n",
    "        of the mean and the variance of the target data itself, which is what\n",
    "        the IMNN should be summarising\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            np.mean(target_data)\n",
    "            >>> DeviceArray(0.10693721, dtype=float32)\n",
    "\n",
    "            np.var(target_data)\n",
    "            >>> DeviceArray(1.70872, dtype=float32)\n",
    "\n",
    "        Note that batches of data can be summarised at once using\n",
    "        ``get_estimate``. In this example we will draw 10 different values of μ\n",
    "        from between :math:`-10 < \\\\mu < 10` and 10 different values of Σ from\n",
    "        between :math:`0 < \\\\Sigma < 10` and generate a batch of 10 different\n",
    "        input data which we can summarise using the IMNN.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            rng, mean_keys, var_keys = jax.random.split(rng, num=3)\n",
    "\n",
    "            mean_vals = jax.random.uniform(\n",
    "                mean_keys, minval=-10, maxval=10, shape=(10,))\n",
    "            var_vals = jax.random.uniform(\n",
    "                var_keys, minval=0, maxval=10, shape=(10,))\n",
    "\n",
    "            np.stack([mean_vals, var_vals], -1)\n",
    "            >>> DeviceArray([[ 3.8727236,  1.6727388],\n",
    "                             [-3.1113386,  8.14554  ],\n",
    "                             [ 9.87299  ,  1.4134324],\n",
    "                             [ 4.4837523,  1.5812075],\n",
    "                             [-9.398947 ,  3.5737753],\n",
    "                             [-2.0789695,  9.978279 ],\n",
    "                             [-6.2622285,  6.828809 ],\n",
    "                             [ 4.6470118,  6.0823894],\n",
    "                             [ 5.7369494,  8.856505 ],\n",
    "                             [ 4.248898 ,  5.114669 ]], dtype=float32)\n",
    "\n",
    "            batch_target_keys = np.array(jax.random.split(rng, num=10))\n",
    "\n",
    "            batch_target_data = jax.vmap(model_simulator)(\n",
    "                batch_target_keys, (mean_vals, var_vals))\n",
    "\n",
    "            imnn.get_estimate(batch_target_data)\n",
    "            >>> DeviceArray([[ 4.6041985,  8.344688 ],\n",
    "                             [-3.5172062,  7.7219954],\n",
    "                             [13.229679 , 23.668312 ],\n",
    "                             [ 5.745726 , 10.020965 ],\n",
    "                             [-9.734651 , 21.076218 ],\n",
    "                             [-1.8083427,  6.1901293],\n",
    "                             [-8.626409 , 18.894459 ],\n",
    "                             [ 5.7684307,  9.482665 ],\n",
    "                             [ 6.7861238, 14.128591 ],\n",
    "                             [ 4.900367 ,  9.472563 ]], dtype=float32)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d : float(None, input_shape)\n",
    "            Input data to be compressed to score compressed parameter estimates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(None, n_params):\n",
    "            Score compressed parameter estimates\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        single_element:\n",
    "            Returns a single score compressed summary\n",
    "        multiple_elements:\n",
    "            Returns a batch of score compressed summaries\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the Fisher statistics are not set after running ``fit`` or\n",
    "            ``set_F_statistics``.\n",
    "\n",
    "        Todo\n",
    "        ----\n",
    "        - Do proper checking on input shape (should just be a call to\n",
    "          ``_check_input``)\n",
    "        \"\"\"\n",
    "        @jax.jit\n",
    "        def single_element(d):\n",
    "            \"\"\"Returns a single score compressed summary\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            d : float(input_shape)\n",
    "                Input data to be compressed to score compressed parameter\n",
    "                estimate\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float(n_params,):\n",
    "                Score compressed parameter estimate\n",
    "            \"\"\"\n",
    "            return self.θ_fid + np.einsum(\n",
    "                \"ij,kj,kl,l->i\",\n",
    "                self.invF,\n",
    "                self.dμ_dθ,\n",
    "                self.invC,\n",
    "                self.model(self.w, d) - self.μ)\n",
    "\n",
    "        @jax.jit\n",
    "        def multiple_elements(d):\n",
    "            \"\"\"Returns a batch of score compressed summaries\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            d : float(None, input_shape)\n",
    "                Input data to be compressed to score compressed parameter\n",
    "                estimates\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            float(None, n_params):\n",
    "                Score compressed parameter estimates\n",
    "\n",
    "            Methods\n",
    "            -------\n",
    "            fn:\n",
    "                Returns the output of the evaluated model\n",
    "            \"\"\"\n",
    "            def fn(d):\n",
    "                \"\"\"Returns the output of the evaluated model\n",
    "\n",
    "                Parameters\n",
    "                ----------\n",
    "                d : float(input_shape)\n",
    "                    Input data to the neural network\n",
    "\n",
    "                Returns\n",
    "                -------\n",
    "                float(None, n_summaries):\n",
    "                    Neural network output\n",
    "                \"\"\"\n",
    "                return self.model(self.w, d)\n",
    "            return self.θ_fid + np.einsum(\n",
    "                \"ij,kj,kl,ml->mi\",\n",
    "                self.invF,\n",
    "                self.dμ_dθ,\n",
    "                self.invC,\n",
    "                jax.vmap(fn)(d) - self.μ)\n",
    "\n",
    "        _check_statistics_set(self.invF, self.dμ_dθ, self.invC, self.μ)\n",
    "        # check shape: array or graph ?\n",
    "        if self.dummy_input is None:\n",
    "          if len(d.shape) == 1:\n",
    "              return single_element(d)\n",
    "          else:\n",
    "              return multiple_elements(d)\n",
    "        else:\n",
    "            return single_element(d)\n",
    "\n",
    "    def _setup_plot(self, ax=None, expected_detF=None, figsize=(5, 15)):\n",
    "        \"\"\"Builds axes for history plot\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax : mpl.axes or None, default=None\n",
    "            An axes object of predefined axes to be labelled\n",
    "        expected_detF : float or None, default=None\n",
    "            Value of the expected determinant of the Fisher information to plot\n",
    "            a horizontal line at to check fitting progress\n",
    "        figsize : tuple, default=(5, 15)\n",
    "            The size of the figure to be produced\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mpl.axes:\n",
    "            An axes object of labelled axes\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(3, 1, sharex=True, figsize=figsize)\n",
    "            plt.subplots_adjust(hspace=0.05)\n",
    "        ax = [x for x in ax] + [ax[2].twinx()]\n",
    "        if expected_detF is not None:\n",
    "            ax[0].axhline(expected_detF, linestyle=\"dashed\", color=\"black\")\n",
    "        ax[0].set_ylabel(r\"$|{\\bf F}|$\")\n",
    "        ax[1].axhline(1, linestyle=\"dashed\", color=\"black\")\n",
    "        ax[1].set_ylabel(r\"$|{\\bf C}|$ and $|{\\bf C}^{-1}|$\")\n",
    "        ax[1].set_yscale(\"log\")\n",
    "        ax[2].set_xlabel(\"Number of iterations\")\n",
    "        ax[2].set_ylabel(r\"$\\Lambda_2$\")\n",
    "        ax[3].set_ylabel(r\"$r$\")\n",
    "        return ax\n",
    "\n",
    "    def plot(self, ax=None, expected_detF=None, colour=\"C0\", figsize=(5, 15),\n",
    "             label=\"\", filename=None, ncol=1):\n",
    "        \"\"\"Plot fitting history\n",
    "\n",
    "        Plots a three panel vertical plot with the determinant of the Fisher\n",
    "        information matrix in the first sublot, the covariance and the inverse\n",
    "        covariance in the second and the regularisation term and the\n",
    "        regularisation coupling strength in the final subplot.\n",
    "\n",
    "        A predefined axes can be passed to fill, and these axes can be\n",
    "        decorated via a call to ``_setup_plot`` (for horizonal plots for\n",
    "        example).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "\n",
    "        Assuming that an IMNN has been fit (as in the example in\n",
    "        :py:meth:`imnn.imnn._imnn.IMNN.fit`) then we can make a training plot\n",
    "        of the history by simply running\n",
    "\n",
    "        .. code-block::\n",
    "\n",
    "            imnn.fit(expected_detF=50, filename=\"history_plot.png\")\n",
    "\n",
    "        .. image:: /_images/history_plot.png\n",
    "\n",
    "        Note we know the analytic value of the determinant of the Fisher\n",
    "        information for this problem (:math:`|\\\\bf{F}|=50`) so we can add this\n",
    "        line to the plot too, and save the output as a png named\n",
    "        ``history_plot``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ax : mpl.axes or None, default=None\n",
    "            An axes object of predefined axes to be labelled\n",
    "        expected_detF : float or None, default=None\n",
    "            Value of the expected determinant of the Fisher information to plot\n",
    "            a horizontal line at to check fitting progress\n",
    "        colour : str or rgb/a value or list, default=\"C0\"\n",
    "            Colour to plot the lines\n",
    "        figsize : tuple, default=(5, 15)\n",
    "            The size of the figure to be produced\n",
    "        label : str, default=\"\"\n",
    "            Name to add to description in legend\n",
    "        filename : str or None, default=None\n",
    "            Filename to save plot to\n",
    "        ncol : int, default=1\n",
    "            Number of columns to have in the legend\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mpl.axes:\n",
    "            An axes object of the filled plot\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = self._setup_plot(expected_detF=expected_detF, figsize=figsize)\n",
    "        ax[0].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[0].plot(self.history[\"detF\"], color=colour,\n",
    "                   label=r\"{} $|F|$ (training)\".format(label))\n",
    "        ax[1].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[1].plot(self.history[\"detC\"], color=colour,\n",
    "                   label=r\"{} $|C|$ (training)\".format(label))\n",
    "        ax[1].plot(self.history[\"detinvC\"], linestyle=\"dotted\", color=colour,\n",
    "                   label=label + r\" $|C^{-1}|$ (training)\")\n",
    "        ax[3].set_xlim(\n",
    "            0, max(self.history[\"detF\"].shape[0] - 1, ax[0].get_xlim()[-1]))\n",
    "        ax[2].plot(self.history[\"Λ2\"], color=colour,\n",
    "                   label=r\"{} $\\Lambda_2$ (training)\".format(label))\n",
    "        ax[3].plot(self.history[\"r\"], color=colour, linestyle=\"dashed\",\n",
    "                   label=r\"{} $r$ (training)\".format(label))\n",
    "        if self.validate:\n",
    "            ax[0].plot(self.history[\"val_detF\"], color=colour,\n",
    "                       label=r\"{} $|F|$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[1].plot(self.history[\"val_detC\"], color=colour,\n",
    "                       label=r\"{} $|C|$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[1].plot(self.history[\"val_detinvC\"],\n",
    "                       color=colour,\n",
    "                       label=label + r\" $|C^{-1}|$ (validation)\",\n",
    "                       linestyle=\"dashdot\")\n",
    "            ax[2].plot(self.history[\"val_Λ2\"], color=colour,\n",
    "                       label=r\"{} $\\Lambda_2$ (validation)\".format(label),\n",
    "                       linestyle=\"dotted\")\n",
    "            ax[3].plot(self.history[\"val_r\"], color=colour,\n",
    "                       label=r\"{} $r$ (validation)\".format(label),\n",
    "                       linestyle=\"dashdot\")\n",
    "        h1, l1 = ax[2].get_legend_handles_labels()\n",
    "        h2, l2 = ax[3].get_legend_handles_labels()\n",
    "        ax[0].legend(bbox_to_anchor=(1.0, 1.0), frameon=False, ncol=ncol)\n",
    "        ax[1].legend(frameon=False, bbox_to_anchor=(1.0, 1.0), ncol=ncol * 2)\n",
    "        ax[3].legend(h1 + h2, l1 + l2, bbox_to_anchor=(1.05, 1.0),\n",
    "                     frameon=False, ncol=ncol * 2)\n",
    "\n",
    "        if filename is not None:\n",
    "            plt.savefig(filename, bbox_inches=\"tight\", transparent=True)\n",
    "        return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title numerical gradient imnn module\n",
    "\n",
    "import optax\n",
    "import jax.numpy as np\n",
    "# from imnn.imnn._imnn import _IMNN\n",
    "from imnn.utils.utils import _check_input\n",
    "\n",
    "class myNumericalGradientIMNN(_myIMNN):\n",
    "    \"\"\"Information maximising neural network fit using numerical derivatives\n",
    "\n",
    "    The outline of the fitting procedure is that a set of :math:`i\\\\in[1, n_s]`\n",
    "    simulations :math:`{\\\\bf d}^i` originally generated at fiducial model\n",
    "    parameter :math:`{\\\\bf\\\\theta}^\\\\rm{fid}`, and a set of\n",
    "    :math:`i\\\\in[1, n_d]` simulations,\n",
    "    :math:`\\\\{{\\\\bf d}_{\\\\alpha^-}^i, {\\\\bf d}_{\\\\alpha^+}^i\\\\}`, generated\n",
    "    with the same seed at each :math:`i` generated at\n",
    "    :math:`{\\\\bf\\\\theta}^\\\\rm{fid}` apart from at parameter label\n",
    "    :math:`\\\\alpha` with values\n",
    "\n",
    "    .. math::\n",
    "        \\\\theta_{\\\\alpha^-} = \\\\theta_\\\\alpha^\\\\rm{fid}-\\\\delta\\\\theta_\\\\alpha\n",
    "\n",
    "    and\n",
    "\n",
    "    .. math::\n",
    "        \\\\theta_{\\\\alpha^+} = \\\\theta_\\\\alpha^\\\\rm{fid}+\\\\delta\\\\theta_\\\\alpha\n",
    "\n",
    "    where :math:`\\\\delta\\\\theta_\\\\alpha` is a :math:`n_{params}` length vector\n",
    "    with the :math:`\\\\alpha` element having a value which perturbs the\n",
    "    parameter :math:`\\\\theta^{\\\\rm fid}_\\\\alpha`. This means there are\n",
    "    :math:`2\\\\times n_{params}\\\\times n_d` simulations used to calculate the\n",
    "    numerical derivatives (this is extremely cheap compared to other machine\n",
    "    learning methods). All these simulations are passed through a network\n",
    "    :math:`f_{{\\\\bf w}}({\\\\bf d})` with network parameters :math:`{\\\\bf w}` to\n",
    "    obtain network outputs :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\{{\\\\bf x}_{\\\\alpha^-}^i,{\\\\bf x}_{\\\\alpha^+}^i\\\\}`. These\n",
    "    perturbed values are combined to obtain\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial{{\\\\bf x}^i}}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "        \\\\frac{{\\\\bf x}_{\\\\alpha^+}^i - {\\\\bf x}_{\\\\alpha^-}^i}\n",
    "        {\\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "    With :math:`{\\\\bf x}^i` and\n",
    "    :math:`\\\\partial{{\\\\bf x}^i}/\\\\partial\\\\theta_\\\\alpha` the covariance\n",
    "\n",
    "    .. math::\n",
    "        C_{ab} = \\\\frac{1}{n_s-1}\\\\sum_{i=1}^{n_s}(x^i_a-\\\\mu^i_a)\n",
    "        (x^i_b-\\\\mu^i_b)\n",
    "\n",
    "    and the derivative of the mean of the network outputs with respect to the\n",
    "    model parameters\n",
    "\n",
    "    .. math::\n",
    "        \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha} = \\\\frac{1}{n_d}\n",
    "        \\\\sum_{i=1}^{n_d}\\\\frac{\\\\partial{x^i_a}}{\\\\partial\\\\theta_\\\\alpha}\n",
    "\n",
    "    can be calculated and used form the Fisher information matrix\n",
    "\n",
    "    .. math::\n",
    "        F_{\\\\alpha\\\\beta} = \\\\frac{\\\\partial\\\\mu_a}{\\\\partial\\\\theta_\\\\alpha}\n",
    "        C^{-1}_{ab}\\\\frac{\\\\partial\\\\mu_b}{\\\\partial\\\\theta_\\\\beta}.\n",
    "\n",
    "    The loss function is then defined as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda = -\\\\log|{\\\\bf F}| + r(\\\\Lambda_2) \\\\Lambda_2\n",
    "\n",
    "    Since any linear rescaling of a sufficient statistic is also a sufficient\n",
    "    statistic the negative logarithm of the determinant of the Fisher\n",
    "    information matrix needs to be regularised to fix the scale of the network\n",
    "    outputs. We choose to fix this scale by constraining the covariance of\n",
    "    network outputs as\n",
    "\n",
    "    .. math::\n",
    "        \\\\Lambda_2 = ||{\\\\bf C}-{\\\\bf I}|| + ||{\\\\bf C}^{-1}-{\\\\bf I}||\n",
    "\n",
    "    Choosing this constraint is that it forces the covariance to be\n",
    "    approximately parameter independent which justifies choosing the covariance\n",
    "    independent Gaussian Fisher information as above. To avoid having a dual\n",
    "    optimisation objective, we use a smooth and dynamic regularisation strength\n",
    "    which turns off the regularisation to focus on maximising the Fisher\n",
    "    information when the covariance has set the scale\n",
    "\n",
    "    .. math::\n",
    "        r(\\\\Lambda_2) = \\\\frac{\\\\lambda\\\\Lambda_2}{\\\\Lambda_2-\\\\exp\n",
    "        (-\\\\alpha\\\\Lambda_2)}.\n",
    "\n",
    "    Once the loss function is calculated the automatic gradient is then\n",
    "    calculated and used to update the network parameters via the optimiser\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    δθ : float(n_params,)\n",
    "        Size of perturbation to model parameters for the numerical derivative\n",
    "    fiducial : float(n_s, input_shape)\n",
    "        The simulations generated at the fiducial model parameter values used\n",
    "        for calculating the covariance of network outputs (for fitting)\n",
    "    derivative : float(n_d, 2, n_params, input_shape)\n",
    "        The simulations generated at parameter values perturbed from the\n",
    "        fiducial used to calculate the numerical derivative of network outputs\n",
    "        with respect to model parameters (for fitting)\n",
    "    validation_fiducial : float(n_s, input_shape) or None\n",
    "        The simulations generated at the fiducial model parameter values used\n",
    "        for calculating the covariance of network outputs (for validation)\n",
    "    validation_derivative : float(n_d, 2, n_params, input_shape) or None\n",
    "        The simulations generated at parameter values perturbed from the\n",
    "        fiducial used to calculate the numerical derivative of network outputs\n",
    "        with respect to model parameters (for validation)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_s, n_d, n_params, n_summaries, input_shape, θ_fid,\n",
    "                 model, optimiser, key_or_state, fiducial, derivative, δθ,\n",
    "                 validation_fiducial=None, validation_derivative=None, \n",
    "                 dummy_input=None,\n",
    "                 no_invC=False, do_reg=True, evidence=False):\n",
    "        \"\"\"Constructor method\n",
    "\n",
    "        Initialises all IMNN attributes, constructs neural network and its\n",
    "        initial parameter values and creates history dictionary. Also fills the\n",
    "        simulation attributes (and validation if available).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_s : int\n",
    "            Number of simulations used to calculate summary covariance\n",
    "        n_d : int\n",
    "            Number of simulations used to calculate mean of summary derivative\n",
    "        n_params : int\n",
    "            Number of model parameters\n",
    "        n_summaries : int\n",
    "            Number of summaries, i.e. outputs of the network\n",
    "        input_shape : tuple\n",
    "            The shape of a single input to the network\n",
    "        θ_fid : float(n_params,)\n",
    "            The value of the fiducial parameter values used to generate inputs\n",
    "        model : tuple, len=2\n",
    "            Tuple containing functions to initialise neural network\n",
    "            ``fn(rng: int(2), input_shape: tuple) -> tuple, list`` and the\n",
    "            neural network as a function of network parameters and inputs\n",
    "            ``fn(w: list, d: float(None, input_shape)) -> float(None, n_summari\n",
    "            es)``.\n",
    "            (Essentibly stax-like, see `jax.experimental.stax <https://jax.read\n",
    "            thedocs.io/en/stable/jax.experimental.stax.html>`_))\n",
    "        optimiser : tuple, len=3\n",
    "            Tuple containing functions to generate the optimiser state\n",
    "            ``fn(x0: list) -> :obj:state``, to update the state from a list of\n",
    "            gradients ``fn(i: int, g: list, state: :obj:state) -> :obj:state``\n",
    "            and to extract network parameters from the state\n",
    "            ``fn(state: :obj:state) -> list``.\n",
    "            (See `jax.experimental.optimizers <https://jax.readthedocs.io/en/st\n",
    "            able/jax.experimental.optimizers.html>`_)\n",
    "        key_or_state : int(2) or :obj:state\n",
    "            Either a stateless random number generator or the state object of\n",
    "            an preinitialised optimiser\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, 2, n_params, input_shape)\n",
    "            The simulations generated at parameter values perturbed from the\n",
    "            fiducial used to calculate the numerical derivative of network\n",
    "            outputs with respect to model parameters (for fitting)\n",
    "        δθ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation)\n",
    "        validation_derivative : float(n_d, 2, n_params, input_shape) or None\n",
    "            The simulations generated at parameter values perturbed from the\n",
    "            fiducial used to calculate the numerical derivative of network\n",
    "            outputs with respect to model parameters (for validation)\n",
    "        dummy_input : jraph.GraphsTuple or jax.numpy.DeviceArray\n",
    "            Either a (padded) graph input or device array. If supplied ignores \n",
    "            `input_shape` parameter\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            n_s=n_s,\n",
    "            n_d=n_d,\n",
    "            n_params=n_params,\n",
    "            n_summaries=n_summaries,\n",
    "            input_shape=input_shape,\n",
    "            θ_fid=θ_fid,\n",
    "            model=model,\n",
    "            key_or_state=key_or_state,\n",
    "            optimiser=optimiser,\n",
    "            dummy_input=dummy_input,\n",
    "            no_invC=no_invC,\n",
    "            do_reg=do_reg,\n",
    "            evidence=evidence)\n",
    "        self._set_data(δθ, fiducial, derivative, validation_fiducial,\n",
    "                       validation_derivative)\n",
    "        self.dummy_input = dummy_input\n",
    "\n",
    "    def _set_data(self, δθ, fiducial, derivative, validation_fiducial,\n",
    "                  validation_derivative):\n",
    "        \"\"\"Checks and sets data attributes with the correct shape\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        δθ : float(n_params,)\n",
    "            Size of perturbation to model parameters for the numerical\n",
    "            derivative\n",
    "        fiducial : float(n_s, input_shape)\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for fitting)\n",
    "        derivative : float(n_d, input_shape, n_params)\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for fitting)\n",
    "        validation_fiducial : float(n_s, input_shape) or None, default=None\n",
    "            The simulations generated at the fiducial model parameter values\n",
    "            used for calculating the covariance of network outputs\n",
    "            (for validation). Sets ``validate = True`` attribute if provided\n",
    "        validation_derivative : float(n_d, input_shape, n_params) or None\n",
    "            The derivative of the simulations with respect to the model\n",
    "            parameters (for validation). Sets ``validate = True`` attribute if\n",
    "            provided\n",
    "        \"\"\"\n",
    "        self.δθ = np.expand_dims(\n",
    "            _check_input(δθ, (self.n_params,), \"δθ\"), (0, 1))\n",
    "        if self.dummy_input is None:\n",
    "          self.fiducial = _check_input(\n",
    "              fiducial, (self.n_s,) + self.input_shape, \"fiducial\")\n",
    "          self.derivative = _check_input(\n",
    "              derivative, (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "              \"derivative\")\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = _check_input(\n",
    "                  validation_fiducial, (self.n_s,) + self.input_shape,\n",
    "                  \"validation_fiducial\")\n",
    "              self.validation_derivative = _check_input(\n",
    "                  validation_derivative,\n",
    "                  (self.n_d, 2, self.n_params) + self.input_shape,\n",
    "                  \"validation_derivative\")\n",
    "              self.validate = True\n",
    "        else:\n",
    "          self.fiducial = fiducial\n",
    "          self.derivative = derivative\n",
    "\n",
    "          if ((validation_fiducial is not None)\n",
    "                  and (validation_derivative is not None)):\n",
    "              self.validation_fiducial = validation_fiducial\n",
    "              self.validation_derivative =  validation_derivative\n",
    "              self.validate = True\n",
    "\n",
    "\n",
    "    def _collect_input(self, key, validate=False):\n",
    "        \"\"\" Returns validation or fitting sets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        key : None or int(2,)\n",
    "            Random number generators not used in this case\n",
    "        validate : bool\n",
    "            Whether to return the set for validation or for fitting\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, input_shape):\n",
    "            The fiducial simulations for fitting or validation\n",
    "        float(n_d, 2, n_params, input_shape):\n",
    "            The derivative simulations for fitting or validation\n",
    "        \"\"\"\n",
    "        if validate:\n",
    "            fiducial = self.validation_fiducial\n",
    "            derivative = self.validation_derivative\n",
    "        else:\n",
    "            fiducial = self.fiducial\n",
    "            derivative = self.derivative\n",
    "        return fiducial, derivative\n",
    "\n",
    "    def get_summaries(self, w, key=None, validate=False):\n",
    "        \"\"\"Gets all network outputs and derivatives wrt model parameters\n",
    "\n",
    "        Selects either the fitting or validation sets and passes them through\n",
    "        the network to get the network outputs. For the numerical derivatives,\n",
    "        the array is first flattened along the batch axis before being passed\n",
    "        through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : list or None, default=None\n",
    "            The network parameters if wanting to calculate the Fisher\n",
    "            information with a specific set of network parameters\n",
    "        key : int(2,) or None, default=None\n",
    "            A random number generator for generating simulations on-the-fly\n",
    "        validate : bool, default=False\n",
    "            Whether to get summaries of the validation set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_s, n_summaries):\n",
    "            The set of all network outputs used to calculate the covariance\n",
    "        float(n_d, 2, n_params, n_summaries):\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "        \"\"\"\n",
    "        d, d_mp = self._collect_input(key, validate=validate)\n",
    "        \n",
    "        \n",
    "        if self.dummy_input is None:\n",
    "          x = self.model(w, d)\n",
    "          x_mp = np.reshape(\n",
    "              self.model(\n",
    "                  w, d_mp.reshape(\n",
    "                      (self.n_d * 2 * self.n_params,) + self.input_shape)),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "        else:\n",
    "          # if operating on graph data, we need to vmap the implicit\n",
    "          # batch dimension\n",
    "          _model = lambda d: self.model(w, d)\n",
    "          x = jax.vmap(_model)(d)\n",
    "          x_mp = np.reshape(\n",
    "              jax.vmap(_model)(d_mp),\n",
    "              (self.n_d, 2, self.n_params, self.n_summaries))\n",
    "\n",
    "        return x, x_mp\n",
    "\n",
    "    def _construct_derivatives(self, x_mp):\n",
    "        \"\"\"Builds derivatives of the network outputs wrt model parameters\n",
    "\n",
    "        The network outputs from the simulations generated with model parameter\n",
    "        values above and below the fiducial are subtracted from each other and\n",
    "        divided by the perturbation size in each model parameter value. The\n",
    "        axes are swapped such that the derivatives with respect to parameters\n",
    "        are in the last axis.\n",
    "\n",
    "        .. math::\n",
    "            \\\\frac{\\\\partial{\\\\bf x}^i}{\\\\partial\\\\theta_\\\\alpha} =\n",
    "            \\\\frac{{\\\\bf x}^i_{\\\\alpha^+}-{\\\\bf x}^i_{\\\\alpha^+}}{\n",
    "            \\\\delta\\\\theta_\\\\alpha}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        derivatives : float(n_d, 2, n_params, n_summaries)\n",
    "            The outputs of the network of simulations made at perturbed\n",
    "            parameter values to construct the derivative of the network outputs\n",
    "            with respect to the model parameters numerically\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float(n_d, n_summaries, n_params):\n",
    "            The numerical derivatives of the network ouputs with respect to the\n",
    "            model parameters\n",
    "        \"\"\"\n",
    "        return np.swapaxes(x_mp[:, 1] - x_mp[:, 0], 1, 2) / self.δθ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(n_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(key)\n",
    "IMNN = myNumericalGradientIMNN(\n",
    "    n_s=n_s, n_d=n_d, n_params=n_params, n_summaries=n_summaries,\n",
    "    input_shape=input_shape, θ_fid=θ_fid, model=model,\n",
    "    optimiser=optimiser, key_or_state=jnp.array(key), δθ=δθ,\n",
    "    fiducial=fiducial, derivative=numerical_derivative,\n",
    "    validation_fiducial=validation_fiducial,\n",
    "    validation_derivative=validation_numerical_derivative, \n",
    "    dummy_input=graph,\n",
    "    no_invC=False,\n",
    "    do_reg=True,\n",
    "    evidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.01 , 0.015], dtype=float32)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "δθ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMNN.set_F_statistics(w=initial_w, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(6100.933, dtype=float32)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(IMNN.F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makinen/venvs/pmwd/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py:3556: UserWarning: Explicitly requested dtype int64 requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"array\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 320 ms, total: 1min 21s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "key,rng = jax.random.split(rng)\n",
    "IMNN.fit(10.0, 0.1, γ=100., rng=np.array(key), patience=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAANcCAYAAACQV5GKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADcC0lEQVR4nOzdd3xUVf7/8dfJpPdGDSV0DB1CV8AOi4gde1kF3V23r99lm7rrurq/7e7qKiriuq6KWLE3FClSBek9kNBCEtLrzJzfH5PEgJSEzGQmmffz8ZgHc+/cOfczmUnmwznnfo6x1iIiIiIijRfi7wBEREREWhslUCIiIiJNpARKREREpImUQImIiIg0kRIoERERkSYK9XcALSk1NdWmp6f7OwwRaWPWrFmTZ61t5+84RKTlBFUClZ6ezurVq/0dhoi0McaYvf6OQURalobwRERERJpICZSIiIhIEymBEhEREWkiJVAiIiIiTaQESkRERKSJlECJiIiINJESKBEREZEmUgIlIiIi0kRKoERERESaKKgqkYuIeIu1lqz8clbszvd3KCLiB0qgREQawe22bM8tYeWeAlbuKWBVVgGHi6v8HVaTGGM+BW611mYZY+4E7gcO1z680Vp7Y8NjjntuFPAecB4QB1xvrX3sDGJYZq0d563jTvC8cOAj4DxrrbOpzxdprFaRQBljegK/AhKstVfV7usGPAIUANuttQ/7MUQRaWNqXG427i+qT5ZWZR2lqKIGgE4JkYzukcKoHsmM6ZlMnz/6OdgzMwj4tbX26UYe/23gVWutyxiTCHwX+EYCZYwxgLHWuk/USGOTojNJnmqfV22M+RiYATx/Jm2INIbfEihjzFzgEiDXWjuwwf7JwD8AB/CUtfZha+1u4HZjzIIGTQwCFlhr/2uMeaklYxeRtqei2sWX+46yMsvTw/TlvkIqalwA9EyNYcrAjoxMT2ZUj2S6JEXhyRMaJ332238Hhno55HVZD0/9UTOePxh4pgnH3wBcX3v/YaCXMWYd8CHwKPA+sAIYAXzLGPMPoCsQCfzDWjsHwBhTaq2NNcakA+8CS4BxwH5gurW2ou44YOBpjvkNcCNwBMgG1lhr/wy8DjyEEijxIX/2QM0D/gX8p26HMcaB5xfxQiAHWGWMedNau/kEz/8CWGCM+TbwnO/DFZG2pKi8htV7C+oTpo37i6hxWYyBszrGM2NkV0b1SGZkejLt4iL8Ha4vDACeMca4gTxr7QUnO7B2WKxng2G92cBAa+3Q2sfTgT7ALdbaL2r3fdtaW1A79LfKGPOKtfb4CWN9gOustTONMfOBK4H/NuYYY8zI2vtDgDBgLbCm9jkbgZFN+3GINI3fEihr7eLaX7qGRgE7a3ucMMa8CEwHTpRA3QbcV9vOAk7yPyljzCxgFkC3bt28FL2ItDa5xZWszCpg1Z4CVuwpYNvhEqyFMIdhSJdE7jinJ6N6JDOiexLxkWFePXcze4q8zhjTFThkrR3cyKekAoWnOWZvXfJU6wfGmMtr73fFkwgdn0Dtsdauq72/Bkg/QbsnO2Y88Ia1thKoNMYsrHtC7TBjtTEmzlpbcpq4Rc5IoM2BSsPTDVsnBxhtjEkBHgSGGWN+Ya19CM9kxvuNMdcDWSdrsLbbeA5AZmam9VXgIhI4rLVkF1SwMquAFbvzWZVVQFZ+OQDR4Q5GdE/iW4M6MapHMkO7JhIZ5vBzxC1uELCpCcdX4BmKO5WyujvGmEnABcBYa2157cT0Ez2/4Sx8FxB1hsecSARQ2chjRZos0BKoE6rt9r3ruH0bgav8E5GIBBK327LzSCkraq+QW51VwMEiz3dnYnQYmd2TuWF0d0b1SCajczxhjqAvgTeYJiRQ1tqjxhiHMSaytsenBM+VeCeTABytTZ76A2OaF+4JLQWeMMY8hOe77BJq/7Nc+5/uPGttjQ/OKwIEXgK1H09Xb50utftEROo5XW62HCxhxZ78+qvkjpZ7vis7xEcwMj2Z0T2SGdkjmb7t4wgJafyE7yAxCM9E66b4ADgb+Mham2+MWWqM2Yhnkvejxx37HnCXMWYLsA3PnFWvstauMsa8CXyFpxTDBqCo9uFzgbe9fU6RhgItgVoF9DHG9MCTOF3L11d9iEiQqna62bC/kC92e3qY1uw9SmmVp8RP95RoLjirA6N6JDO6Rwpdk5t2hVwwstbecAZPexT4MZ4aS1hrj//bXH81tbW2CphyknPH1v6bddxz/nyi4051DPBna+39xphoYDFfTyK/Hs9EdxGf8WcZgxeASUCqMSYHz4Twp40xd+O5HNYBzLXWNmWcXkTagLqSAnVDcmv3HaXK6Skr1LdDLJcN68yoHimMSk+mY8LppuaIN1hr1xpjFhljHNZal7/jqTXHGJOBZ37Vs7UxhgOvW2u3+zk2aeOMtcEzrzozM9OuXr3a32GIyHGKK2tYk1WXMOXzVU4RTrclxEBG53hGpXuKVo7qkUxyTLi/w/0GY8waa22mv+M4HWPMrXiSi8LmHCMigTeEJyJBIL+0ilVZBfU9TFsOFuOuLSkwuEsiMyf4rqRAMLPWzvPGMSKiBEpEWsDBogpW7vk6YdqZWwpAZFgIw7sl8YPz+zCqRzLDuiYRFR50JQVEpBVSAiUiXmWtZV9BOSv2FLBidwErs/LJLqgAIC4ilMz0JK4c3oVRPZIZlJZAeGjQlxQQkVZICZSINMvxNZhW7snncLGn9mFSdBijeiRz67gejO6RzFmd4nGopICItAFKoESkSU5Xg2l0j5TakgLJ9G4fq5ICItImKYESkVOqcrrYkFNU38OkGkwiIkqgROQ4FdUu1u77uqTAl/sKVYNJROQ4SqBEglxd0crlu/P5Ync+67ILqXF5ajAN6JzAjWM8a8iNTA/MGkwiIv6gBEokyFTWuFi79yhf7M5nzb6jrMo6SrXTjSPEMDAtgW+f3YMxPVIYka4aTG2NMeZT4FZrbZYx5k7gfjzryAFstNbe2PCY454bhWeNu/POpBK5Maa0bnkWY8wya+24ExxzP1B6giVb6h5PBK631j7WYN8J22pEPOF4lqU5z1rrbOrzRZRAibRxVU4X6/YVsnx3Pst35fNldiHVTjchBs7qFM8No7txTp9URqYnE6eEKZgMAn5trX26kcd/G3jVG8u4nEnCUysR+C5Qn0CdaVvW2mpjzMfADOD5M4xHgpgSKJE2xuW2bNhfxNKdeXyxO59VWQVU1rgxBgZ0jueWsd0Z2ytFCVMLS5/99qfAvKyHp85Ln/12GPAh8FTWw1P/mz777WjgHeDfWQ9PfSl99tsJwBvAI1kPT301ffbbqcAC4C9ZD09dmD777Y5ZD0891MyQBgPPNOH4G6hd3N0Y8zCQba19tHb7fmp7jowxrwNd8axP9w9r7ZzjGzquN+pXwC1ALpBN7YLAJ2nnYaCXMWYd8KG19p7j2voJnkQP4Clr7d+NMenAu8ASYByeheqnW2srgNeBh1ACJWdACZRIK2etZdeRMpbuzGPpzjyW786npNIzItGvQxzXjuzGuF4pjO6RQkK0EiapNwB4xhjjBvKstRec7MDa4a6eDYb1XgL+Djxau30NcHHt/W9bawtqh/xWGWNesdbmn6TdEcC1wFA830drqU2gTtQOMBsYaK0depK2bgNGAwZYYYz5DDgK9AGus9bONMbMB64E/gtsBEae/EckcnJKoERaoUNFlZ6EaZcnaaorXJmWGMW3BnZifJ9UxvZMoV1chJ8jbX2qnW6eX7GXfh3jGNcr1WvtZj08dVKD+zVAw+3y47aLjtvOO267Wb1PxpiuwCFr7eBGPiUVKKzbsNZ+aYxpb4zpDLQDjlprs2sf/oEx5vLa+13xJC8nTKCAc4DXrLXltXG92eCxE7Vzqtd9dm1bZbVtvVrb/pvAHmvtutrj1gDpta/DZYypNsbEWWtLTtG2yDcogRJpBQrLq1m+K5+lu/JYtiuf3UfKAE+l73G9UhnfO5XxvVPolhytOkxn4K8fbqdzQiTXjupGmMPwyMc7uHxYF8b1SqWsysmPX1rH3ef1ZnCXRH+H6i2DgE1NOL4Cz1BaQy8DVwEd8fRIYYyZBFwAjLXWltdOSG9yrQtvtdNAVYP7LiCqwXYEUNmMtiVIKYESCUDl1U5W7ilg2a58lu3KY9OBYqyF6HAHmenJXDuyK+N6pZLRKZ4QLY3SZP/8eAeFFTX85pIMAFbtKaBrchTXjuqGMYZFP5tEYrSnZENWfhkb9hdRWeP2Z8jeNpgmJFDW2qPGGIcxJtJaW5dsvAQ8iad3amLtvgQ8vVHlxpj+wJjTNL0YmGeMeQjP99E04IlTtFMCxJ2krc9r23oYzxDe5cBNpzq5MSYFz/BlzWniFPkGJVAiAaDa6WZddiHLduWxbGc+X2YfpcZlCXMYhnVL4kfn92V87xSGdE0kzKHFd5tq3tI9fLb9CM/cNgqA/LJqjpZX1z/+v5mjj+m5q0uewFML6/P/O5fQ2p/7+uxCBqUltPbEdRCeCdRN8QGeYbKPAKy1m4wxccB+a+3B2mPeA+4yxmwBtgFfnKpBa+1aY8xLwHo8k8hXnaoda22+MWapMWYj8K619p7j2poHrKzd9VTtUGP6KUI4F3i7Ua9e5DjGWuvvGFpMZmamXb16tb/DEMHttmw+WMyyXXks3em5Uq682oUxMLBzAuN6pzC+VyqZ6UlEh+v/OY1R97fMGMNrX+bwyMc7ee9H5xAR6uC55Vks3pHHYzcMb1YCujO3lMl/X8yPL+zL987tXb/fGLPGWpvZ7BfhYyer8dSYY4wxw4EfW2tP2avTmtTOk5ptrd3u71ik9dFfZpEWYK1lT14ZS3fls6z2SrnC2gV4e7WL4aoRnvk2Y3omH9P7ISfnclustYQ6Qli8/Qg/mb+eBXeNJT01hpSYCDI6xVNc4aRdnIObxqZz09j0Zp+zV7sY/nDFICYP7Nj8F9DK1PbwLDLGOLxRC8rfaq8sfF3Jk5wpJVAiPtLwSrnlu/I5WOSZOtI5IZILzurAuF4pjOuVqvXkGqmsyonLWuIjw9h8oJgZTyzn79cO5fyzOtA1OZpz+qTiqu2FmtC3HRP6tvN6DMYYrsnsCoDT5ea3Czdz2/h0r5/Hh+bR4Gq6ph5jrZ3r1Wj8yFpbDfzH33FI66UESsRLyqqcLN+Vz+c7jvD5zrxjrpQb2yuF79VeLZeeoivlGuNISRXVLjdpiVEcLasm88GPmD25PzMn9KRnuximD+tMh3hP8tkjNYa/zRjaovHlHK3g7Q0HGdA5vkXP2xzW2nneOEZENAdK5KSqnW4+3nKYCX3bERPxzf9ruN2WTQeKWbzjCIu3H2HtPs/E76gwB6N7JjO+VyrjeqdwVkddKdcY+/LLKa6sYWBaAm63ZejvPmDq4E48dIWnVNETn+1ifO9UBqYl+DnSrxWWV5MYHd5q5kCJiPeoB0rkBFZlFfCT+evILqjg/mkZ3Dq+BwDFlTUs2prLoq25LN6RR0GZ50qujE7xfPvsHkzs044R6UlEhDr8GX6rsD67kAOFFUwZ1AmA77+wlogwB/PvHEtIiOGhKwbTPSW6/vg7J/byV6gnpflqIsFLCZRIAwcKKxj38CcApMZ6qnj/d8U+7l+4mfP6t2fx9iM43ZbkmHAm9EllYr92nN27nSp+N8KX+46yZu9R7jinJwDzlmWxdGcekwd2xBjDvdMyiG+wNt/UwZ38FaqIyGlpCE8Ez1Vyr67dz31vbqK0ysnkAR156IpBDHvgw/pjwhyGW8amM2VQJ4Z2TcShYblT+iqnkFfW5PDrSzIIc4TwyMc7eHTRTtb+5kJiIkLJOVpOdHgoyTGtvxdHQ3giwUc9UBL0DhRW8MvXNvDptiNkdk/ij1cNple7WFzuY/9zsfWBKUqaTmHXkVIeXbSTn17Uj7TEKPYVlLNgTQ43j0unV7tYbh2fzqwJPYkM8wxvdkmKPk2LIiKBSwmUBC232zL1n0vYcrCYqDAH903L4Oax6fVJUsNU6Qfn91HyVMtaizGG7IJyZr/6Fd+b1JtxvVOpqHbx2bYjXD2iK2mJUVyU0ZHJ93Wsr+DdcHhORKS1UwIlQSm7oJxfvraBLQeLAXj/RxPolnJsj0jDSgODAujKr5ZkraXK6SYyzEFplZMrH1vG9aO7ccu4dJJjwimucFJR46mpOKBzPKt/fUF9iYbwUC05IyJtV6tJoIwxPYFfAQnW2qtq900CHsCzKOaL1tpP/RWftA7WWl5alc1vF24G4PeXDeS6Ud1O2LvUsFZTdHhwXFVX7XRztLyaDvGRWGs598+fck6fdjxw2UBiI0LJ6BxPh3jPhPmYiFAWfv/s+ueqtpWIBBO/JlDGmLnAJUCutXZgg/2TgX8ADjwLQj5srd0N3G6MWdCgCQuUApFATstFLq1RUUUN/7dgPe9vOsy4Xin86eohpCVGNeq5UW00gcotqeRgYSVDuiYCcM0Ty4mNCOW/d3gW1712VDfSG/TMtXSxShGRQOXvHqh5wL9oUE7fGOMAHgUuxJMUrTLGvGmt3XyC539urf3MGNMB+Ctwg+9DltZo0bZcbntmFSEGfvWts7j97B5NKm6ZENU25u9kF5Sz6UARkwd6SgT8buFm1u49yrJfnA/AXRN7HrPY7l0BWHtJRCQQ+DWBstYuNsakH7d7FLCztscJY8yLwHTgGwmUtdZde/cocMJCPMaYWcAsgG7dunkncGk1XG7L3z/azj8/2QnACzPHMLpnSpPbaa0J1IHCCj7bfoRrMrviCDG8vDqbfy3ayVf3X0xsRCh3TexFldNVPzG8LrESEZFT83cP1ImkAdkNtnOA0caYFOBBYJgx5hfW2oeMMVcAFwOJeHqyvsFaOweYA546UL4MXALLvvxyJvxpEQBXj+jCA5cNrL+EvqniIgPxV+WbcosreXP9AaYPTaNdXASrsgr4xasbGJSWwMC0BK4b3Y1Lh6YRUzskGUjLooiItCat41sBsNbmA3cdt+9V4FX/RCSB7KucQq54bBnAMUuxnKlAXZqlqKKG+auyGd87lYzO8RwuruL3b2+hS1I0kwd25Lz+7Vn0s0n185g6JTRuzpeIiJxaICZQ+4GuDba71O4TOSW321LjdnPVv5ezYX8RnRIi+d30gVyY0cHfoXlNjcvN3CV76NsxjnP7tccYeOjdLfxqagYZnePJ6BzPF784n44JkQDERYYRp/pLIiJeF4gJ1CqgjzGmB57E6Vrgev+GJIGuxuWmz6/ePWbfuz88p00s9jp3yR6iwh1cN6oboSGGuUv3MHVQZ87t1574yDBW//rC+uVQHCGmPnkSERHf8XcZgxeASUCqMSYHuM9a+7Qx5m7gfTxlDOZaazf5MUwJcGv2FnDlv5cfs2/XH77VaiuHz1+VTfbRcn56UT8APtpymMToMK4b1Q1jDJ/8dBIxEV//6raFteRERFobf1+Fd91J9r8DvNPC4Ugr9NzyLH7zhie/vvvc3pRXu7h3Woafo2qadzYc5MPNh+trLG3YX8Tmg8X8pPbKuHm3jTqmqnfD5ElERPxDf4mlVXK7LY8u2slfPtwOwJybRnDRgI5+jqpxlu3M49+f7WLOTZlEhTs4XFzJloPFlFY5iY0I5beXDjimRpWWRBERCTxKoKTVyS2p5McvrWPpznwuHdKZh64YFNC9MjtzS/nrh9v46UX96NUuFqfbcqSkikPFlfRIjeHWcenc1uAqwaYU+BQREf8I3G8dkRNYl13Ij19aR3ZBOfdPy+CWcekBtwZbZY2LBWtyGJiWwNCuiUSEhrB2byEHCivo1S6WCX3bMaFvu/rjAy1+ERE5PSVQ0mosXH+A77/wJWEOw5M3Z3Ju//b+DqledkE5RRU1DExLwBj447tbuX50N4Z2TaRrcjTLf3GeEiURkTZECZQEvIKyaoY/8GH99rs/PIfe7eP8GJFniZgDhRV0TfYUqJz5n9XER4Yx/66xRIQ6+OinE2kf9/XqQkqeRETaFiVQEtAOFVUy5qGPAUhLjOKTn01s8argZ3WKI7ugnF1HSjmnTzscIYafv/IVi7cfYcUvz8cYw4OXD6Rd7Nf1lzrEqxaTiEhbpst7JGBtOlDE5Y8tJTIshOtGdWXp7PNaNHkqqawBICk6nBdW7uPWZ1ZRVu0E4JrMrvzmkgxcbs/yiiO6J9OtdrkUERFp+9QDJQHH7bb0/KWnDFjH+Ehe+c44BnRumUVvrbUs35XP2F4pvP6lZwWhQ8WVDOuWxPw7xxJVuxjxqB7JLRKPiIgEJvVASUAprqzh2jlf1G+/8t2WS54APt6Sy/VPreCDzYfpnhLDlIEd6Z4cTUJUGKN6JBPm0K+MiIgogZIAYa3l3Q0HueSRJazZd5SR6Umsv+8i0hKjWuT8pVWeobnz+rfnr9cM4fz+7RnePYl3Nx6qT55ERETqaAhP/K7G5WbqI5+z/XApSdFhzL9zDCO6t1zC8sf3tvLmugO8/YOzSYwO54rhXQBw17gAOFBU2WKxiIhI66AESvxqx+ESLvzb4vrtV787nh6pMT4/r7WWKqebyDAHUwd1IjTEEBl27AT1Q7WJ08o9BWw+UExG53ifxyUiIq2DEijxm1fX5jD71Q0APHr9cKYO7tRi57792dWUVNbwwswxDExLYGDaN+dZpcZ+XccpLlK/KiIi8jV9K0iLq6xxcd8bm3hpdTajeiTzz+uG+bxuUnm1k7e+OshVw7sQEmK4blQ3sgvKT1ngMjkmvP5+XcFMERERUAIlLWzXkVK+9/xath4q4e5ze/OjC/oQ2gJXtlU73fzfgq/ILa7k7vP6cGFGh0Y/V71PIiJyPH0zSItwutw8+M4WXlqVTURoCPNuG8mkfr5dy+5QUSVvfXWA28/uQUJUGP++YTiDuyY2uZ2SSifFlTXER4Z5P0gREWmVlECJzx0tq2ZY7Vp2A9PiefLmTDol+KY8wc7cEkqrXAztmsiHmw/xx/e2cnafVPp3jGfKoDOfYxXZwsvHiIhIYFMCJT61PruQ7z6/tn77te+O92kxymn/XEqYw/DV/Rdz7ahuTOrX3ivzl8JDVTJNRES+pgRKfMJay2Of7uJP72+jc0Ikr313HMO6Jfn8vA9cNpBth4oBCHOEeCV5GtNTRTRFRORYSqDE6yqqXfzi1a94fd0BAN64+2zaxUWc5lnecdWILl5tL+vhqV5tT0RE2gaNS4hXZReUc/ljS3lj/QF+fEFfdv3hWy2WPH26LZefzl9PWe2yLN7wzNI9fLb9iNfaExGRtkE9UOI1q7IK+Pa8VVgLT96UyQVNKBXgDYeKKvlidz4hp6jt1FS/XbgZUE+UiIgcSwmUNJu1lv+u2MfvFm6ifVwkc28dSb+OcS0aw5vrD9C/UzxLZ5/n1XbP6ZPKef19W25BRERaHyVQ0iyF5dUM/Z2nRMGkfu34x4xhJES3bL2koooa7n9zE5cPS2PoGdR5OpXnbh/t1fZERKRtUAIlZ6ygrJpJf1oEQFpiFE/fMhJHiPeGz06nvNqJI8SQEBXGa98dR/s47y8H8/hnu+jfMc7nRT9FRKR10SRyOSM7Dpdw2aNLqaxx891JvVg6+7wWTZ6KymuY/q+lPLd8LwDdU2KICvd+scuH393KY4t2eb1dERFp3dQDJU323saD3PXftSRFh/HSnWNapL5THWstxhjio0IZ1SOZszrF+/R8H/54QotdRSgiIq1Hq+mBMsb0NMY8bYxZ0GDfZcaYJ40xLxljLvJnfMHik62Hueu/nsrib959dosmTwvW5HDO/1vE/sIKjDE8ePkgxvdO9ek5+3SIIzE63KfnEBGR1sevCZQxZq4xJtcYs/G4/ZONMduMMTuNMbMBrLW7rbW3NzzOWvu6tXYmcBcwo+UiD04vrtzHzP+sYWBaPCt/db5Xqnw3xfBuiUzs246KaleLnldEROR4/u6BmgdMbrjDGOMAHgWmABnAdcaYjNO08+va54gPWGt55OMdzH51A+N7p/LirLE+mbB9IgvW5DD574vJK62iZ7tYHrx8EL3bx7bIuUVERE7Gr3OgrLWLjTHpx+0eBey01u4GMMa8CEwHNh//fGOMAR4G3rXWrj3+8dpjZgGzALp16+a94INAtdONy2056973AJg2pDN/vWaITxcDPl7fDrEM757EoaJKUmM1F0lERAJDIE4iTwOyG2znAKONMSnAg8AwY8wvrLUPAd8HLgASjDG9rbWPH9+YtXYOMAcgMzPT+jz6NmLpzjxueGrFMfv+PmNoi11p9+LKfYzpmcLgLokM7pLYIucUERFprEBMoE7IWpuPZ65Tw32PAI/4J6K261+f7ODPH2w/Zt/6+y5qseSptMrJ79/ewvfO7c13JvVqkXOKiIg0RSAmUPuBrg22u9Tukxbw/qZD9cnT5cPSaB8fwd3n9iYu0vfVxfcXVtA+LoLYiFBenDWGzolRPj+niIjImQjEBGoV0McY0wNP4nQtcL1/QwoO2w6V8OOX1gHw+I0jmDywY4ude29+GVf+ezn/d3E/rhnZlYFpCS12bhERkabydxmDF4DlQD9jTI4x5nZrrRO4G3gf2ALMt9Zu8mecwSArr4yL/76Yaqebz//v3BZLnqz1TEvrnhJDXGQoI9Jbrq6UiIjImTJ1X2DBIDMz065evdrfYQScvNIqxvzhY5xuywszxzC2V0qLnDe7oJzvPL+G/7u4PxP6tmuRc4r4gjFmjbU2099xiEjL8XcdKPGzimoXdzy7Gqfb8tTNmS2WPAG0i4sgISqMGpe7xc4pIiLiDUqggpi1ll+8+hXrcwp5/MYRXJDRwefnrKh28fyKvVQ5XUSGOXj+jjGcf5bvzysiIuJNSqCClLWWv324ndfXHeDHF/RtsTlPWw4V86vXNrJgTU6LnE9ERMQXAvEqPPExay0PvLWFuUv3cE1mF+4+t7dPz1dW5WTN3qNM6NuO4d2SeO9H59C/Y7xPzykiIuJL6oEKMm63ZfYrG5i7dA83j+3Ow1cMJsTHBTLnLcvi1mdWsnRnHoCSJxERafXUAxVEnC43v3h1Ay+vyeGuib34v4v7+Sx5crsteWVVtI+L5M4JPTm7dypDuib65FwiIiItTT1QQaKi2sWs59bw8pocfnBeb34+2XfJE8BPX17P9U+uoMrpItQRouRJRETaFPVABYGCsmqGP/AhAPdeksG3z+7hs3NZazHGcMXwNMb0TCbcoRxdRETaHiVQbdyBwgrG//ETAPp2iPVp8jRv6R6W787nsRtGcE4fFcYUEZG2SwlUG3aoqJIbn16BtfDUzZk+r/N0oKiSj7bkUuNy4whx+PRcIiIi/qSlXNqo/YUVXP/kF+SVVPHMbaMY1SPZJ+c5UFhBldNNj9QYXG7PZ8nh46v6RAKNlnIRCT6aoNIGZReUM+OJ5RSUVfPcHaN9ljwBPPDWZm56egUut8URYpQ8iYhIUNAQXhuTlVfG9U9+QVm1i+fvGM3gLok+Pd+PL+zLgcIKJU4iIhJU1APVhuzMLWXGnOVUOt38b6bvkqclO/KY+Z/VWGvp2yGOSf3a++Q8IiIigUoJVBux/XAJ1875Apfb8sLMMQzonOC1ttdlFzLh/y1iXXYhAIeKK/lw82GOltd47RwiIiKtiYbw2oDNB4q58ekVhIYY/jdzLL3bx3q1/dAQw4DO8XSIjwBg6qBOTB7YkdgIfXxERCQ46RuwlduQU8SNT68gOtzB/2aOoUdqjNfPMTAtgX/fOKJ+OypcJQpERCS4aQivFfty31Guf+oLYiNCmX/nWJ8kT1f+exkr9xR4vV0REZHWTAlUK7Uqq4Cbnl5JUnQ48+8aS9fkaK+f40hJFWv2HmXLwWKvty0iItKaaQivFVq+K5/bn11Fx/hI/jdzDB0TIn1ynnZxESz5+bl0jPdN+yIiIq2VEqhWZsmOPO74zyq6JkXz/MzRtI/zbXLTJcn7PVsiIiKtnYbwWpFF23L59rOrSE+J4cVZY3yaPD23PIv02W9z89yVPjuHiIhIa6UeqFbiw82H+d7za+nbMZbnvj2apJhwn53rrx9s45FPdgJwVqc4n51HRESktVIC1Qq8u+Eg33/hSwakJfCfb48iISrMJ+cpq3LywxfXERcZyoUZHfjN1Ay6pWgIT0RE5Hgawgtwb311gLtf+JIhXRP57+2+S54qql289uV+PtpymPbxETx5c6aSJxERkZNQD1QAe3P9AX780jqGd0tk3m2jiPFR5W9rLRv2F/Hr1zfyvXN7MX1omk/OIyIi0lYogQpQr6zJ4Z4F68lMT+aZW0f6JHmy1nLvG5vomhzFLePS+dW3zuKmsd2JDFOlcRERkVNptUN4xpgMY8x8Y8y/jTFX+Tseb3pp1T5+tmA9Y3ulMO823yRPANUuN0dKqiipdBIR6mDmhJ5KnkRERBohoBIoY8xcY0yuMWbjcfsnG2O2GWN2GmNm1+6eAvzTWvsd4OYWD9ZHnvtiLz9/ZQPn9GnH07eMJDrc+8mT0+WmxuUmItTBv28czo8v6Ov1c4iIiLRlAZVAAfOAyQ13GGMcwKN4EqYM4DpjTAbwHHCtMeZPQEoLx+kTc5fs4Tevb+T8/u2Zc9MIn/UGff+FLxnxwIccLq7EGENIiPHJeURERNqqJnVvGGO6NfLQQmttkxdQs9YuNsakH7d7FLDTWru7NoYXgenW2oeA79UmWK+eIuZZwCyAbt0aG37Lm7N4F394ZysXD+jAP68bTniod3PbsionZdVO2sdFMnlgRwZ1SaB9XIRXzyEiIhIsmjo+9CxggVN1WVg8PUn/OcOYjpcGZDfYzgFG1yZavwRigD+dNBhr5wBzADIzM62XYvKqRxft5E/vb2Pq4E78fcZQwhze7xj8wztbeH/TIT752SRdZSciItJMTUqgrLXn+iqQprLWZlHbs9Raud2WP32wjX9/uovLhnbmz1cPIdQHydMXu/OZPjSNcb1SiY/0TR0pERGRYNIayhjsB7o22O5Su69VK6928pOX1vPepkNcN6orv79sEA4fzEV64K3NfLD5EB/8aCJR4brCTkRExBua1N1hjCkwxlxmjIk3xnxijBnmq8AaWAX0Mcb0MMaEA9cCb7bAeX3mUFEl1zyxnA82H+I3l2Twh8t9kzwBnH9We2LCQwl1aKK4iIiItzR1vCgRCAfCgElAkjeDMca8ACwH+hljcowxt1trncDdwPvAFmC+tXaTN8/bkr7KKeTSfy0hK6+cp27J5Paze2CMd5Mbl9vy5b6jAIzrlcp7P5rgk3lVIiIiwepMhvDsSe43m7X2upPsfwd4x5vn8od3NhzkJ/PXkRITwYLvjKJ/x3ifnOetrw7wk/nree+H59CnQ5xPziEiIhLMziSB+jnwbTzJ04PGmLza/dZaO91rkbUh1loeXbSTP3+wneHdEplzcyapsb4rITCpX3t+cmFfJU8iIiI+ciYJ1PAG98c0uB+QJQL8rbLGxc9f+Yo31h3g8mFpPHTFIJ8VyFy0LZexPVNIiArje+f29sk5REREpOkJVA+fRNFGFZZXc/uzq1mz9yj3XNyP707q5fX5TnW2Hirm9nmruPvc3vzkon4+OYeIiIh4NDWBOmUvU4NK5WdUibwtOVRUyc1zV5CVV85jNwznW4M6+fR8/TvG87+ZYxjWLdGn5xEREZHWUYm81dmTV8ZNT6+gsLyGed8eybheqT4713sbD9IhPpJh3ZIY07NNLAkoIiIS8FptJfJAtelAEbfMXYnbwgszxzCoS4LPzuV0ufnT+9vokRrDU7eM9Nl5RERE5FitoRJ5q7FyTwG3z1tFXGQo/7l9NL3bx/r0fKGOEP57x2iifDQpXURERE5M1RW95OMth7np6RW0i4/g5e+M82ny9OHmw/z5/W1Ya+mUEEVidLjPziUiIiLfpATKC95Yt59Zz62hX8c4Xr5zLGmJUT4939KdeXy+M48qp9un5xEREZET0xBeM72xbj8/fmkdI9OTeeqWTOIiw3x2Lpfb4ggx3HtJBpVOl8/qSYmIiMipqQeqGXYfKeU3r29kWLcknrltpE+Tpw82HeLSfy3hSEkVISGG6HDlviIiIv6iBOoMfbotl+mPLiXUEcIfLh/k84QmOjyUxOgwosPV6yQiIuJv6sZoImstcxbv5uH3ttK/YzxzbhpB1+Ron5/37D6pjO+d4rNK5iIiItJ46oFqgsoaFz9+aR0PvbuVbw3sxCvfGevT5OloWTU3PrWC51fsBVDyJCIiEiDUA9VIR8uquf3ZVXyZXejzde3qxEaGEh3u0LCdiIhIgFEC1Qg5R8u5Ze5Kso9W8O8bhjN5oG/XtSsqryEiLITIMAdzbs706blERESk6ZRAncbWQ8XcMnclFdUu/nv7aEb1SPbp+ZwuNzfPXUFKbARP35KpYTsREZEApATqFL7Ync/M/6wmJjyUl+8aR7+OcT4/Z6gjhFvGpZMUHa7kSUREJEApgTqJT7flMus/a+iWEs2z3x7l8+riJZU1ZBdUkNE5niuGd/HpuURERKR5dBXeCSzflc+dz62hd/vYFlmaBeA3r2/khqe+oKSyxufnEhERkeZRD9Rx1uw9yu3PrqJbcjTP3T6KpJiWWaj3/yb355LBnX1azVxERES8QwlUAzsOl3DrMytpHxfB83eMJiU2wufnXLYrjwGdEuicGEXnFujpEhERkebTEF6t/NIqvv3sKiJCHfz3jtG0j4/0+TmrnC5+8MKX/OaNjT4/l4iIiHiPeqDwLM/yo5fWkVtcxYuzxtAlyfdLswBEhDr4940j6JTg+2RNREREvEc9UMB/V+zj8x153Dstg2Hdknx+vqNl1SzZkQfAyPTkFkvYRERExDuCPoE6XFzJQ+9s4Zw+qVw/qluLnPNPH2xj1nOrKSirbpHziYiIiHe12iE8Y0wI8AAQD6y21j57Ju088vEOalxuHrxsUIsVrvzVt87i0iGdSW6hK/xERETEuwKqB8oYM9cYk2uM2Xjc/snGmG3GmJ3GmNm1u6cDXYAaIOdMzpddUM6Lq7K5flQ3uqX4dhjtUFElv39rM06Xm5iIUMb0TPHp+URERMR3AiqBAuYBkxvuMMY4gEeBKUAGcJ0xJgPoByyz1v4E+M6ZnOz5FfsAuGtSrzOPuJG+2J3P/1buY9vhEp+fS0RERHwroIbwrLWLjTHpx+0eBey01u4GMMa8iKf3KRuom0Tkauq5qp1uFqzJ5rz+7emU4Lv6S7nFlbSPj+SyYWmM65XSIuURRERExLcCrQfqRNLwJEt1cmr3vQpcbIz5J7D4ZE82xswyxqw2xqw+cuRI/f41e4+SV1rNVSN8t+7cnMW7uPBvi8k5Wg6g5ElERKSNCKgeqKaw1pYDtzfiuDnAHIDMzExbt3/pzjwcIYZxvXw3F2nygE4UVdT4tIdLREREWl5r6IHaD3RtsN2ldl+zLN2Vx+AuCT5Ze27lngIAuqVEc8/F/XGEtMzVfSIiItIyWkMCtQroY4zpYYwJB64F3mxOg1VOF1/lFDHWB1fCfbL1MNc8sZz3Nh70etsiIiISGAIqgTLGvAAsB/oZY3KMMbdba53A3cD7wBZgvrV2U3POc6SkCpfb0t0HpQsm9W3PH68cxIUZHb3etoiIiASGgJoDZa297iT73wHe8dZ5DhdXAd6d1P3Cyn1MHtCRpJhwZoxsmYrmIiIi4h8B1QPVUnKLKwFoHxfhlfb25Zdz35ubeHZ5llfaExERkcAWUD1QLeVwbQLVwUs9UN1Sonn9u+Pp1zHOK+2JiIhIYAvOHqiSKkJDDMnRzV+LbvOBYorKa8joHK+r7URERIJEUCZQh4uraB8XQUgzE56iihqm/WsJTy3Z7aXIREREpDUIyiG83JJKr0wgT4gK4y9XD2GsD4txioiISOAJ0h6oSjrEn9kEcpfb8pvXN/LlvqMAXDYszWtzqURERKR1CNIEquqMk56CsmoW7zhSX21cREREgk/QDeFV1rgoqqhpUgLlclve3nCQKQM70i4ugrd/cA6xEUH3oxMREZFaQdcDdaTEU0SzXRNqQC3efoQfvPAlC9cfAFDyJCIiEuSCLhM43MgimtZaDhRVkpYYxbn92/PKd8YyvFtSS4QoIiIiAS7oeqDySmuXcYk79RDevz7ZyeS/L+ZQkSfhGtE9GWNU50lERESCsAcqr7QagNTYExfRtNZijOHSoZ0JCTEnPU5ERESCV9D2QCXFfDMxem55Fve9uQmA7ikxfO/c3oQ6gu5HJCIiIqcRdNlBfmk1idFhhJ0gMco+WsGBwgpcbuuHyERERKS1CKohvCqnm+e+2PuNq+jcbktIiOEXU/rjdFutaSciIiKnFFQ9UEfLPPOfSquc9fsOFFZwyT+XsDe/DGPMCXumRERERBoKqmzhRBfRHSyqoNrlbvlgREREpNUKqiG8sioX8cAD0wfU7xvRPZmPfjLRf0GJiIhIqxNUPVBl1Z6huw83HwagotqFWxPGRUREpImCKoGqc9GAjgD8a9EOJvxpEZU1Lj9HJCIiIq1JUA3h1Xl6yR7ax0UwonsSoSEhRIY5/B2SiIiItCJBmUBldI4nISqM0T1TOK9/B3+HIyIiIq1MUA7h9UyNYdOBYsoalDMQERERaaygTKAA/vzBNl5cle3vMERERKQVCsohvJ9e1I/vTupNVLjmPomIiEjTBWUPlLVWyZOIiIicsaBMoPJrl3QREREROROtNoEyxpxljHncGLPAGPOdpjw3OTrcV2GJiIhIEAioBMoYM9cYk2uM2Xjc/snGmG3GmJ3GmNkA1tot1tq7gGuA8Y09R2xEKCEhJ1gUT0RERKSRAiqBAuYBkxvuMMY4gEeBKUAGcJ0xJqP2sUuBt4F3GnuC+6ZleCtWERERCVIBlUBZaxcDBcftHgXstNbuttZWAy8C02uPf9NaOwW44WRtGmNmGWNWG2NWA7yyNsc3wYuIiEjQaA1lDNKAhgWbcoDRxphJwBVABKfogbLWzgHmAER06mNnTejps0BFREQkOLSGBOqErLWfAp829Xlut9dDERERkSATUEN4J7Ef6Npgu0vtvjOSV1rV7IBEREQkuLWGBGoV0McY08MYEw5cC7x5po29u/GQ1wITERGR4BRQCZQx5gVgOdDPGJNjjLndWusE7gbeB7YA8621m870HF2SorwTrIiIiAStgJoDZa297iT736EJpQpOJVpLuIiIiEgzBVQPVEsYmJbg7xBERESklQu6BOqdDQf9HYKIiIi0ckGXQEWGaQhPREREmifoEqiEqDB/hyAiIiKtXNAlUGf3TvV3CCIiItLKBV0C9dZXmgMlIiIizRN0CVSNS2u5iIiISPMEXQLVPi7C3yGIiIhIKxd0CdR5Z3XwdwgiIiLSygVdAvX2Vwf8HYKIiIi0ckGXQBWU1fg7BBEREWnlgi6B6pwY6e8QREREpJULugTq2+N7+DsEERERaeWCLoFKT43xdwgiIiLSygVdAnWkpMrfIYiIiEgrF3QJ1KGiSn+HICIiIq1c0CVQidFaTFhERESaJ+gSqK7J0f4OQURERFq5oEugcos1hCciIiLNE3wJlCaRi4iISDMFXQKlOVAiIiLSXEGXQHVJ0hwoERERaZ6gS6BUxkBERESaK+gSKBXSFBERkeYKugQqJTbc3yGIiIhIKxd0CVTnxCh/hyAiIiKtXNAlUPsLK/wdgoiIiLRyrTaBMsbEGGOeNcY8aYy5obHPO1pW7cuwREREJAgEVAJljJlrjMk1xmw8bv9kY8w2Y8xOY8zs2t1XAAustTOBSxt7jtTYCC9GLCIiIsEooBIoYB4wueEOY4wDeBSYAmQA1xljMoAuQHbtYa7GnqBjQqRXAhUREZHgFVAJlLV2MVBw3O5RwE5r7W5rbTXwIjAdyMGTRMEpXocxZpYxZrUxZjVAdkG59wMXERGRoBJQCdRJpPF1TxN4Eqc04FXgSmPMv4GFJ3uytXaOtTbTWpsJUFRR48tYRUREJAiE+juAM2WtLQNua+rz2sdrDpSIiIg0T2vogdoPdG2w3aV23xlpH6c5UCIiItI8rSGBWgX0Mcb0MMaEA9cCb55pY3vzy7wWmIiIiASngEqgjDEvAMuBfsaYHGPM7dZaJ3A38D6wBZhvrd10pueIDm+1o5YiIiISIIy11t8xtJiITn1s1cEd/g5DRNoYY8yaugtVRCQ4BFQPlIiIiEhroARKREREpImUQImIiIg0kRIoERERkSZSAiUiIiLSREqgRERERJpICZSIiIhIEymBEhEREWmioEqgBnSO93cIIiIi0gYEVQIVYoy/QxAREZE2IKgSKBERERFvUAIlIiIi0kRKoERERESaSAmUiIiISBMpgRIRERFpIiVQIiIiIk2kBEpERESkiZRAiYiIiDSREigRERGRJjLWWn/H0GKMMSXANn/H4SWpQJ6/g/CitvR62tJrgbb1enz1Wrpba9v5oF0RCVCh/g6ghW2z1mb6OwhvMMasbiuvBdrW62lLrwXa1utpS69FRPxLQ3giIiIiTaQESkRERKSJgi2BmuPvALyoLb0WaFuvpy29Fmhbr6ctvRYR8aOgmkQuIiIi4g3B1gMlIiIi0mxKoERERESaSAmUiIiISBMpgRIRERFpIiVQIiIiIk2kBEpERESkiZRAiYiIiDSREigRERGRJgqqxYRTU1Ntenq6v8MQkTZmzZo1edbadv6OQ0RaTlAlUOnp6axevdrfYYhIG2OM2evvGESkZQXFEJ4xZpoxZk5RUZG/QxEREZE2ICgSKGvtQmvtrISEBH+HIiIiIm1AUCRQIiIiIt6kBEpERESkiYIigdIcKBEREfGmoEigNAdKREREvCkoEigRERERb1ICJSIiItJEQZFA1c2BKijUHCgRERFpvqBIoOrmQJnwKH+H0iImTZpEVlZW/fbLL7/M6NGjGTp0KAMGDOC3v/3tCY9rqKKigokTJ5Kfn89jjz12xrGMGzfOK8ecSHV1NRMmTMDpdJ7R89sib773LpfrjGKIjY0FTv6+3n///fz5z38+ZRuFhYXHfO7O9DMC+pyIiG8ERQIVzJ599ln++Mc/8sorr7Bu3TpWrVpFcnLyaZ83d+5crrjiCkpKSk6aQFlrcbvdp2xn2bJlpz1XY445kfDwcM4//3xeeumlM3p+W9fc997hcDTr/Gf6vsI3E6jmtKXPiYj4QnAlUNbfAbSs4uJifvKTnzB//ny6dOkCQHR0NN///vdP+9znn3+e6dOnM3v2bHbt2sXQoUO55557yMrKol+/ftx8880MHDiQ7OxsAC677DJGjBjBgAEDmDNnTn07sbGxZGVlcdZZZzFz5kwGDBjARRddREVFxTHHAKc87oEHHqBfv36cffbZXHfddfU9GJdddhnPP/+8d35gbYi33vtHH320fn/DnqOTvd8N1b2vAA8++CB9+/bl7LPPZtu2bfX7T9bO8Z+7hm399a9/ZeDAgQwcOJC///3vwKk/O3Xn0edERLwpqBYT9nX+9NuFm9h8oNirbWZ0jue+aQPO6Lmvv/46o0ePpmfPnk16XnV1Nbt37yY9PZ2HH36YjRs3sm7dOsDzRbVjxw6effZZxowZU/+cuXPnkpycTEVFBSNHjuTKK68kJSWl/vEdO3bwwgsv8OSTT3LNNdfwyiuvcOONN37j3Cc6rl+/frzyyiusX7+empoahg8fzogRIwAYOHAgq1atOoOfjvfNeGI5V43owtWZXalxubnxqRVcO6orlw/rQkW1i1ufWcmNY7ozbUhniitrmPnsam4bn87kgZ0oKKvmO/9dw8xzenJBRgdySyppHxd5xrF4472fMWMGP/rRj/je974HwPz583n//feB07/fDa1Zs4YXX3yRdevW4XQ6j3n/TtbO8Z+7f//73/VtPfPMM6xYsQJrLaNHj2bixIkkJSWd8jMWSJ8TEWkbgiqBCjYbN25k6NChTX5eXl4eiYmJJ328e/fuxyRPAI888givvfYaANnZ2ezYseOYL9QePXrUxzJixIiTzr850XF5eXlMnz6dyMhIIiMjmTZtWv3xDoeD8PBwSkpKiIuLa/Jrbau88d4PGzaM3NxcDhw4wJEjR0hKSqJr167A6d/vhj7//HMuv/xyoqOjAbj00kvrH2tKOwBLlizh8ssvJyYmBoArrriCzz//nEsvvfSUnzF9TkTE24IigTLGTAOmdejW26fnOdOeIl+JiYk5ZhijsaKioqisrDxluw19+umnfPTRRyxfvpzo6GgmTZr0jedHRETU33c4HCeNq7HHNVRVVUVk5Jn31njLS3eOrb8f5gg5Zjsq3HHMdnxk2DHbyTHhx2w3p/cJvPfeX3311SxYsIBDhw4xY8YMoHHvd2N4q506p/vsBMrnRETahqCYA1V3FV5UVHD98ZwyZQovv/wyhw8fBjxfIE8++eRpn5eUlITL5aKyspK4uDhKSkpOeXxRURFJSUlER0ezdetWvvjiC6/EX2f8+PEsXLiQyspKSktLeeutt+ofy8/PJzU1lbCwMK+es7XzxnsPMGPGDF588UUWLFjA1VdfDTT9/Z4wYQKvv/46FRUVlJSUsHDhwtO2c7LP3TnnnMPrr79OeXk5ZWVlvPbaa5xzzjmnfV36nIiItwVFAlUnyOaQM2rUKO6//34uvvhiBg8ezNChQ8nNzW3Ucy+66CKWLFlCSkoK48ePZ+DAgdxzzz0nPHby5Mk4nU7OOussZs+e/Y3hveYaOXIkl156KYMHD2bKlCkMGjSIumV5Fi1axNSpU716vrbAG+89wIABAygpKSEtLY1OnToBTX+/hw8fzowZMxgyZAhTpkxh5MiRp23nZJ+74cOHc+uttzJq1ChGjx7NHXfcwbBhw077mvQ5ERGvs9YGza17v4E2GEycONHu2bOnWcetWbPG3njjjd4NrBlKSkqstdaWlZXZESNG2DVr1lhrrb388svttm3b/BlaQGmL7703+PpzAqy2AfA3TjfddGu5W1DMgapjg60LqhmGDx/Oueeei8vlanY9IG+YNWsWmzdvprKykltuuYXhw4dTXV3NZZddRt++ff0dXpsSaO99c+lzIiK+YGwQZRXd+w2ye7dt8HcYPjdv3jwuu+yyU15J15TjpPXQe+8fxpg11tpMf8chIi0nqBKoTr0H2IM7N/k7DBFpY5RAiQSfVjuJ3BhzljHmcWPMAmPMdxrzHJc7eJJFERER8Z2ASqCMMXONMbnGmI3H7Z9sjNlmjNlpjJkNYK3dYq29C7gGGN+Y9iNCA+rlioiISCsVaBnFPGBywx3GGAfwKDAFyACuM8Zk1D52KfA28E7LhikiIiLBLKASKGvtYqDguN2jgJ3W2t3W2mrgRWB67fFvWmunADc0pv1qp9ub4YqIiEiQag1lDNKA7AbbOcBoY8wk4AogglP0QBljZgGzAOI6N21hVREREZETaQ0J1AlZaz8FPm3EcXOMMQeBaeGOkBG+jktERETavoAawjuJ/UDXBttdavc1mq1dC6/hYqMiIiIiZ6o1JFCrgD7GmB7GmHDgWuDNpjRgjJlmjJlTVlHlkwDbmt27d3P77bdz1VVX+TsUaWF670VEGiegEihjzAvAcqCfMSbHGHO7tdYJ3A28D2wB5ltrm1QNs64HyhHaakcsm2TSpElkZWXVb7/88suMHj2aoUOHMmDAAH7729+e8vk9e/bk6aefPmWbDVVUVDBx4kRcLheFhYU89thjZxT3uHHjvHLMyVRXVzNhwgScTucZtxHo/Pnen4nY2Nj6+yd6b++//37+/Oc/n7KNE33mzvRzEgyfERHxjoBKoKy111lrO1lrw6y1Xay1T9fuf8da29da28ta+2BT263rgcIVfH8Un332Wf74xz/yyiuvsG7dOlatWkVycjIAGzZs4JJLLjnmlpub2+RzzJ07lyuuuAKHw3HKBMpai9t98ishly1bdtpzNeaYkwkPD+f888/npZdeOuM2WpOWfu+b60zf2xN95s60rWD7jIhIM/h7NeOWvHXslWGDwcSJE+2ePXtsUVGRTU5Otrt27Tqjdq688spvtHkiY8eOrX9sxowZNjIy0g4ZMsT+7Gc/s3v27LF9+/a1N910k83IyLBZWVl2+vTpdvjw4TYjI8M+8cQT9e3ExMRYa63ds2eP7d+/v73jjjtsRkaGvfDCC215eXmjj/nd735n+/bta8ePH2+vvfZa+6c//an+HOvWrbNTpkw5o59Ha+DP9/7nP/+5/de//lX/2H333Vf/sz/de97w/u9//3vbp0+fE75/J2rn+M/c8e3+5S9/sQMGDLADBgywf/vb36y1p/78nMlnBFhtA+BvnG666dZyN78H0CIvEqYBc+LSeltfu+bxZXb+qn3WWmurnS57zePL7Ktrs6211pZXOe01jy+zb67bb621tqii2l7z+DL77oYD1lpr80ur7DWPL7MfbjpkrbX2cHHFGcVQ94X37LPPnlGykJeXZ++8807bs2dP+4c//OGYNo9XVVVlO3ToUL+9Z88eO2DAgGO2jTF2+fLl9fvy8/OttdaWl5fbAQMG2Ly8PGvtscmRw+GwX375pbXW2quvvto+99xzjTpm5cqVdsiQIbaiosIWFxfb3r17H/MF7HQ6bWpqapN/Jo1xzePLTnt74rOdxxxf91mpe+8b3s6EP9/7tWvX2gkTJtRvn3XWWXbfvtrXd5r3vO7+6tWr7cCBA21ZWZktKiqyvXr1Oub9O1E7x3/mGrZb115paaktKSmxGRkZdu3ataf8jJ3JZ0QJlG66Bd8tKCYFWWsXAguTu/ef6e9YWtLGjRsZOnRok5+XkpLC448/3qhj8/LySExMPOUx3bt3Z8yYMfXbjzzyCK+99hoA2dnZ7Nixg5SUlGOe06NHj/rYR4wYccI5OCc6Ji8vj+nTpxMZGUlkZCTTpk075jkOh4Pw8HBKSkqIi4tr1Gtsjfzx3g8bNozc3FwOHDjAkSNHSEpKomtXzwW0jXnPAT7//HMuv/xyoqOjAbj00kuPefxE7XTs2PGkMS5ZsoTLL7+cmJgYAK644go+//xzLr300pN+xoLlMyIizRMUCVSd8NDmz9M4nZfuHFt/P8wRcsx2VLjjmO34yLBjtpNjwo/Zbh8X2axYYmJiqKioaFYbpxMVFUVlZeVp46jz6aef8tFHH7F8+XKio6OZNGnSCZ/fsOSEw+E44etozDEnUlVVRWRk8362J9LwvWvq8ce/983lr/f+6quvZsGCBRw6dIgZM2YAjX/PT8db7dQ51efHV58REWk7AmoSua/UTSKvrq72dygtasqUKbz88sscPnwY8HwpPPnkk149R1JSEi6Xq/6LLC4ujpKSkpMeX1RURFJSEtHR0WzdupUvvvjCq/GMHz+ehQsXUllZSWlpKW+99dYxj+fn55OamkpYWJhXzxto/PHeA8yYMYMXX3yRBQsWcPXVVwNNe88nTJjA66+/TkVFBSUlJSxcuLD+sZO1c6rP3DnnnMPrr79OeXk5ZWVlvPbaa5xzzjmnfF3B8hkRkeYJigTK1pYxsCG+74EKJKNGjeL+++/n4osvZvDgwQwdOvSMrrQ6nYsuuoglS5YAniGg8ePHM3DgQO65555vHDt58mScTidnnXUWs2fPPmZozxtGjhzJpZdeyuDBg5kyZQqDBg0iISGh/vFFixYxdepUr54zEPnjvQcYMGAAJSUlpKWl0alTJ6Bp7/nw4cOZMWMGQ4YMYcqUKYwcObL+sZO1c6rP3PDhw7n11lsZNWoUo0eP5o477mDYsGGnfE3B8hkRkWby9ySslrwldO1ng8GprpryRZtr1qyxN954o1fP1xwlJSXWWmvLysrsiBEj7Jo1a+ofu/zyy+22bdv8FZrPBft77w1n8hlBk8h10y3obkExB8oYMw2YlpjW29+htEnDhw/n3HPPxeVyeaUeUHPNmjWLzZs3U1lZyS233MLw4cMBT5HEyy67jL59+/o5wrYj0N775tJnREQay1hr/R1Di2nfM8Pm7t7s7zB8bt68eVx22WWnvTrO322K9+m99w9jzBprbaa/4xCRlhNUCVRC1/62KHurv8MQkTZGCZRI8AmKSeQiIiIi3hRUCVRYqPF3CCIiItIGBEUCVVcHqqYm+BYTFhEREe8LigTK1taBcpugeLkiIiLiY8ooRERERJooqBKocEdQvVwRERHxkaDKKIKoYoOIiIj4UFAlUNUut79DEBERkTYgKBKouqvw3G4lUCIiItJ8QZFA1V2FFx7a+tfqEhEREf8LigRKRERExJuCKoGq0RwoERER8YKgSqBEREREvCGoEqjQkKB6uSIiIuIjQZVRWFQISkRERJov1N8BnCljzGXAVCAeeNpa+8HpnlPjUgIlIiIizRdQPVDGmLnGmFxjzMbj9k82xmwzxuw0xswGsNa+bq2dCdwFzGhc+96PWURERIJPQCVQwDxgcsMdxhgH8CgwBcgArjPGZDQ45Ne1j5+WQxmUNNGWg8UUlFX7OwwREQkwAZVAWWsXAwXH7R4F7LTW7rbWVgMvAtONxx+Bd621a0/WpjFmljFmtTFmtcvl8l3w0iZN+cfnXPX4Mn+HISIiAaY1zIFKA7IbbOcAo4HvAxcACcaY3tbax0/0ZGvtHGAOQGSnPpoEJU3y1M2ZdE6M8ncYIiISYFpDAnVC1tpHgEcac6wxZhowLaJjb98GJW3OBRkd/B2CiIgEoIAawjuJ/UDXBttdavc1Wt1aeCGaAyVNdM0Ty/n3p7v8HYaIiASY1pBArQL6GGN6GGPCgWuBN5vSgDFmmjFmjttqBE+aZuWeAv743lZ/hyEiIgHG2ABKKowxLwCTgFTgMHCftfZpY8y3gL8DDmCutfbBM2k/olMfW3lgO0Y9UdJIn20/QqeESPp2iPN3KBLAjDFrrLWZ/o5DRFpOQCVQvlI3Byq8Y++ZpTnbCHO0ho43EWktlECJBJ+gyCTq5kABVDvd/g5HWpFp/1zC3z/a7u8wREQkwARFAlU3BwrAqeVcpAk27C/i7x/t8HcYIiISYIJiCK9ORKc+dt+2DXSIj/R3KNJKLNuZR4eESHq1i/V3KBLANIQnEnxabR2oM1WjITxpgt4dYokOD7pfExEROY2gG8ILnv428YZRD37M2Ic+9ncYIiISYIIigWo4ibzGpR4oaZqSSqe/QxARkQATFAlUQzlHK/wdgoiIiLRyQZFANRzCc4SoiKaIiIg0T1AkUA2H8GIiNCFYREREmicoEqiGapwuf4cgIiIirVzQJVBf7S/ydwgiIiLSygVdApUQGebvEERERKSVC4oEquEk8uTYcH+HIyJtRGF5Nc8s3ePvMETED4IigWo4ibyyWnWgRMQ7EqPDuWlMd3+HISJ+EBQJVENf7Mn3dwgi0oaEOoLuz6iIEIQJVIf4CH+HICJtwJaDxUx95HM26sIUkaAUdAlUu7hIf4cgIm1AebWTqDAHHeL1N0UkGAVdVcnKatWBEpHmG9E9mQXfGefvMETET4KiB6rhVXifbT/i73BEpJWrrHFRpaK8IkEtKBKohlfhdU2O8nc4ItLKLViTw8jff8Shokp/hyIifhIUCVRDHROUQEnTqbdBGjqrUzzXje6mi1JEgljQzYE6Uqz/MUrTWevvCCSQjOiexIjuSf4OQ0T8KOh6oN786qC/Q5BWKDLM4e8QJEBsO1SioTsRCb4EanjXRH+HICKt2F8/3MbVTyzzdxgi4mdBNYQXYgzdUqL9HYa0ErbBuF1ReQ0J0VqIWuBnF/XjkKYCiAS9VptAGWN6Ar8CEqy1VzX2efml1bjcFkeI8V1w0iYc0DCNnECfDnH06RDn7zBExM8CagjPGDPXGJNrjNl43P7JxphtxpidxpjZANba3dba25t6jle/3M+Rkipvhdymvbn+AM+v2OvvMPwmqUGPk3qfBOCTrYf5YrfW0xSRAEuggHnA5IY7jDEO4FFgCpABXGeMyTiTxkMMjExPIjay1Xa8tagfvPAlv3pt4+kPbKPCtEisHOevH27nHx/t8HcYIhIAAiqTsNYuNsakH7d7FLDTWrsbwBjzIjAd2NyYNo0xs4BZANGdepEaG0FsREC97IBSWF7NgcJK2sdHkPXwVH+H41d5pV/3VB4qqqRjgtY8C3bz7xxLXkm1v8MQkQDQGv6LnQZkN9jOAdKMMSnGmMeBYcaYX5zsydbaOdbaTGttZlhoKLkllRSV1/g65lbr1mdW8a1HPifz9x/xi1c3+DscvzrYYA6UpswJQHR4qC5EERGgdSRQJ2StzbfW3mWt7WWtfehUx9athedyuVizt5CPtx5uqTBbDZfb8sLKfZzVKb5+3wsr99H7l+/4MaqmKa92sje/zGvt9UiJqb/fPl69T8Hugbc28/6mQ/4OQ0QCRGtIoPYDXRtsd6nd12h1a+FFhofSMSGCkenJXg2wLXjrqwP84tUNvLByH5kNKiwPSEvwY1Qn95/lWew+UspfP9jGwPvex+W2fPf5tUz806deO0ec5spJrYpqF59szWXH4RJ/hyIiAaI1fEOsAvoYY3rgSZyuBa5vSgPGmGnAtKQuvXGYELomqwu+oa2Hivnhi+vqtw8UVgDw12uGcMXwLi0ez+qsAjolRpGWeOJ1C/fll3PvG5uYPKAjZ3WKp7TKyd3/W0uvdrGM65XitTgaDuHtPlJKz3axXmtbWpeocAef/HQiNS6t6SMiHo3qgTLGdGvkLf70rZ3yPC8Ay4F+xpgcY8zt1loncDfwPrAFmG+t3dSUdut7oCLCKaqoYV9+eXPCbHOSosNJig7j/105mCduGsFr3xvPlIEdOb9/hxMeb62l2uk+4/Otyirgxy+tAyC7oJz02W8zZ/EufvDClyzblcdVjy8nK+/robi80irufG41eaVVZOWV0TkxksuHpfGzi/tx93m92XD/RYQYw8dbDnP72T3POK7jbTpQVH9fV+QFN6fLjTGG8FB9DkTEw9hGrJJqjFkEWOBUU2ktMM9a+x8vxeY1dT1QqV17z4y5/u9MH9qZf1w7zN9h+ZW1li92F7A3v4xrR3U74TGPfLyDj7Yc5s27zz5mf/rstwF48uZMNuwv4icX9gXA7bY8v3If14/qxs7cUhZvP8IVw9N4dvleluw4wsUDOpIaG8FPX14PwO4/fIt1OYVc8dgy+rSPZUduKSt/eT5Ld+UxeUAnosI968/d8NQXLN2ZT78OcRwsqmDp7POIizy2LtP+wgrP1ZUW4qNCMab5s75ziysZ9YePAYL+isRg9uW+o3zv+bXMuTmTgScZ0jbGrLHWZrZwaCLiR40awrPWnuvrQHzJWrsQWNit36CZALeNS/dvQH6w6UARfdrHER4awrR/LmHywI7sPlLGK2tzuGJ4lxP+z7pzYhQDOn+zU/HhKwbxxroDbMgp5Kkleygsr2bJzjx+MzWD37y+kV25pXRPiebBd7ZQWFHNyPRkvtx3lIfe3cqo9GS2/G4y67ILCQkxDO+WRNbDU3G7LSG1l7pdPqwLb6zbT2J0OCO6J/HItcN47ou9XDuyG2v2Hv1G8gSQlhjF45/t4uF3t7Lld5Prk6/mSI4Jb3Yb0vqFh4bQu0Mc6akxpz9YRIJGo3qgWru6HqgO3XrPjLzu76z59QWkxEb4Oyyfc7stFjjvL5+yN7+c2VP6c2FGBy7+22IuGtCB60d1p2/HWNrHNe4KM2vtMT07TpcbR4hh5IMfk1daxfp7L+K2eSt55LphJEZ7ko+GNbestRwtr2lUYnLhXz8jOiKUvfll/HpqBleNOP1crK9yClmddZQbx3T3ylDL7iOlnPeXzwB46/tnn7T3QUQ9UCLBp8kJlDHm59baP/ooHp/qlTHYui59iL/PGMplw9L8HY5PFJZX86vXNtI5MZInP9/DM7eN5LZnVh1zzKbfXkxMI4uJFlXUEBpiiIkI5fUv9/O/lft47IbhpDZIQK21ON3Wq/OEDhdXEmIMf/1wG3dN7EX3lJb/3/8zS/fw24Weeq0f/WQCvdtr/bNgUlHtYt6yLG4dl37aHk0lUCLB57TfeMaY+Q1uLwN3tEBcPhFS23vyo5fWUeM680nQgaKksgZrLVl5ZWTc+x4/nb+ef3y8g7c3HGRgWgLpKdEM6ZLI32YMYc9D32L+nWMBiAxr3PDWztxShvz2Az7Y7Kl94wgxRIY5SI4+tgfJGOP1SdYd4iNpFxfBQ1cMbnTyVFnjIrekEpfbO72qFw3oWH8/3Q8JnPjXyqwC/vjeVjbsLzr9wSISdBrzrVdsrb2m9nY18JGvg/K2ukKaFeWeq+8emD7glLPhA8Xh4kqW7szjcLHncvq5S/bwvxX7AHhy8W4G3f8BxZVOEqPDMMCUgR0ZlZ7M7y8byPShaXx6z7kkx4Rz+bAuGGMY1SOZrIen4mhkWe30lGh6toshOtzTWzVtSGf+8+1R9XOVAs2ra/czqnY40Rs6xH3dy+b0UlImLctaS1mVE3ft+3e4uJJlu/Lqk+z12YU8/tmu+sf/t2Iflz+2FICJfdvx+vfGM6qH6saJyDc1JoF6EMAYk1q7/SvfheMbdWUM4mI9vQjpqTFUB0APVHm1k437i6hyugCYvzqbK/+9jLIqJwAL1uTws5fXU1zhWXrmw82H+XRbLgC7ay/zL66oITE6nE2/m8wFGR2YMqgTN47p7pX4Qh0h/H76QO58bg3/W7GPQJ8vNzI9id9fNtBrax1uPfR10cQvdud7pU05PafLXZ/gFFfWsGbvUUprfyf25ZfzzNI9FJR51qNbu+8oP35pHbklnv9kvL/pEOf/5dP6/3T894u9DLjvffJrj3/7q4Nc/+QKSio9v1Mr9uTz8LtbKa/x/A5GhoWQFB1ef/6hXRNb5kWLSKtz2gTKWrun9u7c2u0Cn0bkQ3U9Jx9uPkzGve+zbGdek9uw1uKsTb6stVTWuOqHA91uy/7CivqEqKiihte+zKn/4/7lvqNc9e9l7MwtBWDR1iNc8s8l7D7iSYZ6psYQERpSn0BNHdSJ3146oH4R2xdmjWHOzZ5pFg9dMYish6f6vChoXaL5y9c28MXuwH7r+3SI48Yx3U86v+uZpXsYfP/7bDtUwtwlexj/8CccLKrgUFElT32+u76A6D8+2sEf39vKS6u+XoIxupHDnsHK6XLjdLmpcrpYsTu/PoE5WlbNU5/vrv/M7y+s4J6X17Oxdlhs++ESbpm7sn572c48ev/qXVZneT5ra/Ye5cp/L2NbbTK77XAJv124mZyjnt7kwvJqVmUVUFrp+Z1JiAqjf8f4+h7m4d2T+MWU/kSEef7UXTywIy/MHFPfq3rz2HTPnMDaOU5XDO/C3FtHNrqXVkSCV1MmrrTavyh1Q3hlpZ4/4n3ax/LAZQMZ3mDJkobcDYZrFm8/wu4jnucVVdSQ+fuPeGqJJ6csqXLS/zfv8fwXewEoKK9m/MOf8OJKzxfvztwSfvzSejYdKAY8xRjDHCH1CVZmehKP3ziczrUVtzPTk/nfzDH1666lp8Zw0YCOJ7xsv6VM7NuOP145iBmZXRnTM7CHMqqdbnKOllNe7fzGYy63ZUT3JIornXSMj6Rnuxj2F1bQIS6Sdzce5MF3thAeGoLT5ebN9fspKK3m9rN71D8/vV3wzYGqcrrqf5YFZdW8s+EgheWenpyth4r55Wsb2F9YQWF5Nb1/9S73vrmJIyVVzJjzBZ9tOwJ4fmd+//YWNuwvBDzz1JbszONIiWeYNTTEcLS8ur6HqXtqDD+5sG/978SQLonMu20kvWurwJ/TJ5W1v7mQAZ09V0Se178DS35+Xn2V+DE9U3j0huH1v0MDOidw58RexNf+DqUlRjG2V0r9VZqRYQ5iIrxTN0xEgoy1tlE34M3GHhuot8FDh9nuP3/L/m/FXltnQ06h3Xao2FprrdvttpP+tMje98bG+scH3Pte/bbb7bb3vbHRrs7Kt9ZaW1njtI8u2mHX7i2w1lpbUllj/7dir91e215JZY3dmVtiK6qdVnxvzd4C2/3nb9lPth4+Zv9NT6+wP5u/zlprbY3T9Y3nVVQ7bWF5df32a2tz7OHiCmuttd1//pbt/vO3bM7Rch9G7jtut7v+/r78MvtVdmH99rsbDtq5S3bXb898dpX99Wsb6rfP/dMie/f/1lprrV28Pdd2//lbduUez2f/k62H7YgHPrBbDhbZgtIq2/3nb9n73thoK6qddsmOI/U/P6fLbYsqqk/4c29LgNU2AP7G6aabbi13a8pkkVb/X7S6q/DKqz09QJU1Lu58bg19OsQy77ZRGGOYNqQz/Tt+fbn683eMJrV2MrExhvsvHVD/WESog+9O6l2/HRsRynUNqnrHRoQSq/XTWkyPlBhmTejJniNl/Om9z5k+tDN3TuzF2J4phDk8733oCa4WjAxzHHNl4olKXCzZcYQZI09csd3XGhYZ3ZdfjtPtpme7WCprXLy0KpsR3ZMY0DmeO55dTV5ZNa99ZxwhIYZHPt7B8yv2snz2+YSEGP71yU4+3Z7Lil9eAHiGsldlFXDbeE9PW492MSQ1uMLyrom96mt2DemayLs/PKf+asRz+7Vn9a8vrD+2YaX28b1T6+87Qkx974+ISFvSlATqFz6LooXUJVAVtcMSkWEO3vr+2WQf/XptvLplSeoM0STSViMpJpwtB4uZs3g3AON7exYW/s6kXs1ue112YaMSqCqni4pqV30h0T15ZeSXVpGZ7hn+XLz9CAeLKurbem55FjmFFfxiylkAPPDWZg4VVfLoDcMBuO2ZlRRV1PDqd8cDcM+C9Vhg/p1jWZ11lPve3MQ9F/djYFoCUwd34sWV2TjdlvAQw6AuCYzKTaHS6SI6PJTbzk4/Jjl86IpB9YklUB9DnWtGdq2/Hx8ZRnwnJUIiInUanUBZazf6MhBfqqtE3qtXbxwhpr4HCjxfuklasqPNOKdPKh3iI/nz1UO82u4LK7P54fl92ZNXxmOf7uQ/3/b0WD66aCevrMnhk59NAuDBt7fw5voDrLv3IgDmLN7Nh5sPs/rXnl6fhesPsHRnXn0CtetIGdsPf321X3JM+DF1rC4Z3JmKmq8/rz+9qB9103XG9Ezm7R+cTa/aXs4rhnfhiuFfV2w/t197zu3Xvn67f8djl+XRwrgiImeuUQmUMWYWMNJaO7PBPgM8Aayx1j7ho/i8wtauhZeZmTmzKsxxTAIlbcusCc3vbTqZXUdKeWfDQUqrnPWTjrunRDO6weT6KQM70afD10PAt5/dg6tGfN3rc9+lA2h4gVfDIWGA753b+5jtK49bwqZhTaJQR0j9ZGoREWlZje2B+ikwv+EOa601xhwEfoYnkWoVosIdVCiBkiZ66IpB9EiN4cHLBx2z/5LBnblkcOf67bG9UhjbK6V+u3f7Y+fAeatGlYiI+Fdj+/C7AVkn2L8P6HqC/QErNjK0/pJpkVOx9uuhtN7tY+svrRcREWlsApUHXHWC/VcBR7wXju/FR4ZRXFuFWORUGuRPOF2BXYVdRERaVmPHE14BfmCM+Yqv18K7ABgAPOKLwHwlPiqMogolUHJ6DVMmby1QLCIibUNjE6hfAUOBCcDABvs/pZWtjRcfGUpOQfnpDxRpwGWVQImIyNcalUBZa8uAScaY84AReP5zvsZau8iXwXlLXRmD3r17qwdKGq3hHCiX2/+LT4uISOBobBmDugqCO2tvx++vU2itLfZSbF7TsIxB3Rwoa63Wv5JTatjnpDlQIiLSUGOH8J5txDEWmAf854yjaQHxUaHUuCyVNW6iwh2nf4II4NYQnoiINNDYIbxzfR1IS0mI8ixHUVxZowRKTqlhzlSjHigREWkg6NZyqFvYtFjzoOQ0bINBPPVAiYhIQ8GXQDXogRJpLF14ICIiDQVfAhXpGbXUF2LbU1njIiuvzGvtNex0uveNTV5rV0REWr9Wm0AZY2KMMc8aY540xtzQ2OfV90BVaDmXtuYn89cx6c+fUlmjtQ5FRMS3GlvG4Cenetxa+1dvBGOMmQtcAuRaawc22D8Z+AfgAJ6y1j4MXAEssNYuNMa8BDzfmHPUzYH60Uvr6J4SzbBuSd4IvcXV1Sg6vhRDZY2LEGMIDTG4rSXUEUJuSSVVNW6SY8Ipr3YRGRZCXGQYbrclJMTz/MPFlRwqqqRTYiSLtuaycX8xDgfMW7qXMT2TefzGEcRFhhFioKzaxUebD3PRgA5U1bh5Y91+rsrsSrgjhB25JfRqF0uNy01EqIM9eWX0bh9LztFy8suq6Z4cTXm1i6KKGqLDHXRPiaGooobIsBC+3FfI6B7JuK1nztGuI6VkdIqnssbNgjXZXDuqG/sKyklLjKLK6SY0xFDjcvPxllxS4yJ4Z8MhAJ78fDeXDU2jXVwExkBFtYvIMAefbjvCRRkdyC2pIsxhCHWEEBkWQlFFDe1iIzDGcLSsGlv7c6zrrQSo/TFRVuUkzBFCeKjn/x5VThcO42mr7ucfGuLZttbWx3my7WqXu/75dduhISE4QgzWWlxuS4gxhNRuO+u2DeSVVuMIMcRFhuIwhr0F5XRLjqbK6SLcEUKoI4SyKichxhAV7sDlttS43IQ7QggJMfXbEaEhWAvVLjdhDs+53W57zHaNy43bWiJCPRdeOF1u3Jb6n8OK3flkpidTc4o23G5LjfvY1+d0WxwNXp/ben7Wxphj6nCdqOSItRZrwdQe73JbnG7P66vbNlD/GXe63BhjcNRuH1/KpMrp+d0Jq30vS6ucRIZ6fo51bYeFHPuziwzThSgiwcrYRkyONcbcV3u3HzASeLN2exqw0lp7o1eCMWYCUAr8py6BMsY4gO3AhUAOsAq4DpgOvGutXWeM+Z+19vrTtR8XF2eHZ45i7+iv88Ew46Z3xwS6JkaydPEiquLScFSXEl6Wi8ES0nUYGd3b0zs5nDdef43S1AzCyvOILD0A1uLsOZ6RvTvQO8HBy/NfpLjzKCKKs4kqycFtHFT2nMiFA9PoGVPDf55/icKu44ku2EFkSQ5uRzjlvc7j0qFd6BRaynMvvkpht3OIydtEZMkBXKERHE0/H4BhHUL58lA1ifsWY42Dom7nYJyVDOkST16Vg6LSCsoqKnA7Ij3fKLW+NzqVR1fknfRn8r1xHQmPjuNvH+047fvjqC7FUV1CdWyn+n3hDkO1N69Qq/tGrHXl8DQ+35FHbkmVV9qNKMqiKiH9Gw+nH11D5YHt5PW8CGdUCgAxVFDGsQsIG1cNkcX7iD+0lqFDh/KJsy8XZXTkq9cfp7q6msP9ryImfwuxRzYxatRoFpZ044rhaSx9/u9Y4yC3/xXEHv6KmIJtjDl7Im/kt2fGiM58+t9/4HJEkNf3UmIPriG6cDfjzr2Y1w/Fc+3wDnzyv8dwhsVQ0GsyxllFyu73yet7KQAOA5G5GylrN5Cw0sPUxHYg7sAqbr5wBI9tCsECKTvfwRUWTWH3SSRlfYLDWcGgSdNYkWvoHGM4vGUlJR2Hk5CznMiSbGoikijoeSG3jUsn/8ghPlyxkcqE7rTf/jrWGEo6DIV2fXji5pF8sHQt/932dY9u3MHVRBfu4aaZ3+PPS3I5P83Nxo9ewRkeR36vySTs/4KI4mxuvPNH/HXxfs7tUMWmzxZSE5nE0fTzScheSkTZIWbM+hH/XJzNpJQyNi97n+qoVAq7TSBx32IcNWWMv+xW3txwmCnJeaxZ9inlSX0o7TCE9lsWYLCMu/5HvPblfkZH5ZK15lMKu4zFGR5P6p4PCI+IIGXCzeSXVdGjbAvrd+2nqMtYYvK2EJu3mZiEJHZ3voBJ/drBtkV8mVNEUZfxJO/+gLCqIsLSMsiJH8Csc3qy4eMFvPi3e9dYazOb9yEVkdakUQlU/cHGLAamWmtLarfjgLettRO8FpAx6cBbDRKoscD91tqLa7d/UXtoDnDUWvuWMeZFa+21J2lvFjALICIiYsToMWPYO+ae+sfDjZvYqEgSo0PZl3MAZ3gsxrox1o3FQGg4oSEhGAPVNTVgHMd8wYuI7P3jJUqgRIJMYwtp1ukAVDfYrq7d50tpQHaD7RxgNJ5FjP9ljJkKLDzZk621c4A5AJmZmfazTz9l4foDPLN0D3dN7MVFAzqeUVBut2f4wW09N0eIqa9W7XRb3G7PRfA1Lnf9ZGRjPB0hFs9QhcEzVGXtsROW67is5aucQvYXVjCuVyqRYSGUVjoJCTGUVjopr3ZSUukkKTqc6AgHm/YXU+1y8/mOI5zVKZ4hXRLp1zGOH774JdsPl5JQu4xNWmIUlTUubhmXTkaneBwOw/xV2by78dApX/NPL+zLpH7tmf3qV2w6UMz0oZ0Z1jWR+xduBuD8/u35eGtu/fFhDvON+kmZ3ZNYvfdo/XZyTDgFZV9/pG4e253U2Aj++uF2AP5w+SDKq538/u0txEeGUlzprP85ju2Zwsb9RZRUnXg+23WjuvLCymymDOxIcWUNS3fm86ML+vBVThGfbM3l++f15ukle7AWHrthOBGhIVQ53dw2bxWhIYYbRnfDEWJ4ZW0ORRVO+neMY2Lfdpzbvz1hjmOT6LqOs+M60L7hZI8fu9/U3zd8PXxl8Ay1Pv7ZLr5/Xh/e3nCQBWtymD60M9MGd+aO/6zme+f2YkKfdligosbFXz7YRmJUOLef3YPY2iHJEANZeeWkp8ZwtKyaT7fn0q9DHGd1ivfEUn9eOFhUyY7DpUzom1ofa108ns+t57N8zRPLSY4J55/XDSMyLKT+8YYavr6G26b+cXPcdv1P47jnf12GpO41udye4c7qBkOSdQrKqkmMDscRAiHGeIYNQzzD3CHGUFheQ87Rcvp0iK1/H0KMYfeRMmIjQ0mJCf/G+a2FvNIqKmvcdE2OIvOPiEiQaWoP1K+Aa4DX8Pydmw7Mt9b+wWsBfbMH6ipgsrX2jtrtm4DR1tq7m9Bm3Vp4Mzdv3UZ5lYvYyND6uRByckUVNfXFR4NRQVk1wx/4kE4JkSz/xfn+DkcClDFGPVAiQaZJV+FZax8EbgOOAvnAbd5Mnk5iP9C1wXaX2n1nZOvBEob87gMWNegpkZML5uTJWsvh4koArhie5udoREQkkDQpgTLGRAD9gRggEZhmjLnXB3E1tAroY4zpYYwJB67l60nsjWKtXWitnZWQkECH+Ah+c0kG/TrG+SRYaTuqXW6m/ONz7rm4H/dc3N/f4YiISABp6hyoN4AiYA3QzMuivskY8wIwCUg1xuQA91lrnzbG3A28j6eMwVxr7RlXNWwfH8ntZ/fwSrzStoWGhHD/tAyGd2+dpS5ERMR3mjoHamPD+kytRcM5UGs3bKba6SYpOry+PozIyewvrODe1zfynUm9yExP9nc4EqA0B0ok+DS1EvkyY8wgn0TiQw2H8F5YuY8Rv/+IclWrltOw1pJTUM6uI6VU1rj9HY6IiASQpiZQZwNrjDHbjDFfGWM2GGO+8kVg3mSMmWaMmVNUVMT43qn89tIBxISrgrCcmsttmTHnC64c3oWz+6Se/gkiIhI0mjqE1/1E+621e70WkQ9lZmba1atX+zsMaSXcbsuLq7IZ3CWBgWkJ/g5HApiG8ESCT1PLGOwFivEUz+ze4NZqHC6uJK/U6/PfpQ0KCTGMTE/ij+9tZUNOkb/DERGRANKkq/CMMXcAP8RTi2kdMAZYDpzn9ci8qMEkcma/8hV5pdUs/P7Z/g5LApy1lqz8cg4UVuB0aw6UiIh8ralzoH6IZzHhvdbac4FhQKG3g/K2hpPIbz+7Jz88v4+/Q5JWYuZ/VjNtSGeGdVMpAxER+VpT60BVWmsrjTEYYyKstVuNMf18EpmPaDKwNMX/u2owAzrH+zsMEREJME3tgcoxxiQCrwMfGmPeAAJ+AnnDq/B25pZqDpQ0ijGGPu1j+d3CTSzamkulSl+IiEitJl2Fd8wTjZkIJADvWWurvRqVj2RmZtqaaX/gsqGd+e30VlcPVPxgwZocfvbyegAevX44B4sq2JNXxm8uySAyzMHqrAK2HCwmOjyUmAjHsf+GhxId4SAmPJTIsBCMUeHWtkpX4YkEn6YO4dWz1n7mzUBayh+vHERaYrS/wxAf+XzHEcqrXVw8oKNX2qtLngB6pMYQHe7gX4t28n8X9ycyzMH7mw7x5Od7TtvO+nsvIiE6jEcX7WTh+gO896MJADz1+W7W5xQRHeYgOsJBdLgn+YoM89yPiQglLjKUc/u1ByCvtAoDpMRGeOX1iYjImTnjBKq1mjyw0zHb1lqqXW4iQlVYsy3458c7WZlVAMA1mV2YNqQz5/Rpd8bt/d/F/fh/728DIKNzPBmd41l370X1j//0on7MmtCL8monZVUuyqqdlFe7qKjdLq92UlbtIibC8/lKS4xiSJfE+ufnllSxaX8R5dVfP9flPrZXOC4ylA33XwzAfW9sYuuhYj7+6SQ+3ZbLrc+sokdqDAbYnVfG1EGd6JQQSUxEKAlRYdS43Fw5ogupsRGUVNbgdkN8VKh6w0REmikoEqi6Mga9evVm4/4iuiRFkRgdjtPl5ponlnO0vIZPfjoRYwxvf3WQuMhQuqdEkxobQZgjhDCH0RdOK1BUUUN6ajSZ6Uk8v2If81fnMH91Duf3b8+t49MZ1ysVRxPXP4yJ9PyK/P6yEw/5RoY5iAxzAI3rEbpsWBqXDUur3/7lt87il98665hjalxuyuuTMSdVzq9LKNwwuhuFFTUAjOmZwl0Te5FztJyiihp255Xx9oaDRIU5qKidrxVi4MoRXQD496e7eGLxbnY+OAWAP7+/jXc2HORQcSVXDu9CUkw4SdFhJEaHERUWSlS4g/jIUAZ3SaTux6bfAxERjzOeA9UaDR02whZe/DsevHwgN4z21P9cvN0z5DN5YEestQz93YcU1X5B1YmNCKVfxzj6d4yjf6d4RqYn0b+j58qspTvzSEuMIj01hmqnm+2HS8gvq8ZhDKN7JhPmaOo8fTlT67ILuezRpTx9Sybnn9WBffnlTPjTomOO+fz/zqVr8omHcCtrXOzNLyfnaDkjeySzPruQxduP8MmWXObdPpKuSTEt8TK8wuW2FFXUsP9oBQM6xxMSYliz9yibDxZz0xjPZ//Jxbv5eOthvthdQGJ0GEUVNRz/5+Ds3qn8/dqhHCqq5LJHl/LvG0dwYUYH1uwt4JGPd1LjctMxIZJX1+4nNTaC1747jvioMGIjQpucrLZmmgMlEnwalUAZY7o1sr1Ca21x80LynREjMu1D/3mLXu1iWZ9TyPShacf8kbfWkldaza4jpezLL6ewopoal+VQUSXbDpWw9VAxxZVOrh7RhT9dPQSAXr98h+9O6sVPL+rH4eJKRv/h42POGR3uIDU2gq7JUbSPiyQ1NpxLBndmSNdEKmtcHCmponNiVEB/2ThdbrKPVtAjNbATiM+2H+GWuStZcNdYMtOT6/dX1rh46vPd/PmD7QBs+u3FFFbUcONTK/jkpxN5cVU2jy7ayYHCCtwn+HW4+9ze/OziVlWt44y43JbiihoKK2oor3ZSUe2isLyGUT2Tqax28cyyLK4cnkbv9nGsyirggbc2c7i4ksPF37yqdeY5PfjV1AxKKmuY+sgSfnh+H64c0YX9hRXc98YmosMdRIV5hjUdDkO72AhSY8NJjA4nJsJB/47xdE6MosblpqiihoSosID+z4gSKJHg09ghvGcbcYwF5gH/OeNofMwYuGhAR6Y/upT12YWEOkK4dEjnBo8b2sVF0C4ugjE9U77xfGstB4sqqW4wpPLirDF0SogEICUmnCduGkFqbDiHiqrYsL+IGpebIyVV7C0oZ29+AXmlVQzonMCQrols2F/E1Y8vZ95tI5nUrz07c0t5eXU27eIiGNwlkS5JUcRFhhIXGeb7H85JfLDpELOeWwPAwrvPZlCXwF0Trq7nMCHq2J9XZJiD753bmxdXZZNz1HMV3cKvDrAnr4xdR0oZ3i2JEd2TmD60M307xPHPT3YSFeYgr7SKg0WVzF26hytHdAn4BLK5HCHGM4wXE/6Nx+Ijw/j55P712yPTk3nz7q+r+S9cf4DfvbWZ75/Xm2qnu37tQAsM75ZIapxniLOyxkXO0XIqa1xU1Liw1pO4FZRXH9P7VddLvPVgCdP+tYQ5N43gogEd+XzHEX6+4CtiI0OJiQglNsJztWNsZO39CM/E+6mDOtE9JYa80iq2HSphSNdEYiNCqaxxUeNyExMeSkgA/6dFRAJfUA3hDRsxwj78n7e487m1hDtC2F47F6QlWWux1rPOWm5JJZ9syeWCjA6kxkbwxGe7+MuH249J0Op0iI+gY3wkidHh/G76AL7KKWL+6mzm3JRJVLjvJsCnz34b8Hy5Lpt9Hh3iI312ruaavyqb/3vlq1MO0zXFy6uzuWfBVwD1X+DiG1VOF0UVNRSV11Ba5SQtydNje6Skinc3HuTcfu3pmhzNxv1FPLM0i7IqJ2XVTkqrnJRWOimr8twvq52E/9ztozinTzve2XCQ7z6/lvd+dA79O8bz7LIs7ntzEwAxtVc5xkaEkhgdRo/UWJJjwogKD+WWsd2bdKWjeqBEgk+jeqCMMbOAkdbamQ32GeAJYLW1do6P4vOqimoXdz63FoDnZ472SwyeKu6e++3jIrl21Nejo3dO7MWdE3uRW1LJxv1FHC729GK9/uV+RvdIoaiihrG9UuieEsPzK/bx+Y48dh0ppVe7WM669z0AZk/pz9RBnbySQDRMrnf94VvNbs/XnLXjb94a6vnWoE7cs+Ar2sdFMKHvmV/JJ6cXEeqgfZyD9nHHJujt4iK4eWx6/fbAtAT+cs2Qk7ZjraXK6a4fEh/bM4WXZo2he7Kn93BE9yR++a3+lFa5PElYbeJ1pKSKZbvyKKqooaLGxdUjupAClFU5iYkIimttRKSJGvuX4afA/IY7rLXWGHMQuAcI6ASq7iq8Hr16c8XwNF5du59BaYE7FNU+LpLz+nu+SK4D/nD5oG8cc/vZPejdPpYBnePZfri0fv/D727l4Xe3AjCsWyJP3zKS6HAHg+5/nxqXJ8FYNvs8OidGnTaOrYdKAOqThy/3HeWjLYe55+L+p3qaV5RU1nDfm5u4dEhnJtXWQDodV+2Cv96aT+auTSB7toupvdJOAp0x5pj3KikmnNENhuMHpiXUDy+ejNttMQa2HCzmlrkreWHWGHq1i/VZzCLSOjX2v+rdgKwT7N8HdPVaND5St5hwcmIC8ZFhxEWGtvovxA7xkVyT2RVjDP06xrHzwSn849qh9G7/9R/6L/cV8tHmw/T/zXv1yRPAuIc/oai85kTNHmPZrnyA+sWXV2cd5dFFu1ifXejdF3MCf/lgO6+u3c+tz6yisLxxhe7reqBCvZRAPfeFZ5WiL3YXsCevzCttSuALCfGULUmNjaBvh7hv1OUSEYHGJ1B5wFUn2H8VcMR74fhWjcvN1kMltGuDVZxDHSFMH5rGRz+ZSNbDU8l6eCqf3TOJCzI61B8zvFti/f373tyI+zRfDHWJy+DaiePXjupKTLiDecuyjjluzuJdXPS3zygoq+b7L3zJA29tZm9+8xKOJTvzAE8doz/VFrI8nbovOofDOwlUw4KXBWVaPzHYtIuL4L93jKZvhzh/hyIiAaixQ3ivAD8wxnwFfFS77wJgAPCILwLzhcLyGr7YnU9m9yR/h9Iiuqd45n1kPTyVV9bkcF7/9iTFhJNx73vkHK1g7tI9/O3D7Xx1/8UnHPYqq3IRFxFaP6coLjKMqzO7Mm9ZFq99uR+A60Z1o7iihu2HS9mwv4iF6w8A8PSSY5c3uXZkV0oqnUSGOfjLNUPILa7EEWJIjgnHGENljYs31x3gqhFdCAkxXJjRgdvGp7Mrt4y5S/fQv2McN4zuTlZ+GT1PMpzi7R6oAZ3j6++HqIBk0MorreKZpXv48QV9CQ3gUgoi0rIam0D9ChgKTAAalmT+tPaxViExOoyohMiAvpLMV+qqUQOsv+8i9uaX8em2I0zs164+edqQU0RqXDidEjzzo1xu9zd6c354fp9jeqB2HSll6qBOPHrDcCqqXbSPi+Bf1w/nmieWH/O8F1dle+IY3oXxD3/C/sKKE8Y5rncK+aXV9ZfMl1U5mbt0D795YxOvrzvAriOlfPLTSSQfd6m92225ZHAnBnZO8NqyPHXVvMF786qk9Vm71zN03bdDHNOHpp3+CSISFBqVQFlry4BJxpjzgBF4yrussdYuOvUzA0uYI4RffussUmO/WecmmIQ5QujdPo7e7T1DEzUuN997fi0fbD4MeApNzluWxT2T+3PP5GMnjCfFhLP1gcks2prLlEHHrisYFe5g5a8uADy9XnXcbktJpRMTAhGhIbyyZj+/fG0D3xrUkX0F5Wzc/3Xt1XBHCJc9tpSHrxjEjJHdiIkI5ScX9uWvH27nN5dksO1QMUnRYazPLiQyzEFCVBhuaxn38CdMHtCRx24Y7rX6Pkt25NXfVw9U8LrgrA50T4nmpVXZSqBEpF5jyxjUXWu/s/Z2/P46AV2JvKLGRVpiJCO6J5/+4CCyKqugPnkCOOf/LaKgrJoeqTF867gkCTyFKY9Pnk4lJMSQEP11ccvrR3fj+tHHfnTySqtwuiyJ0eHce0kGIxtUEv/B+X34Qe1E9qFdE9mTV8b0R5d+4zwfbTnMm+sPHLPWXHOEh349XKMeqOAVEmK4dEhnHl20kyMlVbSLa3tzKEWk6Ro7oP9sI27zgMu8HqEX5RZX8YMX13G0rHFXdQWLcb1Sj9kuqP35fPf5tfzfgvUtEkNqbAQdEyIJDw3htvE9TjrPCSA95Zs1rsIdIZzdJ5UfvbTOazHFN6horgQquF0yuDNuC+9tPOjvUEQkQDR2CO9cXwfSVMaYnnjmXyVYa090heA3pMSGs/9oBR9uOcw1mQFffaFFbfrtxfzhnS08v2LfMfvnr87h/1118sKF/mCM+cYQYUiI4WBRBVsPlnjtPHENCihqCC+49e0QS892Mby36RA3NSjsKSLByy+XlBhj5hpjco0xG4/bP9kYs80Ys9MYM/tUbVhrd1trb2/KeaPDHcy7bSTn9Ek9/cFBJiYilAdPULDzu5N6+SGapqmb89QpIYpz+zeu6GZjNOx1Ug9UcDPGcPGAjnyxu6BRNdREpO3z1zW584DJDXcYYxzAo8AUIAO4zhiTYYwZZIx567jbGX1Llle7SI2NqL/KTL7pgx9P4Pz+7RnT0zMHqbgyeL8sjrkKTz1QQe/iAR1xuS0fbz18+oNFpM3zSwJlrV0MFBy3exSws7ZnqRp4EZhurd1grb3kuFtuY89ljJlljFltjFm9N6+UB97aTFmV04uvpm3p2yGOp28dyYuzxgLw3y/2neYZbVdS9NdXa3Y7wbwrCS6D0xLoGB/J+5sO+TsUEQkAgVQVLg3IbrCdU7vvhIwxKcaYx4FhxphfnOw4a+0ca22mtTazXVwkK/YUUFgRvL0qTXHdqK5MHdz4q+3amlgtIisNhIQYrs7soh5sEQEaX0gz4Fhr84G7GnNs3WLC7bv1JgqIj2y1L7tFPXTFYH+HEDCKK2uIjww7/YHSpv30on7+DkFEAkQg9UDt59iFibvU7vOaKpdnbTX1LEhjtI//ut6Py6UFZcXDWltf6kNEglcgJVCrgD7GmB7GmHDgWuBNbzRsrV1orZ1V4YKIUAdGE4KlERouCZMUE9zV6+Vrd7/wJdc/+YW/wxARP/NLV4wx5gVgEpBqjMkB7rPWPm2MuRt4H3AAc621m7x0vmnAtLi03nRODL518ETEe6YN7kxplRNrrf4zJhLE/JJAWWuvO8n+d4B3fHC+hcDC+K79ZnZPifF28xIEKmtcRIZ5Z5Fiad0mD+zo7xBEJAAE0hCez1U73aRoKEZEmulwcSVvrj/g7zBExI+CIoEyxkwzxsyxwMGiCn+HI62QlnKRhhasyeEHL3xJztFyf4ciIn4SFAlU3SRygCFdE/0cjbRGWspFGrp0SGcA3v5KiwuLBKugSKDqeqDaR8F1o7r5OxxphZQ/SUNdk6Pp1yGOJTvz/B2KiPhJUCRQdT1Q4RFROFXPR86ArraS443vncqKPQVaGkokSAVFAlUnp7CCDzZrHSsRab6LBnSg2unm021H/B2KiPhBUCVQoCrkIuIdI9OTSYkJ1+LCIkEqKBKoujlQnaJhWu3kTxGR5nCEGC7M6MDHWw77OxQR8YOgSKDq5kCZ8CiKKzVfQUS84/JhaQzvnuTvMETED4IigapzoLCCL3bn+zsMaUXG907h0iGd/B2GBKjRPVN47vbR/g5DRPwgqBIogNRYVSKXxlu6M58316vWj4iIHCsoEqi6OVAAcZFh/g5HWpkwh0oYiIjIsYIigWpYibxcNVukiQZ0TvB3CCIiEmCC7pr+Ek0ilyb4+4yhdIiP9HcYIiISYIIugerXMc7fIUgr8qOX1gGQ9fBU/wYiIiIBJSiG8BqKUSFNERERaaagS6CqnC5/hyAiIiKtXFAkUA2vwqtxajFhERERaZ6gSKAaXoXXNSnK3+GIiIhIKxcUCVSdvh3iCA9z+DsMERERaeWCKoEqqqihxuX2dxgiIiLSygVVAnW4uBK31RwoERERaZ6gSqBCQwzhjqB6ySIiIuIDQZVN9OsYjzFa10xERESaJ6gSqPzSKn+HIK3Q8G6J/g5BREQCjLGtdE6QMeYyYCoQDzxtrf3gdM+J6tzXVhzY7uvQRCTIGGPWWGsz/R2HiLQcv/RAGWPmGmNyjTEbj9s/2RizzRiz0xgz+1RtWGtft9bOBO4CZjTmvBGhQdXhJl5QXu1U9XoREfkGf2UU84DJDXcYYxzAo8AUIAO4zhiTYYwZZIx567hb+wZP/XXt804rRPOfpIky7n2fsQ994u8wREQkwPhlZV1r7WJjTPpxu0cBO621uwGMMS8C0621DwGXHN+G8cwGfxh411q79mTnMsbMAmYBRHXs5Z0XIEHjX9cPo1+HOH+HISIiAcYvCdRJpAHZDbZzgNGnOP77wAVAgjGmt7X28RMdZK2dY4w5CExzY0Z4LVoJCpcM7uzvEEREJAAFUgLVJNbaR4BHGnnsQmBhavpZM30blYiIiASDQJpVvR/o2mC7S+2+ZjPGTDPGzHE6nd5oTkRERIJcICVQq4A+xpgexphw4FrgTW80bK1daK2d5TaB9HJFRESktfJXGYMXgOVAP2NMjjHmdmutE7gbeB/YAsy31m7y0vmmGWPm1Di1kLCIiIg0X6stpHkm2vfMsLm7N/s7DBFpY1RIUyT4BNWYVtekaH+HICIiIm1Aq70KrymMMdOAae279fZ3KCIiItIGBEUPVN0k8kq3w9+hiIiISBsQFAlU3STyULSmmYiIiDRfUCRQdT1QPTok+DsUERERaQOCIoGqc6S0yt8hiIiISBsQVAlUUXmNv0MQERGRNiAoEqi6OVAOqzlQIiIi0nxBkUDVzYEKjwj3dygiIiLSBgRFAlWnskY9UCIiItJ8QZVAudzBs2yNiIiI+E5QJFB1c6DCjRYTFhERkeYLigSqbg5URESEv0MRERGRNiAoEqg6FdWaAyUiIiLNF1QJlNtqDpSIiIg0X1AlUPFRYf4OQURERNqAoEqg2sdpDpSIiIg0X6i/A2gJxphpwLQO3Xr7OxQRERFpA4KiB6ruKjy3Q0N4IiIi0nxBkUDViY0Iig43ERER8bGgSqCSY7QWnoiIiDRfUCVQ+WXV/g5BRERE2oCgSqDKq5z+DkFERETagKBKoKLCHf4OQURERNqAVptAGWPOMsY8boxZYIz5jr/jERERkeDhlwTKGDPXGJNrjNl43P7JxphtxpidxpjZp2rDWrvFWnsXcA0wvjHnLddaeCIiIuIF/uqBmgdMbrjDGOMAHgWmABnAdcaYDGPMIGPMW8fd2tc+51LgbeCdlg1fREREgplfCiNZaxcbY9KP2z0K2Gmt3Q1gjHkRmG6tfQi45CTtvAm8aYx5G/jf6c6rOlAiIiLiDYGUUaQB2Q22c4DRJzvYGDMJuAKI4BQ9UMaYWcAsgHZdenghTBEREQl2gZRANYm19lPg00YcN8cYcxCY5sQxwtdxiYiISNsXSFfh7Qe6NtjuUruv2erWwosIVyVyERERab5ASqBWAX2MMT2MMeHAtcCb3mjYGDPNGDMnxK1K5CIiItJ8/ipj8AKwHOhnjMkxxtxurXUCdwPvA1uA+dbaTd44X10PVKeUBG80JyIiIkHOX1fhXXeS/e/gg5IExphpwLRO3Xt7u2kREREJQoE0hOczdT1QMTHR/g5FRERE2oCgSKDq5kBRXe7vUERERKQNCIoEqq4HKiFBc6BERESk+YIigapTVFHj7xBERESkDQiKBKpuCK+iXEN4IiIi0nxBkUDVDeF1VBkDERER8YKgSKBEREREvCmoEqhizYESERERLwiKBKp+DlSF5kCJiIhI8wVFAlU3B6pDsuZAiYiIyP9v787joyqvBo7/zkz2hJCwhzUoKKuA4oa4gLu4tu5dtGp9W7Vq7SK2drO2xWpt3VqlarV13/cFRERR9n3fCTuBACEh+8x5/7h3hklISEImzHa+fmJm7tz7zLmZIXPyPM89T8slRAJljDHGGBNOlkAZY4wxxjRTQiRQgTlQxcXFkQ7FGGOMMXEgIRIoW8rFGGOMMeGUEAmUMcYYY0w4WQJljDHGGNNMlkAZY4wxxjSTJVDGGGOMMc2UEAmUXYVnjDHGmHBKiATKrsIzxhhjTDglRAJljDHGGBNOoqqRjuGwEZESYEWk4wiTDsDOSAcRRvF0PvF0LhBf59Na59JLVTu2QrvGmCiVFOkADrMVqjo80kGEg4jMjpdzgfg6n3g6F4iv84mnczHGRJYN4RljjDHGNJMlUMYYY4wxzZRoCdT4SAcQRvF0LhBf5xNP5wLxdT7xdC7GmAhKqEnkxhhjjDHhkGg9UMYYY4wxLWYJlDHGGGNMM1kCZYwxxhjTTJZAGWOMMcY0kyVQxhhjjDHNZAmUMcYYY0wzWQJljDHGGNNMCbUWnsfj0fT09EiHYYyJM2VlZaqq9gepMQkkoRKo9PR09u3bF+kwjDFxRkTKIx2DMebwsr+YjDHGGGOayRIoY4wxxphmsgTKGGOMMaaZLIEyxhhjjGkmS6CMMcYYY5rJEihjjDHGmGayBMoYY4wxppksgTLGmFYmIs+KSKGILA7Z1k5EJorIKvd7biRjNMY0jyVQxhjTDD6/UlnjC35V+/xNOew54Lw628YCk1S1LzDJvW+MiREJVYm8pk1X7n5jAQ9cPiTSoUSlnaWVzF6/i+lrd5GR4uWfX6zhppG9uffCAZEOLSLKq3z0/+0nACz+w7lkpSbUPxcTorCkgpuen83ybSVU1dROmK44rnujx6vqlyKSX2fzJcAZ7u3ngS+Au1saqzHm8Ei4T4RXZ29i7Pn9yc1MiXQoUWPznnIuemwqu/ZVHfDY01PXRWUCVVXjJyXJ6UDdvKectunJYU9wFm8pDt7eV1ljCVQC65iVypEdszjpiPZkpiSR5JXgY/3z2vAQJInI7JBDxqvq+Eaa7ayqW93b24DOYQ47rETkUuBtoL+qLm/GcenAJ8C3gKtU9Z+H+PzfqOqIlu7TwHEpwGfAaFWtOZT4EkXI6zkaaANceyivaWu+nu6xrf6aJtYngs9JEBZtLua0ozpGOJjosGzrXm7+3+xg8vSv7xzL5BWFvDZ7E12y00hLjswor6oisv9DqqLah0eE579Zz+rCUqavK2LKL0YBMPbNhZRV+fjuST35YMFWnrn++LDEkOzdf+6ds9PC0qaJHarKr95ezI0je9OnUxZ/v2rowXavUdXhLXguFRE91OMPk2uAj9zvv2vGcTcAb+F82N4CHPBhK84/dlHVBsdDm/JBeqgftqpaJSKTgKuAFw+ljXglIl5V9YVsugF4S1V9IpLDIb6mrfl6use2+muaWAmUvxpIzARqb0U1bVKTEBEe/HQ5T0xeQ6/2GRQUldEmLYnnfnA8p/TpQLLXw+j+nfjZOUfz0KcrmLp65wFt+f3Ksm17GZCXXSvJqava52dbcQXdc9Mpqaxh/c59zCnYzfgv15KS5OGK47pTWFLJlJU72Ly7nEuGduPP3xrE6X/9gttG9+G7J/ViR0klf/1kOa/P2RRst216Mv26tMHnV7we4cenH8nOfVWsKSxl0vLCA5KvQ+XzR/vnmWlNm3aXM3HpdvrntaFPp6zWeIrtIpKnqltFJA8orLtD/tgP/wEMDfPzzl8/bsydzTlARLKAEcBI4FOal0B9B7gWGAccKSLzgYnAE25bM4DjgAtE5BGgB5AGPBLaiycipcAg4GNgqhvPZuASVS0P7KOqWe5wab37ichvgO8CO4CNwBxVfQh4B/gLEU6g8sd++EU9m19bP27MP/PHfpiBk8TW9dz6cWOeyx/7YQfgjdAH1o8bc0ZzYxCR14FdwBDgA+D+kIcDrye04DVt4mvVktcTWvk1TawEyuck0Ys2FTeyY/woLqvm7L9PobCkkhtH9uae8/vxxOQ1AHhFuPu8flx7Qk/aZiQHj0lN8tI520taspfKmgP/eLjg0a9Yvq2k1rbhvXIpqahhxfYS3rvtFI7pnsMv31jIgk17mHDnaUxatp2fvrqg1jEPTVhJSpInOKekfVYKPr8yok97erXP4P4PlvLijA1UhUzSfeiKIVxeZ87JiD4dAHhs0ioAavxKsrflCVRJRXXw9uLNxQzq1rbFbZrY0aNdBl/+8gxSvK3WC/secB3Oh9B1wLut9URhcAkwQVULRGSniBynqnMaO8gdRjlCVdeLyFhgkKoOdR/LB/oC16nqdHfbDaq6yx0mmiUib6pqUZ1m+wLXqOoPReQ14NvAC/U8/QH7icgKd/8hQDIwFwicx2IgPN3XsW8w8JqqnhS6MfT1dDcdzte0ua8ntPJrmlgJVHUZx3Rvy4ZdZZGOpNV9s2Yn/5i4ipnrdwW35aQns7W4grdvGUF5tY+Tj2h/0J6a9BQvpZU1B/To/OcHx3PyXz7n+hH5zNuwmwWbipldsDv4QdPOnV92xXHd6ZGbjohwypEduH10H0b168TQHjlU+5Qqn7/eeUUPXzmUOQW7eHrqOnq1z+DZ64/nyI6N9wAkuc9f41OSvU37OR3M7rL9c8LapicfZE8T7Wp8fjwieDzC1uJy1u3cx46SSopKq9hdVsWufSHf91Wzq6yKiT89jZyM8MyVFJGXcSaMdxCRTTg9OOOA10TkRqAAuLLucc3tKWpF1wCPu7dfd+/PcedFjQGygWdUdUKd4zoAew7SbkHgg9Z1u4hc5t7ugfOhWffDdp2qzndvzwHyG2i7vv06AO+qagVQISLvB3Z2h6SqRKSNqpYc0NphcrAeo/XjxpSx/8KD+h7febDHm0JE0oB2wH31PNzY6wmt95o26/WE1n9NEyuB8vs4tmcub8zZFLZhnmji9yu/ensRlw3rxn3vL63VSzS8Vy6fLdvOh4u28vEdpzbp3I/p3pYzjurIviofWalJrNlRSnZaMnlt01k/bkxwv2qfn6LSKrq0rT1PaESfDsHeoU7Zadx1ztHBx1KSJDgJvD7H9WrHBz8ZSX6HzCZP3k7yOOdU4/cDLc+gQudA9WiX0eL2zKGpqvFTtK+S3IwU0pK9bC0uZ96GPZRW1FBSWcO+yhpKK2soqaihpKKakooa9lZU8+fLBtM/L5s35mzi568vYOrdo+iem8Fbczfz4Kcrgu2LQG5GCrkZybTLTKFX+wyG9cwhnCO4qnpNAw+dGb5naR0i0g4YjlNqAZwhom9E5Beq+g7wjlvD6iGgbgJVjjN005B9Ic9zBnAWcLKqlonIFw0cWxly2wekN9B2U/cLlQpUNGG/eDYQmNHAxOvGXk9ovdf0UF5PaMXXNLESKPEEf9kW7auiQ1ZqpCNqsQ1FZXy0eCs922Vw3sAuvDVvM2VVPu6/dBD7qnyc7s712lBUxu2vzONHpx/Z5MTxwmO60qlNGiUV1WSlJvHgJytYtLmYqXePqtVGstdzQPIUDs0dMgtcGVXjC88nX2h9n3hMuFtDVY2fKp+f8iofqckestOSKauq4cuVOxnYNZse7TLYuKuM/00vwOdX/Kqowp6yKnaVVVNcXk1pRTWllTX87qKBXDA4jzkFu7nm39N56YcnMuLIDsxav5vbX55X63nTk71kpSWRlZpEdloSbdKSUfdtMLBrNnedfRQZKc6vu4uHdOW4Xrm0z0yhQ1Yq2enJeD322h7E5cBHqloNoKqbRWQjcCrwpbvPvTjzX2pR1d0i4nV7NUpwJpI3pC2w2/2g7QecdJB9D9XXwFMi8hecz78LgcCcnPbAzsB5JrDBwML6Hgh9Pd1en0i/pg2+ntD6r2liJVDeFF6fs4kkj7B1T0XMJlBzCnbz89cXsG5nMNHnwmPyuGBwHi//8CT8qgzPb1frmJ7tM3jn1lOa/Vy3vDiHoT1yefq64fzsnKMoKCqL2kQiOIQXpq6DveX7/wCbsW4XJx3RPiztxpMNRWUU7NrH956ZecBjd519FLef2Zfi8mp+9MIc/vKtwVxzQk/2lFXz32nrSfJ4EJweoDZpyXTISiE7PZluOWm0SU2mc7bz77NPpyz+8q3BHNHBGcY9vW9HPr3zNDJTvbRJTSYz1Rt87evTPy+b/nnZwfs92mVYj2LzXAM8WGfb68A1IvIVzlDkx6o6t4HjJwAjVfUzEfnarcb+MQcmXJ8APxKRZcAKYDphpqqzROQ9nARhO7AICEyKHQV8GO7njEGDgQP/Qe83Aedigs9UtSiSr2kjrye08msqqolzpVFqm1zNu/UFXvu/kzmhd7vGD4gCgZ6PjbvKuOXFuSzafOAE+MeuGcaFx+S1SmLz8IQVdM1J5+oTeoa97XB7eeYG7nlrEdPuGU1e26b27jZsy55yRoz7HICXbjoxOBwZz3x+pbCkgu17KykqreTG553SRqv/dD5JXg8PfrqcldtL+ff3nSv2v/2vb5hTsDt4/PUj8umak0Z6ShJDu+cwuHtbanx+VmwvoXtuRtzOJRORMlXNjHQch5uI3I4zAX4WMF9Vn6xnn2OBn6rq9w53fPURkSxVLRWRDJwetJtVda6IvAWMVdWVEQ4xqsXK6+k+1qqvaWL1QLkKSyq46qlpPHL1sFYZegqX1YUlnPXwl3z5i1GUVtYEk6e26cn846qhjOrXqdVjuG10X/47bT3jPl7O2QM6c1yv6F2uKzgHKkxDeBkp++dRhatXK5qUVdWwZU8FX6/eye/eW0K3nHS2762o91xr/EqSFzpkpbJ9b2Uwsb/7vH7U+P3MWLuL28/sW+9QWJLXw8CudgVjPFLVR4FHG9lnrohMrqeeUKSMF5EBOHNxnnfjSwHeseSpcbHwekLwisFWfU0TK4Fye9s27CpjxrpdbNhVFhUJlKqycFMxL8/cwLqd+7hwSFe+d1IvUpO8tElNoqLGR/+8NsFegMPp7jcX8va8zQC0z0yJ6gQqKzUpOOxTn6oaP0u2FDO0R85Be+u+XLmDaWuLGBwyBysWe2p37avi2D9O5NS+HejTKYvteyvYUVLJoG5t+d1FA/l6dRG/fnsR5wx0CmAfn59LXk463XLS6ZKdRsc2qVT7/PTt1IY097LGH5zSu9ZzBHpyRxwZ/71z5tCp6rORjiFAVa+tZ1sV8N8IhBOTov31dLe3+muaWAmUa1DXtrWuIosEVWVOwW4+WrSNT5dsY/OecgB6hszN6NEug0V/ODd4PykMtY2aKzSek4+M7jlA5w/O4/zBeQ0+ftS9HwPQLSedST87nbfmbmbyikK+WFHId07sxWlHdWDEkR34/rPO8H/HNvuTsWhMn1SV575Zj8+vXHl8D0780yT+ftVQzhvUhXvfWcQL0zcA8NWqncwt2E2Xtmm0z0ylR67zmh7bM4e7z+tHfodM7r90cCRPxRhjYk5iJVC+at788YhgVeGZ63axtdipgH24vDC9gCkrd7Bu5z5WF5aSkuThtL4d+OnZR3FW/05hqzsTLnee1ZdH3AKVA0Im4saarcXlwdub95TT7zefkJ2WREqSh2qfk4g89836Wse89eMRnPrXyQAHLCDb2iprfFT7lKzUJCqqfby/YAs7S6soKq3kkyXb6NU+g69X7y+lktc2nfJqH4FUb1iPXF6btYkqn5+Pbj+VAV0PfO3aZ6Xy7SYshGuMMeZAiZVAobWGoJ7+ai2zC3Zz3qAupCbVrhtUUe0LDltU+/ys2VFKvy7Oh9DV46cxt2APuZnJ5Gak0D4rxfmemcLAbm0Z3a9T8Aq/t+dt4l9frOG920aSluyloGgfG3eV0TUnnRtO6c3FQ7tG9SK1IsL0e84k2esUIYxmCzbu4R+freS3Fw2kd4fa83nX7XCuWLz6+B68MmsjADN/fRZJHuGzZdvp0ymLFdtK+XLlDrbtreCPlwyqdaVWlc/Pb99dzH+nFdA9N52Tj2hPklfwiJDkcX423XMzuHGkM8T1v2nryU5PDibn//5yLXvKq6io9lNZ43O/+6mo9lFR7aOsysdxvXL51QX9UVWG3TeRq4/vyW8vchZy/sUbzlXFGSleyty6XAG3n9mXcwd2ZtLPTg8WHP32cd0tOTLGmFYUvZ/crUJ4ffZGTjqiPT3aZfC9k3sxYel2Plq0le65GXyzuogFm/awbOtethZXMPGnp9G3cxvGfbyc12ZvZNavzyIt2cuZ/TozpEcOe/ZVU+RWMF66ZS87Sip5floBAL+7aADfPzmfzm3SOLpLNjtKKunRLoNfjxkQ4Z9B80XDPLGmqPb5KdpXVW9v0bG9cnnpphPpl5fNDSN7k5rkCSbI5w1yhv36dGrDmGPqHwJcu6OU/7qv7abd5UxdvZMav+L3Kz5VfD5lQNfsYAL1yqyNdM1JDyZQT325hj1l1cHnDX5P9pKW7CEjxUumW6dIRLjn/H7BZCgt2ctXvxxF+6yUYC2j+jSlWrsxxpjwiPoyBiJyNPBqyKYjgN/iTA57Faec+3rgSlXdXff4UKmZ2Zr3k5d59JphXDykK36/ctbfp7DW7Z0QgaM6teHoLm3I75DJraOOJDXJy5yCXRSVVjG6X6eDTuJWVRZtLuabNUXsLKnkllF9gsuamMjYWVrJL15fwAPfPoZO2c1PBPPH1i4h0pwSCaHFNwMLH5v4lKhlDIxJZFHfA6WqK3BXIxcRL84qzG/jLGI4SVXHuYtUjgXubqQ1AHx+p4fC4xHGf284z369jqHdczhrQOd6E57jejWtZpSIcEz3HI7pntOk/U3rm762iMkrdrB2575DSqBC3X1ev2bVlwq90s+SJ2OMiS+H95r4ljsTWKOqBTirgz/vbn8euLTRo93OttA6QX06ZfHnywZz5fE9rLcoxm0rruC8f3zJp0u2AfD795awo6SSBb87JyxVxPt1OdiKBcYYYxJJrCVQVwMvu7c7q+pW9/Y2oHN9B4jIzSIyW0Rm+/1OzS9fHBZFNOARWL6thJ+8NI/8sR/y3Dfr+cP7S1s0SX9kSPXxXu1t+Q+TuETkUhFRd02z5hyXLiJT3BGEQ3neUvf7Nw08/nsR+XkjbeSIyC0h9+ttq4nxpIjIlyIS9SM4pnXFTALlVhW9GGcNplrUmchVb1akquNVdbiqDvd4nNOttgQqLqW5lcOrQhYBnnr3qBYNn71w04nB23Wv7DMmwVwDfOR+b44bgLdaWrVaVUe04PAcIJhAtaQtt0DjJOCqFsQTs8KVELu3D0hkDyUhbqitJsZzyAlxzCRQwPnAXFXd7t7fLiJ5AO73wkZbUD+f3HkqFzVwpZWJbenJtf89L//jeXTPbVmv0bQ1RZxyZHsevXpo1C6ibExrE5EsYAROEtLcxOE7wLsiMk5Ebg1pM/hBKSLviMgcEVkiIjc3EEPoB++vRWSliEwFjg7Z3lA744AjRWS+iDxYp627RGSx+3Wnuy1fRJaJyL/dtiaISOgEyHfc84p79SRKYUmIoUWJbA4hCXFL2mpJQhxLXZDXsH/4DuA9nEUsx7nf321KI4FaTib+JIdcIbnw9+cEyxS0xDX/dhYPf/CKIS1uy5hDkT/2wy+A59aPG/Nc/tgPk4GJwNPrx415IX/shxk4vUL/Wj9uzKv5Yz9si/O78NH148a8lT/2ww7AG8Df1o8b837+2A+7rB83ZtshhHEJMEFVC0Rkp4gcp6pzGjvIHTk4QlXXi8irwD+AJ9yHrwQCSy3coKq73CRlloi8qapFB7YIInIcznSOoTifYXOBOY20MxYYpKpD3TZ+HNLWD4ATAQFmiMgUYDfQF7hGVX8oIq8B3wZecJ9nMXB8Y+d/KNzXuzEfrB835qGQ/QPvj8DrHbR+3JgzmhuDiLwO7AKGAB8A94c8/B3gWne/ccBGVX3Cvf97oFRVHxKRd4AeOGvUPaKq4+t5nlJVzRKRX+N8jhcCG9n/etJAO8GEGJioqr8ItOUecxdOogfwtKr+Q0TygY+BqTh/DGwGLlHVcpyE+C/Ai835OcVED5SIZAJnA2+FbB4HnC0iq4Cz3PuNeu7rdSx2F+U18Ss7LTms7fmjvNyHMa3sGvZPn3jdvR+YF/VvEXlVRM6p57gOwB4AVZ0HdBKRriIyBNitqhvd/W4XkQXAdJwPy74HieVU4G1VLVPVvTh/TAc0px2AkW5b+1S1FOcz5lT3sXWqOt+9PQenZA7uufiAKhGJ1ytLBgPbVfUkVQ0mT6EJsbvpVZxEOOBK9pcdukFVjwOG47wu9V7JUychvoADE9P62hmLc0HZUFX9RT3tBZLik4Afisgw9+G+wBOqOhDnffltd/shJcQx0QOlqvuA9nW2FeFcldcMwu/fX8qvLujHoG62OrxpOsufTKSE9iCsHzemGgi9X1bnfnGd+zvr3G9275OItMP58JrkbnoD+EZEfqGq7wDviEgu8BAwoc7h5Tg9BwGvA5cDXXA/aEXkDJw/gk9W1TIR+aLOMU2NMyzthKgMue0D6tYwSQUqWtB+vZrbY1Tn/VHr9T4UIpIGtAPuq+fhYEIMTlIsIp1EpCvQkQOT4svc24Fktr5exWBC7D7/e3Uer6+dg72Pg0mx214gKX6PBpJiVfWJSJWItFHVkoO0XUtM9ECFm80hj1/L7juPVX86P+ztWg+USWCXAx+pajWAqm7GGWY5NWSfe9k/NBfkFjf2uh/K4CRNV7ttBnq02uJ88Ja5V/id1Eg8XwKXupOZ2wAXNaGdEqC+3qKv3LYy3JGOy9xtB+X2guwM/EzizEBghqrW1PNY3YQY9ifFV1F/UjwEmFfPcY0KVzsh6ibFoZ1IzU6IEyyBChTStA/DeJWe4q01FypcBJtAbhLWNdSZV4M7jCeOB4CPVXVuA8dPwOkVQFWX4CQym0PK0HwCJInIMpypGNMPFoz7PK8CC3DmtMxqrB13xOJrd6L4g3Xaeg6YCczAmS8z72DP7xoFfNjoXrFpMLCwvgfqSYih5UlxQwnxwdppKCGGQ0iKDzUhjokhvHCL9uVrTPQ4q39njuuVS0+rAWUSlKqOqmfbIwAicjtOD0FbEemjqk/W08QTwE+Bz9xjB9dpqxLnKuuGnj8r9Lt7+0/An+rZ/WDtXBty9xch2x8GHq6z73pgUMj9h+o0dy3OPJx4NBgnoWxIICEOvJ5L3MSnblL8IzeZXcFBkmJVneteYLAAZxL5rJCH621HVYtE5GsRWYyTvP+iTnvPhZzD0+5QY/5BzumQEuKoXwsvnFLTMzTvjte56+yjuP3MxuYWGmNM04ithXdQInID8Hw4Ln2PNHci9dWq+t9IxxIJInIs8FNV/V6kYwkXd57UWFVd2azjEiqBSsvQNVt3kp2eHPartEx8mlOwi6emrOV3Fw+kW07T18EzicUSKJNILCF2j020BKqyoizSYZgYkj/W6dX95M5TrYaYaZAlUMYknoSbRP6vL9bwzZqdkQ7ExBi/v/F9jDHGJI4ES6Dg7xNX8tUqS6BM81gZA2OMMaESLoESsQ9D03z2njHGGBMq4RIojwh+qwNlminJk3D/VIwxxhxEwtWB8nrEKpGbJrtoSFeO7ZnDgK42gdwYY8x+CZdA2RCeaY7HrhnW+E7GGGMSToKVMUjX7buKSU3ykp7ijXQ4JgYs3lzMXz5exm8uHGBlDEyDrIyBMYkn4XqgcjJSIh2CiSEXPjYVgL3l9a2raYwxJlEl3MzYxz9fxSeLt0U6DGOMMcbEsIRLoJ6fVsCUlTsiHYaJMYk01G2MMaZxCZdAecQ+DI0xxhjTMgmYQAk+q2NgmskuOjDGGBMq4SaRe8TqQJmmu3J4d4b1zOWY7jmRDsUYY0wUSbwEymNDeKbp/nr5kEiHYIwxJgolXB2oktJ9eD2C1yORDsfEgEWbirn33cXcf8kgBndvG+lwTJRqrA6UiPwUuAlQYBHwA1WtOFzxGWPCL+HmQKUkeSx5Mk120eNTWbBxD6WVVgfKHBoR6QbcDgxX1UGAF7g6slEZY1oqJhIoEckRkTdEZLmILBORk0WknYhMFJFV7vfcprT12KRVvDJzQ2uHbOKMkjg9taZVJAHpIpIEZABbIhyPMaaFYiKBAh4BPlHVfsAQYBkwFpikqn2BSe79Rr2/cIvVgTLGHDaquhl4CNgAbAWKVXVCZKMyxrRU1CdQItIWOA14BkBVq1R1D3AJ8Ly72/PApU1pz8oYGGNaQZKIzA75ujnwgNs7fgnQG+gKZIrIdyMVqDEmPGLhKrzewA7gPyIyBJgD3AF0VtWt7j7bgM71Hez+IrsZwJucYmUMzCFpk5oc6RBMdKtR1eENPHYWsE5VdwCIyFvACOCFwxWcMSb8YiGBSgKOBX6iqjNE5BHqDNepqopIvWmRqo4HxoNzFZ6VMTDNcf2IfIb0aGtX4JmW2ACcJCIZQDlwJjA7siEZY1oqFhKoTcAmVZ3h3n8DJ4HaLiJ5qrpVRPKAwqY0luL14LGr8EwT/f7igZEOwcQ49w+/N4C5QA0wD/ePOmNM7IqJOlAi8hVwk6quEJHfA4F6K0WqOk5ExgLtVPWXB2snNS1dKyvKWzlaE08WbSrmrtfm88Dlx3BszyZd6GkSUGN1oIwx8ScWeqAAfgK8KCIpwFrgBzgT4F8TkRuBAuDKCMZn4tRFj08FoLzKF+FIjDHGRJOYSKBUdT5Q3wTNM5vb1uOfr8LjEW45o0+L4zKJIwY6ao0xxhxGUV/GINy+XLmTr1bujHQYxhhjjIlhCZVAKSACPutOMMYYY0wLJFQCBeD1iJUxMM2Wk2F1oIwxxuwXE3OgwskKaZrm+PEZRzK4W1sGdbM6UMYYY/ZLuAQqOz3J6kCZJrv7vH6RDsEYY0wUiok6UOGSkpauVVYHyjTDrPW7uOXFuTxx7bGc0LtdpMMxUcrqQBmTeBKuB8qY5rjiyWkAVNZYHShjjDH7Jdwk8icmr+bPHy2LdBgmxiRQR60xxpgmSLgeqDkFuyksqYh0GMYYY4yJYQnXAyVYb4IxxhhjWibxEiixBMo0X/uslEiHYIwxJook3BCeiGD5k2mqn59zFP3zshnY1epAGWOM2S/hEqgOWalU1fgjHYaJEbeN7hvpEIwxxkQhqwNlzEF8saKQm56fzXM/OIGRfTtEOhwTpawOlDGJJ6F6oBIoVzRhcv1/ZgFQ47deS2OMMfsl3CTyRyet4u43FkY6DBNjLPc2xhgTKqF6oERg+ba9rNxeGulQjDHGGBPDEq4HShASad6XMcYYY8IvoRIoVbcOVKQDMTGnS3ZapEMwxhgTRRJqCA/cOlCWQZkmundMf/p1yaZ/XnakQzHGGBNFEiqBEoGuOWnsq6yJdCgmRtx06hGRDsEYY0wUsjpQxhzEhwu3cutLc/nP9cczql+nSIdjopTVgTIm8STcHChjmuPWl+YCoDZzzhhjTIiYSKBEZL2ILBKR+SIy293WTkQmisgq93tuU9r6x2cr+dH/5rRuwCbuWPJtjDEmVEwkUK5RqjpUVYe798cCk1S1LzDJvX9QIrChqIwlW4tbM05jjDHGxLlYSqDqugR43r39PHBpk44SsFU5jDHGGNMSsZJAKTBBROaIyM3uts6qutW9vQ3oXN+BInKziMwWkdk+vx9BDke8Js50z82IdAjGGGOiSKyUMRipqptFpBMwUUSWhz6oqioi9c5SUdXxwHiA5NR09QhWidw02Z8vG0zfzlkc3aVNpEMxxhgTRWIigVLVze73QhF5GzgB2C4ieaq6VUTygMLG2hGB/A6Z7CmvbuWITby4+vgeVPn8+PyK12O9l8YYYxxRP4QnIpki0iZwGzgHWAy8B1zn7nYd8G5T2rt1VB/+/f3hje9oDPDWvM30+80nfL680fzcGGNMAomFHqjOwNsiAk68L6nqJyIyC3hNRG4ECoArG2vIbyN3ppl+/vqCSIdgjDEmCkV9AqWqa4Eh9WwvAs5sTlsC/H3iSmasK+KVm08OU4QmEdi8OWOMMaGifggvnESgsKSCNTv2RToUY4wxxsSwhEqgAETEqkobY4wxpkUSKoHyqzOMZ8MxprmO6GjrxBpjjNkv6udAhZPgDONZ+mSa6pGrh9K7QyZ9OlkdKGOMMfslVgIl0LdTG04+wupAmaYZMziP4vJqKmt8pCZ5Ix2OMcaYKCGJNJyVkpauVRXlkQ7DxJD/TS/gN+8s5snvHsd5g7pEOhwTpUSkTFUbHOcVkRzgaWAQTif4Dao67TCFZ4xpBQnVA2V1oExz/eadxZEOwcSHR4BPVPVyEUkBbHFFY2JcQk0iD9SBOvfvX0Y6FBNzLPs2h0ZE2gKnAc8AqGqVqu6JaFDGmBZLrARKYG9FNVuKbRjPGHPY9AZ2AP8RkXki8rS7LJUxJoYlVAIFIFgdKGNM2CWJyOyQr5tDHwOOBf6lqsOAfcDYiERpjAmbhJsDJWJ1oEzz9e1sZQzMQdWoakOrlG8CNqnqDPf+G1gCZUzMS6gEStwvS59MU/37+8Pp2S6DIztmRToUE6NUdZuIbBSRo1V1Bc4anksjHZcxpmUSK4ES6J+XzTkDOkc6FBMjTj+qI9uKK9hXWUNmakL9czHh9RPgRfcKvLXADyIcjzGmhawOlDEH8e8v1/Knj5bxr+8cy/mD8yIdjolSjdWBMsbEn4SaRG51oExz/emjZZEOwRhjTBRKqARKgEc+W8Xw+z+LdCgmxljubYwxJlRiJVACFTU+isurIh2KMcYYY2JYQiVQ4F6FZ90JxhhjjGmBhEqg/KpOHahIB2JiSnqylwF52ZEOwxhjTBRJqOuyxf0vka48NC3z0k0nkpeTTn4Hu8DKGGPMfgnVAyUCg7plc/lx3SMdiokRx/bKpbzKR3F5daRDMcYYE0USqg5Uclq6VlsdKNMMj05axcMTV/L4tcO48JiukQ7HRCmrA2VM4omZHigR8bormX/g3u8tIjNEZLWIvOpW+D2oREoWTXg8PHElYBceGGOMqS1mEijgDiC0quEDwN9VtQ+wG7ixsQYE4dFJq+jzq48smTLGGGPMIYuJBEpEugNjgKfd+wKMxlnVHOB54NLG23GuxKvxq/UomGaxt4sxxphQMZFAAf8Afgn43fvtgT2qWuPe3wR0a6wRxemFCtw2xhhjjDkUUZ9AiciFQKGqzjnE428WkdkiMtvvd+pAgc2HMk3XqU0qw3rkRDoMY4wxUSQW6kCdAlwsIhcAaUA28AiQIyJJbi9Ud2BzfQer6nhgPEByarpKYHurh23iwdu3jKBTdhrdctIjHYoxxpgoEvU9UKp6j6p2V9V84Grgc1X9DjAZuNzd7Trg3cbaEoFjeuRw/Yh8pLGdjQH6dclmy55yikorIx2KMcaYKHLYEigROVtE/i0iQ937N7ewybuBu0RkNc6cqGcaO0AVTj+qI7+/eCBJ3qjPHU0UePTzVVzx5DSmrt4Z6VCMMcZEkcM5hHcD8GPgXhFpBwxtbgOq+gXwhXt7LXBCM4+nxuenxq+kJnkQsX4oc3D/+mJNpEMwxhgThQ5nN0yJqu5R1Z8D5wDHH8bnBkBE+PdX6+j3m0+oqPY3foAxxkQBEfGIyK8iHYcxZr/DmUAFKoiPxJkI/t/D+NwOAet0MsbEGlX1AxdGOg5jzH6HJYESkWHAqSKyHvgzsExVHzscz12LEpw87rcyBsaY2LJQRH4nIjaB05go0GpzoETkKOAanCvnduBUDR+hqlta6zkboxpSBypSQZiYc0THTI7PbxfpMIxpB5wO/FhEZgALgYWq+npkwzImMbXmJPLlwIfAOaq6sRWfp8lEZH8lcuuBMk0w8aenkZORQsc2qZEOxSQ4Vb0SQERSgYHAYJwLaSyBMiYCWrMr+FvAPmCqW77gHBHxtuLzNU5gaM8cbh11JMlWxsA0QbfcdBZvKWZbcUWkQzEGAFWtVNW5qvq8qv4i0vEYk6iktXtiRCQTuARnOO844CPgDVX9pFWfuB7JqelaXVl+uJ/WxLDfv7eE575Zz9+vGsJlw7pHOhwTpUSkTFUzIx2HMebwafVuGFXdp6ovqepFON3OM4GI/NXkV6Wi2kdRaSV+vw3hmcY99836SIdgjDEmCh3WcSxV3a2q41X1zMP5vAEeEV6euYHj7v+M4vLqSIRgYpRNmTPGGBMqsSYCyf4yBvZ5aIwxxphDlVgJlBJcvsWuwjPGGGPMoUqoBMqvisfqQJlmGtIjh5F9OkQ6DGOMMVHkcC4mHHEe2b+Wi1UiN03x9djRZKUm0TY9OdKhGGOMiSIJ1QMFMKxHDr8872gyUxIqdzSHKDcjmamrdrJxV1mkQzHGGBNFEi6LGNStLYO6tY10GCZG/OG9pbw6eyMPXTGEHu0yIh2OMcaYKJFQPVB+VUora9hQVEaNzx/pcEwMeHV2VKxCZIwxJsokVALlEeH9BVs47cHJ7CitjHQ4JobYVZvGGGNCJVQCBSF1oOzz0BhjjDGHKPESKCtjYIwxxpgWSqgEyq9qhTRNs53atwOj+3WKdBjGGGOiSEJdhecRsSE80ywLfnsOqcke0pK9kQ7FGGNMFEmoHiiAYT1z+MPFA2mbYYURTeOSvMJ787ewdkdppEMxxhgTRRIugerTqQ3XjcgnO80SKNO4e95axC/fXMjsgt2RDsXEOBHxisg8Efkg0rEYY1ou6hMoEUkTkZkiskBElojIH9ztvUVkhoisFpFXRSSlsbb8qhSXVbN0y14qa3ytH7yJee8t2BLpEEz8uANYFukgjDHhEfUJFFAJjFbVIcBQ4DwROQl4APi7qvYBdgM3NtaQR4TJKwq54NGv2Ly7vDVjNvHG5syZFhCR7sAY4OlIx2KMCY+oT6DUEZiAkux+KTAaeMPd/jxwaVPaszIGxphWkCQis0O+bq7z+D+AXwK2BIIxcSImrsITES8wB+gDPAGsAfaoao27yyagWwPH3gzcDOBJ3j/KZ1fhGWPCqEZVh9f3gIhcCBSq6hwROeOwRmWMaTVR3wMFoKo+VR0KdAdOAPo149jxqjrc+eUmwTpQ1gdlmuq8gV04d2CXSIdhYtcpwMUish54BRgtIi9ENiRjTEvFRA9UgKruEZHJwMlAjogkub1Q3YHNjR1vdaBMc63+0/mICF6PNL6zMfVQ1XuAewDcHqifq+p3IxmTMablor4HSkQ6ikiOezsdOBvnSpbJwOXubtcB7zalvaE9cnjoiiF0yk5rhWhNvKmo8fP8N+tZsa0k0qEYY4yJIhLtS5qIyDE4k8S9OAnfa6p6n4gcgdMd3g6YB3xXVSsP1lZSarrWVNrVd6bpbnp+Np8t2864bw3m6hN6RjocE6VEpExVMyMdhzHm8In6ITxVXQgMq2f7Wpz5UM1pi937qli9o5SBXbPJSIn60zcR9tmy7ZEOwRhjTBSK+iG8cPKIMGNdEVc8OY31O8siHY6JIdHdT2uMMeZwS6gEyuFMBlb7SDTGGGPMIUqoBEqBwMVUUT71yxhjjDFRLKEmAalqsA6UJVCmKTwCo/t14uIhXSMdijHGmCiSUD1QtepA2RBe3Nm9r4opK3eEtU2/wmfLCslMTai/NYwxxjQioRIogGO6t+XJ7x5Lr3Z2xXG8ufWluVz37EzKq3xha/OSoV05d2BnFm8uDlubxhhjYl9CJVAKdMpO47xBebTNSI50OCbMTunTAdi/YHRLqSpFpVV8umQ7CzbtCU+jxhhj4kJiJVCqFJVW8sWKQorLqyMdjgmzcCVOoaau3gnAtuKK8DdujDEmZiVUAuURYeHmYq7/zyzW7CiNdDgmzD5atBWAap8/7G0/9vnqsLdpjDEmdiVUAgXYYsJxrMAtjmovrTHGmNaWUAmUUwdKQu6ZeHLJMKfUQIo3od7WxhhjIiChPmmcOlDObb/lT3Gnc5s0ALye8E2GSk/2AnD/pYPC1qYxxpjYl1AJlFMHygppxquVhc68Nl+YsmMRobzaKYlwat8OYWnTGGNMfEioBApgYNds/nfjCRzduU2kQzFhtmDjHgAqq8M3ifymkb0BuPm/c8LWpjHGmNiXUAmUouRmpnBq345WByoOfe+kXgB4wvSu9vuVeW5StmJ7SXgaNcYYExcSK4FSKCqt5MOFW9lZWhnpcEyYBea3hWt0VoE5BbvD1Joxxph4klAJlEeEVYWl3PrSXFZusx6FePPW3M2A03NkjDHGtKaESqAgpA5URKMwrWHznvJIh2CMMSZBJFQCpSgidhVevLpsWDcAMlOTIhyJMcaYeJdYCZSGzpOxDCre5LgXBnjDuChedpqTjD12zbCwtWmMMSb2JVQCJSK2lEscW7Z1LwCVNeEpY+D1CHsragDo2zkrLG0aY4yJDwmVQAH0y8vmrVtGMKRHTqRDMWFWUOSshVdWVRO2Nu8+rx8ANz0/O2xtGmOMiX0JlUCpKlmpSRzbM5e26VYHKt5cc0LPsLbn8yvvLdgCwKbdNkHdHGjFthKG/GFCpMMwxkRA1CdQItJDRCaLyFIRWSIid7jb24nIRBFZ5X7PbUp7RaWVvDZrI1vsiq24E/Y6UKrBYUFj6tOzXQbPXj880mEYYyIg6hMooAb4maoOAE4CbhWRAcBYYJKq9gUmufcPSkTYuLucX765kOXb7IMx3rw0YwNg89vM4ZOe4uW4Xu0iHYYxJgKi/npvVd0KbHVvl4jIMqAbcAlwhrvb88AXwN2NtWeTyOPXjpLwVpe3t4hpzMrtJWzaXRbpMIwxERALPVBBIpIPDANmAJ3d5ApgG9C5gWNuFpHZIjJbVfG44zxWrDr+XDLUqQPVLjMlwpGYRPHW3M220LQxCSpmEigRyQLeBO5U1Vrjb6qqNNBhoKrjVXW4qg5XQubJWBdU3MlyazZ5PeGrAxW42MDmuZj63HRqb9659ZRIh2GMiYCYSKBEJBkneXpRVd9yN28XkTz38TygsLF2Qj9XLX2KP0u3FAOwrzI8ZQySvR6Ky6sBaJeZGpY2TXzpkJXKoG5tIx2GMSYCoj6BEmftlWeAZar6cMhD7wHXubevA95tSnt9OmXx6Z2nMeLI9uEN1ETcztIqAErDlEAB3H/JIABueG5W2No08eOTxduYt2F3pMMwxkRA1E8iB04BvgcsEpH57rZfAeOA10TkRqAAuLKxhlQhLdnL0V3atFasJoKuHN6D+Rv3hO0CgWqfn8cnrwZg176q8DRq4srv31vCaUd1iHQYxpgIiPoESlWnsv/iubrObFZbOB+E78zbzOh+ncjvkNni+Ez0CPc6h6qwbW9FWNoy8emtW0bgEeHBSAdijDnson4IL5w8QGFJBfd9sJSlViCxUYUlFazbuS/SYTTZc1+vB6xEhTl8uuak06VtWqTDMMZEQEIlUECwjIF9yDbuhD9NYtRDX0Q6jCbbXWbDbObw2VFSyUszNrCt2HopjUlECZVAKfvHAv2WQcWdS4Z2BaBztvUImNa3ansJv3p7UUz10hpjwifq50CFU606UBGNJDZcNKQrSzYXRzqMJktN8uL1SNjqQElIMy/98MSwtGnixwm92zHtntHkZljhVmMSUUL1QEnI/62QZvxZtLkYn18pLqsOS3vJ3oT652GaKcnrIa9tOmnJ3kiHYoyJgIT7hOjVPoOpd4/i7AH1rvxiQsxat4u1MTQ8Ue3zA7C3IjwJVKgXp28Ie5smtk1bU8QL0wsiHYYxJkISLoFK9nronptBRkpCjV4ekpOOaEev9hmRDqPJLhvWLaztVdb4grfDVRrBxI8PF23hbxNWNLqfiPQQkckislRElojIHYchPGNMK0uoBEqB4rJqHpu0imVWxqBRlTV+yqp8je8YJSTMV1jaKK85mN9dNJBJPzujKbvWAD9T1QHAScCtIjKgNWMzxrS+hEqgBNhTXsXfJq5k6RZLoBrz8eJt7CipjHQYTfbklDVA6/QWWTJl6kr2emiX2fgEclXdqqpz3dslwDIgvN2lxpjDLqESKAAJTCKPcBwm/MK1iLAxTfG/aeuZtGx74G6SiMwO+bq5vmNEJB8YBsw4TGEaY1pJQiVQtcoYWJdC3LnYrQPVs134523de6GNuJjaHvt8NZ8tKwzcrVHV4SFf4+vuLyJZwJvAnapqXeDGxLjEmkmt9d40DRgzOI8V20siHUaTeUVI9kpwLlRLeULa6ZaTHpY2TfyYds+ZtS40OBgRScZJnl5U1bdaNTBjzGGRUD1QIiHFES2DijsLNxVT7VN2loZn3lZK0v5/HiFDNcYA4PVIk67mFSejfwZYpqoPt3pgxpjDIqESKIC8tunM/+3ZXDKsa6RDiXpzN+xmdWFppMNoslQ34dlbHv46UK/M2hj2Nk3sWrdzHw9+upzNe8qbsvspwPeA0SIy3/26oHUjNMa0toRKoBTnr8acjBRSk6x6cGNOPrI93XNjZ+gqMAcqXJ2LFdWxU8LBHF5rCkt5csraJiXrqjpVVUVVj1HVoe7XR4chTGNMK0qoBAqFkopqxn28nHkbdkc6mqi3t7yaXfuqIh1Gs7VGHSi75sCEOmtAZ1befz5Hd24T6VCMMRGSWAmUQHmVjyenrGGJ1YFq1GfLCmOqkObDE1e6t8Kf7Xxmc6BMHV6P4AnTwtXGmNiTWAkUBH/h+a1LIe5U1/gjHYJJEOO/XMP/bB08YxJaYiVQCske55RrfJZAxZvzB+cB0KdT+IdVMlNszpzZ78uVO5m2ZmekwzDGRFBi1YECRJzEyee3BKoxGSnemBrCm7GuCIAPFm7hwmNafpWlJ+TPi3ZZjS/ZYRLHCzedaMV4jUlwidUDxf4FZ2ssgWpULCVPAIs3O/PaPly4NSzthV6puXFXuX1gmlrCVbDVGBObEi6B8gis+tP5/N9pR0Q6FBNmndqkAjCkR06rtG/5kwFYsa2Eu16dz7qd+yIdijEmgqI+gRKRZ0WkUEQWh2xrJyITRWSV+z23qe3tKa8h2euxq2eaYfm22Lhi8eIhzrCdN0w9A2VVtRcnrvLZJHUDhSUVzFi3y6YBGJPgoj6BAp4DzquzbSwwSVX7ApPc+01SXlXDve8s4t35m8MXYZybsXZXpENoksCwbEllTSN7Nk3dHqfyGBvSNK3j1L4d+XrsaPp0yop0KMaYCIr6BEpVvwTqfoJfAjzv3n4euLSp7WWmJvHC9A3c8cr8sMSXCGLlg+K5b9YDkOINTw9U3f4F628wxhgTEPUJVAM6q2pgpvA2oHNDO4rIzSIyW0Rmg5UvOBTJ3th6m8xa3zpV5m0SuQH44wdLefzzVZEOwxgTYTFfxkBVVQK1Cep/fDwwHsCTnKYFu2ziZ3NV1sTW0NWUlTsiHYKJY9v3Vtj8J2NMzCZQ20UkT1W3ikgeUNjUA20eS/PlZiRmDaSkOhca2EemAXj82mMjHYIxJgrE1tjMfu8B17m3rwPebeqBXbLTWyWgeHbrS3OpaeUr0J6Zuo7Bv/s0eH/Z1r34I/xXflpy7erjOenJEYrEGGNMtIn6BEpEXgamAUeLyCYRuREYB5wtIquAs9z7TVLt358ILN5cHOZo41NBURnb9la06nNMWrY9ePXcvA27Of+Rrxj/1dpWfc7mSoqxuWAm/GasLeKqp6ax3mpAGZPwov4TQVWvUdU8VU1W1e6q+oyqFqnqmaraV1XPUtUmX2e/fOterj2xJwDri+yXYFNlpLTuaO/QHjnBIbNNu8sBWNTMBPeHp/YOa0x7K6pr3S8sad0k0kS/Kp+fKp+fNmmxOvvBGBMuUZ9AhVtFtZ8JS7YB0D03I8LRRKd731lE/tgPa22r8bfuEN6OkspgHaf89pkAnHFUx2a1Ud3KV1j+9ZMVrdq+iX6n9u3I27ecQvus1EiHYoyJsIRLoHp3yGRnaRVAq8/riVUvTN9wwLZZ61pWGmBbcQWbdpc1+PjrczYFbycnOT1RWanN+ys/UAcqXOpWLbD3izHGmICES6BCl+PYXVZ9kD1NqJKKlv2sfv32In70wpwGH++Ws39yf2ZKEif2bkfbjNaftP3C9ALmbmhacjiwa9tWjsZEsxqfn9MfnMwrMw/8A8MYk3gSLoGaH/JhOXt9bCxRcrh1zj5weGLF9pIWtbmqsJQV2xpu49vHdQ/erqzxM2PdLj5cuLXB/Zur2ufnhekFB9Tvufedxfz67cUNHFXbiUe0C1s8Jvbsq/RxbM9cOtXz78MYk3gSLoGqDvkA3WM9UPX61QX9D9hWVdOy4asNu8oOOkcpUHFJVYO9XS/O2BDctrqwtNHnCO3FquuZqeu4953FvDKrdu9Bh6xUhvbIqfeYFLvqzoRom5HM368ayuh+DS58YIxJIAn3CXFUpzbB26/O3hjBSKJXfesEVrfy/J8nJq8O3g5chRfwwvQCznp4CrMa6THcvKe8wcfy2qYBkJNeuyjoztJKdpZW1ntMekrtOlCrtjeexJn41dI/Iowx8SXhEqgqmwh8SL5ZU9ToPhXVPjbtLjukpV9OPrJ98LbUWQt4wSannME6t/bO+p37eHTSqgPWpstvv/+qyrpJUZdsN4GqZ17VxKXb642pbvsrC1s2jGli28WPT+WetxZGOgxjTJRIuARqwtJt/PM7zlIMlw3rFuFoYkfo8NjkFYXkj/2QjbvK2L63gpnrdlHt8zNtTREjH5jM0i17m93+gLxs0pI9SN3sCefKSdifBF33n5k8PHEl2/dW8vRXa8kf+yFVNX7OGdgleMzK7SU8MXk1+WM/pKLax94Kp0hncybDF9VJwvaW25Bvfap9fh76dAWlbiHUeKSqXDSkKycd0b7xnY0xCSHhqsEJEvwl2DUnLcLRHD6rC0vo3SELr+fABKUpZqzbP3z2vFsuYOX2Ep6aspaZ63fx1S9HBScyBfptikorWbNjH4O6ZXN8fi7J9cwpKi6r5sLHvyLF66Gi2o+qBofbTnInbQfqQuW1TWNDURkDu2ZTUFRGsld43B3621dZU2vNvvaZqTwzdV3wsUDl6H2VB/aOndq3Q/D2nrIqht43kWeuG06+m7gF7C2P3wShJd6eu5nHJ6+mrMrHby8aEOlwWoWIcOuoPpEOwxgTRRKuB6pflzY89vkqABJlNG/l9hLOevhLHp20KiztpSY5b5vs9OTgEi81fmW6O8y30r3abtraIq58ahqbdpdzxXE9uGBw3gFtTV29k427ylmzw0lw/Lq//lJgzsmZ/TvxzdjRVNb4Oe3Bycx0a1KFLq1y5VPTeHLKmuB9f53htx7tnB60AV2za23PTkviyI5ZwfuBKwWfnLKGjDpzoNSWE65XvzxnXmE8X6W4fue+Vp8HaIyJLQmVQIlAcXk17y9wLo8f3a9ThCM6PCqrnV/8def0hFJVnpyyho8Wbmm0vUAPXp+OWRS7w1p/m7CCsiqndycwzyzwdAJMWLqdp750EpyvVu3gzZDCmaH8qrR1F+2du2EP4PR+jRj3OR8tcl63wPwmVaWHW01+VWFpMLEDqPEpd5zZl05tUslMTQomy546Q4R7K2qYF1LaItMt3nlkxyx2llTV2vf4/PhNEFoi8DNdF6frw/n8yrX/ns6dr86PdCjGmCiSUAkUwLvztwQ/gAN/Oce7JK/zAVd8kDk8M9btYtzHy7mjCR8S3z6uO1N+cQbZ6cnBNj+op2bTZ8ucydnLt5Xw2bLtbNxVjqryvWdm8rPXFwAHDqN+sngblXWudgokOP+bVlDnvDz0D3kNj+meE7z9n2/Wcd2IfGb++izSkr0s3+bMyzrvkS+ZU7CrVg2w0J9LYI2zE3q3O2Ay+x/eX9rATySxBea8jft4eYQjaR0C/PHSQXz3xF6RDsUYE0USLoEKHZbZUNTw0iLxoqi0kvMf+QqA56cVsHbHgZfi+/3K1eOnAwdfT26Ne+yKbSXc8cp81u2s3VYg4Qj0RASKVoa2GFhGJ2BYz1zWjxtDn07OMFqbtCSK9tXeJzB/qaSyhiHd2wbnLCV5hNnr9/celVXtn6P01tzNvDJzA3e9Op89ZfvbU4Vv/2salz85bX/7Ie+DtunJ3HlWX/p1yaZDlK939vrsjcH5aADvzNtMcUhts48WbeWBT1o/qfEdpGczHng8wpn9O9e6UtQYYxIqgfKI0Cl7f4/HizMKDrJ3fPhLnV6B0X+bckBP1GOfr6YpAktY/Pz1BczfuIfdZdW1ygJsLXbmQwV6nuoT+tjl//qGDxZuYUdJZbBQ5rKtJWwNqef032nrWR5Swfzd20YGE6K5BbtZGzJsVLfUwti3FvHWvM1MWLKdkxu5esrvJnsej/CPz1ZxwaNfkVtPyYOK6uaXaGgtv3hjIb97bwlLt+zlf9MLuPPV+fzwf7ODj9/y4lz+9cWag7QAa3eU8uXKHQcd3m1Mz3bxuyj3Z0u38/RXa60GlDHmAAmVQAEUl1VxVn+nkvDLMzfW+os9Hswp2F1rLkp9n4u/fnsR787fHLxMf3tJRZPaXripmB0llRS4PTYbd5Xxz2uPDT4emIO0cVc5g3/3aXBY7915m4P7jP9ybfD27ILd3PbSPI7/02fBbQ98spw2afsTl+17K2pN3s0f+yGLNjtDRtc+PaNWfDnp9a+d96ePlpGbuf8KvU5tUoNtBQR6z0J/Xkf/5pMD2upXz7bDqbSy5oC5Rhc8+hW/fddZjmbmugOLjR4sORr9tyl8/9mZBwybBlz37MxGk7DdIT18ob2A4eDza6sv4hwohVGfScsLeXPuZuq7eHXeht0UNvHfjjEm/iRUAiXi9JKE9iI0NXmIlO17K9i+t+kxfvtf3zDqoS+CV5P17nBg78AHC7dyxyvzOe7+z6jx+XlpRtMWR52xbletD++7XlvAgxNWBO+H1n8qCakJNGl5YfB2UyYa7w2p1fTE5DXBhK0xexqY41VcXh0cxgQoLDmw8vhJf5nErPW7mrRkTCSvxrr0ia8Z9dAXbNxV+2dysA6k3vd8VOv+6sIS8sd+yJyC/cOf339m5gHHqSpTVu5ocBiwxufn8+Xba70+F4T8nOtTUlF9QJL1+uyNPDt1HeVVB/bunTLuc/r8+uMDtj88cSXjv6w/sauo9jF5xf733JItxcEexoBlW/fyhnshw/0fLgPg69U7Aee8y6t8zCvYzS1nHMHLPzyx1hWfAZf98xvO/8fBz9cYE7+kJV33sSY1PUPz7nide8f0D/7SvGp4Dx64/JgIR9awwF/G68eNqbW9otpHlc9PdloyFdU+npyyhp0llbzQxGQo4NS+Hfhq1c6wxZso3r9tJL07ZuIVIckrJHs9qCpVPj+qTpKVkuQhyePB6xH8fqcIQrXPj8+vpCd78bjdGjU+P4ozWbmixk9akodNu8sp2ldJ385tSEvykpLkobzKR//fNq0H7JM7T+W8Oh/uT39/OO2yUrjtxblsKa4/KX/rlhEM7tYWjwgV1T4G/u7T4GPv3XYKT01Zy4fu1ZAd26Syo55k9J7z+3HRkK7ktU1D1fnD5Rk3QfrbxJUA/PDU3vz7q3X1/lzzO2Twv+kF/PWT/cl5bkYy4749mP/739xa+18ytCvXjejFt/45rd738p8uGxRcLPreMf3577QCqmr8wfIbD3x7MHe/uSi4/2d3nc7yrXu57eV5wW3nD+rCE25Pq1+V0X+bQpfsNGa6FyL88NTe3HvhwDJVrV04zBgT1xIqgUpLz9Aud7zO+7eN5KLHpwJw7sDOPPW94RGOzPmr16/g9QiVNT4WbCymZ7sMTvrLJADO7t+ZQd2zObF3ex77fBVfr258aRVzeHTISmX2vWcxeUUhNz43i9DOjs7Zqcz41Vl8tGgrt7xY+8N//bgxVFT7DhgWPO2ojny5ckfw/jkDOvPAt49h2B8ntup5BFx3ci+enxae+YH57TP4340ncOpfvwhLe4fDlcO789rs2mU2/u+0I3h9ziZ21bnAIaDggQstgTImwSTcEB7Apt37hxy+e5JzafLk5YW1hjRemrEh2KUPzmTmKSEfav/6Yg2TQ4am/jZhBZ8v3z9B+pYX5/LefGfuT43Pz89eW8CnS7YBTu/Rt/75NW/Pc35J795XRZ9ffcTfJ67kxRkFnPrAZK58aloweQKYuGw7f5+4iqvHT7fkKYpkpnjJyUhm8opC5hXs5pGrh/GLc48OPn6pu1zQljoLHZ9xVEf+8P4SUkKGhgJr+YVeNQhOQvXPL1YfUNiztXRum0ayV2ifldL4zgdx3sAuHN2lDSUVzZsX1bFNKkd1bnqJkW8dG94lmbrlHDjsPbBbW07Ibxesjm+MMQm1lIu4a42ELjQbKI74xw+WMrBbW47rlQvAE5NXc9IR7Tmlj3PJ/GOfr+as/p05/aiOgFOp+tKhXRnlFuN8YXoB1T5ldL/OlFbW8NGirXy0aCvT1+2iR246X63awYeLtlBR7eeJa4cxd8Me5m7Yw09fXRCMJbAsiTm8erbL4KIheTwx+eCTpQOW/OFcnv5qHU9OWcOS+84D4K+fLOd/0wpY9IdzAWce11tzN3PP+f0B2FFSSZ9OWXx21+kA/OXjZcxctwuPR1g/bgwPfLKcZVv38sUvRgHw4KfLKSgq43F36OjhCSu4ZGhXXp65EYBje+aws7SSDbucxOzCY/KCk/YX/+Fc3pq7iaoaf3Co+u7zjia/fSbnu9XgP1iwhY7ZqZyQ34535m+mtKKGswd2pku2U7H9+hH5JHs9VNX4WVVYSu8OmcECp6FqfH5EhJv/O5tJywv59M7TOLrLgclPYAi6xuen2qekp3hZXVhCUWkV7TJTeG32Rn5+7tGkJtVOEovLqvnHpJXcc35/UkIKpVbV+Jm7YXewqOvDVw4NPqaqlFTW0CY1CZ9fGf/VWi4b1o28tvvXc6yo9pHkEZK8HlYXlrJxdxn7KmsYMzgPEeEno/twxK8+4toTe/LnywYDcPGQrrViW7y5mCkrd3DrqD7IAwecsjEmziXUEF5mZqZ2vv01bhvVh0fdS/e7tk3jm3vOZN3OfWSmeINlDgILo2a5lanLq3x4PRL8Je73a3AOS2WND1WnkGB2ejJPTVnT4FVNh1PfTlmsamRS9PH5ucwp2I2/CW+DDlkpvP+TkZz8l8+D2645oScvz2zevKuGpCZ5uH5EPj8+40iG3ucMV/XukHnAxPM2qUmUVNZwTHdnrs78jXsabPPeMf3p0jaNU/t0ZOrqnXTPTeeSJ74OPv7ZXacHa1BVVPsor/IFh8qO7ZnL3JAq5XDgXLTD7bpnZzJl5Q5W3H8eqUleFmzcQ05GMuuLyrju2ZmccXRHnvvBCcH9p68tomvbdHq2j99SA9FARGwIz5gEk1A9UAB5bdNZF3LV0JbiCkoqquldZ+HYQOIUkO4On1TW+Cgur+aJz1ezfFsJnbPTeG9B48ufXHNCD1ZtL2V2wW7ev20kPdtnkOX+hRz6lzU4H+QecZK1xZuLufCxqaQne+mUncrfrhhC305tOPPhL4JFKe84sy8922Wwa18VXo9w3wdL+e2FA7hhZG8u++fXzHOXRKnrj5cO4nvuEOYXKwq5/j+zgo8N6ZHDgjqJyR1nHUVe23S++uUoTv3rZH58xpH88tyjaZ+ZQlZaEt87qRd//GApv794INPWFLFmRyn3f7iM6fecWWs4MpAA/fHSQZwzoDNt05NJ9nrw+ZUkj+DxCA9dMYSfv76A938ykuuenRkcXn30mmFcdEwecwp2c1yvXHbtq+K4+50yCDeN7M13TuzJqL9NAeDNH48I9igCjDnG6X156nvHUVxWzZXH96h1fmnJXtKSvVw6tCvvzN/C09cN51g3mfrNmP6cM7BLo69za/vXd49lw66yYE/NkB45gPO+vvm0I7htdO0Fb09qpP6VMcaYQ5NwPVDf+dcUVhWW8s6tpzDkDxMAZ4LoPRf0r/eYimofb8zZRKc2qYzs24E/vLeUV2dvPOjznNC7HX/51mDenbeZ60bk076VKloX7q1gb0VNsAelPjc+N6tWGYFzBnRmWM9cju6SxaijOyEh65WE1sKZ/PMz2FdZw4WPOZPt6/6MCor20T03A299BXLqcfvL83hvwRY+vuNU+udlN35AiD1lVfzm3SVcNbwHI90q5KF++up83p63mdn3nkWHrNQGr1xsqmqfn9KKGjJSvRx9rzPB++nvD+esAZ0PqT0T/6wHypjEE9M9UCJyHvAI4AWeVtVxjR1zTPe2TFjqTPb+7YUDuO+DpTz15dp6E6jfvbu41tVI/3faEUxf50zgzkjx8vwNJzCsRw4eEdbsKGVflY/B3doGk4q7zjn6gDbDqVN2Gp0ayUUevGIIL8/cwIOfOpeEj/9+w1ccpiV7qHAXHk5L9tC7Q1u++uUo2qQlkZNRe0Jxr/bN+6yorHFq/NRdX64pcjJSeOyaYQ0+/verhvL3q4YCztDq/ZcOqne+TlMlez3kZqbUqhdWWhneApEmsRzK7ypjTHSL2QRKRLzAE8DZwCZgloi8p6oHXfF1sLvg7G/fXcyDlw/hvg+c3T9ZvJVXZm1k+toiKqr9PP394WS5C8tmpSZx48jeXDQkr8Geqr7NuGqoNQXKIfhV8YqQk57Mj04/krkbdpOTnkxljY9kjyc4fyug2uenotrPsJ45fP+kXmzZU8E787bwwCfLuXdMf646vgcpSR68ItT4lcoaPxkpXrwibCkup2ObVJI8HopKK8lOTybF68GnSkW1j/RkL7+7aAAn9m5P15x09pRVkZ2WjMcjVPv87KusweMRtuwpp2+nNvj8yuyCXQzrkes8p0coKq2kTVoyPr/i9QiKkuzxUO33U17lw+MRXp6xgb98vJyXfngiQ3vk4Pcre8qryU5Losav7CytrLW+nUeEkopqkjwe0lO8iDi1mPZW1JCVmkRVSMHMO1+dz9kDOpOS5MFzkCywJT260ki7gdfV51d8qvj9SlqylySPoO751D2mxh0WDdSZqvsczY23oRhLKqrJTE066M+m7vMGYgpEEDgy8BxNiU1Eau13sJ/hAcc2ZZ9Gdqrxa6PtHOrvKmNMdIvZITwRORn4vaqe696/B0BV/9LQMZmZmbplxx6G3DchuC0tyUOy11OrcrZHCH4wgfsLXp3vgZ+XcxsUdb87G/3uB0OgnYbU/bFrrce03u31HRdK5OCPh0rySHB/Zf/Cv4dTc+I1JpodrA7UofyuMsZEv5jtgQK6AaGTkTYBJ9bdSURuBm4GSElJoW1GMm/++GSenbqenu0zgpdVry4sZVVhCaOP7kRKkueAv2Sd3gkJ9lIEt4lbHEGcHgBh/1+tfj34X7l1/7qVkL1DHzugjZAHQx/zq+IRwSOC1wNVPkVV+WZNEXMKdtOvi9O7c87Azvj8+xM8EZiwZDtF+6q46vgeLN5cfEBF5x+ckk/7zBT86vzVPWHJNi4YnEdJRTX//modw3vlcuIR7Xhi8hpuOeNIvB5h174qXpyxgVtHHcm0NUXM3bCHX13QD3CWVymtqCEnI4X/fL2OswZ05osVO7jwmDw6tUnloQkrGdmnA0N75FDl8/Pa7I1cOrQbizYXc2rfDqzcXkLvDpmUVtTw3oIt5GSkMGZwHo9PXk2btCT6d8mmX14bvly5g8uGdcfrgYnLCjm7fye27a2gXUYKKUkepq7eSX77TLrnpgdfsxemFzCkRw7HdGtLktdD7w6Z/OiFOVw/Yv/P4GA9E4cwSnlAolwfjziLHXtF8HqE9UX7eGfeFm4+7YhaPTmhvB6hxqd4PTR4pWVT462d5Ie+z5W3523m4iFd3YrsTRuqDcRc93tjsYXuq7r/fdycXLwpibs20qKq84eIX+HOB0gSkdkhD49X1fHu7Sb9rjLGxJZY7oG6HDhPVW9y738POFFVb2vomMzMTN23r/G12IwxpjkONon8UH5XGWOiXyxXIt8MhF6H3t3dZowx0cR+VxkTh2I5gZoF9BWR3iKSAlwNvBfhmIwxpi77XWVMHIrZOVCqWiMitwGf4lwa/KyqLolwWMYYU4v9rjImPsXsHKhDYXOgjDGtwQppGpN4YnkIzxhjjDEmIiyBMsYYY4xpJkugjDHGGGOayRIoY4wxxphmsgTKGGOMMaaZLIEyxhhjjGmmhCpjICJ+oDzScYRJElDT6F6xI57OJ57OBeLrfFrrXNJV1f4gNSaBxGwhzUM0V1WHRzqIcBCR2fFyLhBf5xNP5wLxdT7xdC7GmMiyv5iMMcYYY5rJEihjjDHGmGZKtARqfKQDCKN4OheIr/OJp3OB+DqfeDoXY0wEJdQkcmOMMcaYcEi0HihjjDHGmBazBMoYY4wxppkSIoESkfNEZIWIrBaRsZGOpyEi8qyIFIrI4pBt7URkooiscr/nuttFRB51z2mhiBwbcsx17v6rROS6CJ1LDxGZLCJLRWSJiNwRq+cjImkiMlNEFrjn8gd3e28RmeHG/KqIpLjbU937q93H80PausfdvkJEzj3c5xISh1dE5onIB+79WD6X9SKySETmi8hsd1vMvc+MMTFGVeP6C/ACa4AjgBRgATAg0nE1EOtpwLHA4pBtfwXGurfHAg+4ty8APgYEOAmY4W5vB6x1v+e6t3MjcC55wLHu7TbASmBALJ6PG1OWezsZmOHG+Bpwtbv9SeDH7u1bgCfd21cDr7q3B7jvv1Sgt/u+9EbovXYX8BLwgXs/ls9lPdChzraYe5/Zl33ZV2x9JUIP1AnAalVdq6pVwCvAJRGOqV6q+iWwq87mS4Dn3dvPA5eGbP+vOqYDOSKSB5wLTFTVXaq6G5gInNfqwdehqltVda57uwRYBnQjBs/HjanUvZvsfikwGnjD3V73XALn+AZwpoiIu/0VVa1U1XXAapz352ElIt2BMcDT7n0hRs/lIGLufWaMiS2JkEB1AzaG3N/kbosVnVV1q3t7G9DZvd3QeUXd+brDPsNwem5i8nzcIa/5QCHOh+saYI+qBpYFCY0rGLP7eDHQnig5F+AfwC8Bv3u/PbF7LuAksxNEZI6I3Oxui8n3mTEmdiTaUi4xTVVVRGKq7oSIZAFvAneq6l6n88IRS+ejqj5gqIjkAG8D/SIb0aERkQuBQlWdIyJnRDiccBmpqptFpBMwUUSWhz4YS+8zY0zsSIQeqM1Aj5D73d1tsWK7O8SA+73Q3d7QeUXN+YpIMk7y9KKqvuVujtnzAVDVPcBk4GSc4Z/AHyGhcQVjdh9vCxQRHedyCnCxiKzHGc4eDTxCbJ4LAKq62f1eiJPcnkCMv8+MMdEvERKoWUBf9yqjFJyJsO9FOKbmeA8IXBF0HfBuyPbvu1cVnQQUu0MWnwLniEiue+XROe62w8qdJ/MMsExVHw55KObOR0Q6uj1PiEg6cDbOnK7JwOXubnXPJXCOlwOfq6q62692r2zrDfQFZh6Wk3Cp6j2q2l1V83H+LXyuqt8hBs8FQEQyRaRN4DbO+2MxMfg+M8bEmEjPYj8cXzhX3qzEmbfy60jHc5A4Xwa2AtU4czBuxJlvMglYBXwGtHP3FeAJ95wWAcND2rkBZ1LvauAHETqXkThzUxYC892vC2LxfIBjgHnuuSwGfutuPwInaVgNvA6kutvT3Pur3cePCGnr1+45rgDOj/D77Qz2X4UXk+fixr3A/VoS+Pcdi+8z+7Iv+4qtL1vKxRhjjDGmmRJhCM8YY4wxJqwsgTLGGGOMaSZLoIwxxhhjmskSKGOMMcaYZrIEyhhjjDGmmSyBMmEjIioifwu5/3MR+X2Y2n5ORC5vfM8WP88VIrJMRCbX2d5VRN5wbw8VkQvC+Jw5InJLfc9ljDEmOlkCZcKpEviWiHSIdCChQipsN8WNwA9VdVToRlXdoqqBBG4oTk2rcMWQAwQTqDrPZYwxJgpZAmXCqQYYD/y07gN1e5BEpNT9foaITBGRd0VkrYiME5HviMhMEVkkIkeGNHOWiMwWkZXumm6BRX4fFJFZIrJQRP4vpN2vROQ9YGk98Vzjtr9YRB5wt/0WpwDoMyLyYJ398919U4D7gKtEZL6IXOVWw37WjXmeiFziHnO9iLwnIp8Dk0QkS0Qmichc97kvcZsfBxzptvdg4LncNtJE5D/u/vNEZFRI22+JyCciskpE/hry83jOjXWRiBzwWhhjjGk5W0zYhNsTwMLAB3oTDQH6A7uAtcDTqnqCiNwB/AS4090vH2edsyOBySLSB/g+znIcx4tIKvC1iExw9z8WGKSq60KfTES6Ag8AxwG7gQkicqmq3icio4Gfq+rs+gJV1So30Rquqre57f0ZZ4mTG9wlX2aKyGchMRyjqrvcXqjL1FlUuQMw3U3wxrpxDnXbyw95yludp9XBItLPjfUo97GhwDCcnr8VIvIY0AnopqqD3LZyDvJzN8YYc4isB8qElaruBf4L3N6Mw2ap6lZVrcRZYiOQAC3CSZoCXlNVv6quwkm0+uGsWfZ9EZkPzMBZwqOvu//MusmT63jgC1Xdoao1wIvAac2It65zgLFuDF/gLH/S031soqrucm8L8GcRWYizvEg3oHMjbY8EXgBQ1eVAARBIoCaparGqVuD0svXC+bkcISKPich5wN4WnJcxxpgGWA+UaQ3/AOYC/wnZVoObsIuIB0gJeawy5LY/5L6f2u/RuusOKU5S8hNVrbXwq4icAew7lOAPgQDfVtUVdWI4sU4M3wE6AseparWIrMdJtg5V6M/NBySp6m4RGQKcC/wIuBJnjTdjjDFhZD1QJuzcHpfXcCZkB6zHGTIDuBhIPoSmrxARjzsv6gicRWw/BX4sIskAInKUiGQ20s5M4HQR6SAiXuAaYEoz4igB2oTc/xT4iYiIG8OwBo5rCxS6ydMonB6j+toL9RVO4oU7dNcT57zr5Q4NelT1TeBenCFEY4wxYWYJlGktfwNCr8b7N07SsgA4mUPrHdqAk/x8DPzIHbp6Gmf4aq478fopGulZVdWtOPOOJgMLgDmq+m4z4pgMDAhMIgf+iJMQLhSRJe79+rwIDBeRRThzt5a78RThzN1aXHfyOvBPwOMe8ypwvTvU2ZBuwBfucOILwD3NOC9jjDFNJKp1R0WMMcYYY8zBWA+UMcYYY0wzWQJljDHGGNNMlkAZY4wxxjSTJVDGGGOMMc1kCZQxxhhjTDNZAmWMMcYY00yWQBljjDHGNNP/A00KpX0KF/oYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = IMNN.plot(expected_detF=None);\n",
    "ax[0].set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(4.6451797, dtype=float32)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(IMNN.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fe60153ec50>"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(IMNN.F)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 3.957057e+09, -8.500888e+08],\n",
       "             [-8.500888e+08,  1.889597e+08]], dtype=float32)"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMNN.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(IMNN.history['detF'][:1750], label='training')\n",
    "plt.plot(IMNN.history['val_detF'][:1750], color='k', linestyle=':', label='validation')\n",
    "#plt.plot(jnp.ones(len(IMNN.history['detF'][:]))*det_theoryF, c='k', linestyle='--', label='lognormal information')\n",
    "#plt.ylim(1e-2, 1e8)\n",
    "plt.legend()\n",
    "plt.ylabel(r'$\\det F$')\n",
    "plt.xlabel('number of epochs')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do GA for target data\n",
    "np = onp\n",
    "folder_name = maindir + 's8_p'\n",
    "mass,X,V,Npart = load_single_sim(folder_name, 330, 1.5e15)\n",
    "target_data = numpy_to_graph(X, V, mass, Npart, connect_radius=200, return_components=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = jnp\n",
    "estimates = IMNN.get_estimate(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.35830262, 0.9568876 ], dtype=float32)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.3175, 0.834 ], dtype=float32)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tfp.distributions.Blockwise(\n",
    "    [tfp.distributions.Uniform(low=low, high=high)\n",
    "     for low, high in zip([0.3, 0.78], [0.38, 1.1])])\n",
    "prior.low = np.array([0.3, 0.78])\n",
    "prior.high = np.array([0.38, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[7.5362987e-09, 3.3904183e-08],\n",
       "             [3.3904179e-08, 1.5781971e-07]], dtype=float32)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMNN.invF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makinen/repositories/imnn/imnn/lfi/lfi.py:451: UserWarning: No contour levels were found within the data range.\n",
      "  levels=levels))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "GA = imnn.lfi.GaussianApproximation(\n",
    "    parameter_estimates=IMNN.get_estimate(target_data),\n",
    "    invF=np.expand_dims(np.linalg.inv(IMNN.F), 0),\n",
    "    prior=prior,\n",
    "    gridsize=200)\n",
    "GA.marginal_plot(\n",
    "    known=None,\n",
    "    label=\"Gaussian approximation\",\n",
    "    axis_labels=[\"$\\Omega_m$\", \"$\\sigma_8$\"],\n",
    "    colours=\"C1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
